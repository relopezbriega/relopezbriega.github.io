[{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\n\u0026ldquo;Conocer la realidad implica construir sistemas en continua transformaci√≥n que se corresponden, m√°s o menos, a la realidad\u0026rdquo;\nJean Piaget\nIntroducci√≥n# A medida que el mundo se hace m√°s interconectado y complejo, nuestra habilidad para comprenderlo se hace menos efectiva. Los modelos simples que sol√≠amos utilizar ya no alcanzan para responder muchas de nuestras preguntas. Para poder entender las din√°micas de los sistemas complejos; necesitamos de nuevas herramientas. El poder de c√°lculo de las computadoras actuales nos permite realizar nuevos tipos de experimentos. Una de las nuevas metodolog√≠as que disponemos para analizar los sistemas complejos son los modelos basados en agentes.\nModelos basados en Agentes# La idea central de los modelos basados en agentes es que muchos de los fen√≥menos que vemos en el mundo pueden ser modelados con agentes, un ambiente, y la descripci√≥n de las interacciones entre los agentes y entre los agentes y el ambiente. Un agente es un individuo aut√≥nomo o un objeto con propiedades particulares, acciones y posibles objetivos. El ambiente es el territorio en el que los agentes interact√∫an. Las interacciones pueden ser de los agentes entre s√≠ o de √©stos con el ambiente y pueden llegar a ser bastante complejas e incluso pueden llegar a cambiar con el paso del tiempo. Como las interacciones est√°n construidas en base a un intercambio de informaci√≥n, luego de una interacci√≥n el agente puede actualizar su estado interno o tomar distintas decisiones.\nLos modelos basados en agentes constituyen una nueva generaci√≥n de m√©todos computacionales que permiten modelar la estructura de un sistema complejo y simular su evoluci√≥n din√°mica a lo largo del tiempo. Constituyen un tercer modo de hacer ciencia, distinto y complementario a los dos m√©todos cient√≠ficos cl√°sicos: la inducci√≥n y la deducci√≥n.\nUn modelo basado en agentes es un tipo particular de modelo cient√≠fico que se implementa como una simulaci√≥n computacional. Por lo tanto, son modelos formales que deben ser distinguidos tanto de los matem√°ticos (basados en ecuaciones diferenciales o de otro tipo) como de los estad√≠sticos (orientados por variables y expresados como ecuaciones de regresi√≥n, estructurales, o de otro tipo).\nV√≠nculo micro-macro# Los modelos basados en agentes abordan el v√≠nculo micro-macro en una doble direcci√≥n. En primer lugar, permiten modelar y simular el v√≠nculo de lo micro a lo macro, es decir, \u0026ldquo;c√≥mo las interacciones locales y descentralizadas entre agentes aut√≥nomos y heterog√©neos generan una determinada regularidad\u0026rdquo; macrosocial. Se suele utilizar el concepto de emergente para referirse a la aparici√≥n de \u0026ldquo;cualidades o propiedades de un sistema que presentan un car√°cter de novedad con relaci√≥n a las cualidades o propiedades de los elementos considerados aisladamente\u0026rdquo;. Los fen√≥menos emergentes son, en consecuencia, dif√≠ciles de explicar y predecir en la medida en que las cualidades nuevas a nivel macro de un sistema no pueden deducirse ni reducirse al conocimiento anal√≠tico de las partes a nivel micro.\nEn segundo lugar, los modelos basados en agentes contribuyen a comprender el v√≠nculo de lo macro a lo micro, relativo al modo en que \u0026ldquo;las estructuras sociales construyen e influyen las acciones futuras y las interacciones entre los actores individuales\u0026rdquo;. El inter√©s anal√≠tico de esta fase del modelado del v√≠nculo micro-macro se centra en comprender c√≥mo \u0026ldquo;los individuos elaboran representaciones mentales de construcciones sociales\u0026rdquo; que influyen en su propia conducta pr√°ctica. En otros t√©rminos, la acci√≥n social (nivel micro) no puede escindirse del modo en que los individuos piensan y razonan sobre el orden social (nivel macro). Esta problem√°tica ha sido conceptualizada bajo el nombre emergente de segundo orden, es decir, a la aptitud reflexiva de los agentes para razonar sobre las propiedades emergentes que la misma interacci√≥n social produce (emergencia de primer orden).\nPensar con modelos# Aprender es explorar. Organizar e interpretar datos con modelos se ha convertido en una competencia fundamental para la estrategia en los negocios, la planificaci√≥n urbana, la econom√≠a, la medicina, la ingenier√≠a y la ecolog√≠a, entre muchas otras. Pero no necesariamente debemos quedarnos con un solo modelo, muchas veces la aplicaci√≥n de un conjunto de modelos puede ayudarnos a dar sentido a fen√≥menos complejos. La idea central es que el pensamiento con m√∫ltiples modelos produce sabidur√≠a a trav√©s de un conjunto diverso de marcos l√≥gicos. Los distintos modelos acent√∫an diferentes fuerzas causales. Sus conocimientos e implicaciones se superponen y se entrelazan. Al utilizar muchos modelos como marcos, desarrollamos una comprensi√≥n m√°s profunda de la realidad.\nAlgunas de las ventajas de utilizar modelos son:\nLos modelos son explicativos porque se√±alan los mecanismos esenciales que subyacen un fen√≥meno. Pueden funcionar como una prueba de que los mecanismos hipot√©ticos son suficientes para dar cuenta de una observaci√≥n. Los modelos nos proporcionan una prueba de concepto de que algo es posible.\nLos modelos facilitan la experimentaci√≥n. Los modelos se pueden ejecutar repetidamente para notar variaciones en su din√°mica y sus resultados. Los par√°metros del modelo se pueden variar para ver el efecto en su comportamiento y resultados.\nLos modelos nos proporcionan analog√≠as. Dado que los modelos son simplificaciones de la realidad, nos permiten encontrar similitudes con otras simplificaciones similares, incluso si los fen√≥menos modelados son aparentemente muy diferentes.\nLos modelos se pueden utilizar como veh√≠culo de comunicaci√≥n y educaci√≥n. Los modelos nos brindan una herramienta educativa que encapsula conocimientos que pueden no estar f√°cilmente disponibles en la observaci√≥n del mundo real.\nMesa# Una de las razones por las que amo Python; es que siempre es posible encontrar alguna librer√≠a para hacer casi cualquier cosa en Ciencia con su ayuda. Obviamente, modelar modelos basados en agentes no pod√≠a ser una excepci√≥n.\nMesa es un paquete de Python de c√≥digo abierto con licencia Apache 2.0 que nos permite crear r√°pidamente modelos basados en agentes utilizando componentes centrales incorporados (como programadores de agentes y cuadr√≠culas espaciales) o implementaciones personalizadas; visualizarlos usando una interfaz basada en el navegador; y analizar sus resultados con las herramientas de an√°lisis de datos Python.\nEl principio rector de la arquitectura de Mesa es la modularidad; hace suposiciones m√≠nimas sobre la forma que tomar√° un modelo. Se divide en tres categor√≠as principales: modelado, an√°lisis y visualizaciones. El componente principal de modelado, nos brinda todo lo que necesitamos para construir un modelo. La clase Model para guardar los par√°metros a nivel modelo; una o m√°s clases Agent que describen a los agentes del modelo; un Scheduler que controla la activaci√≥n de los agentes y maneja el tiempo y el espacio o red en la que se desarrollan las interacciones. Los componentes de an√°lisis son el data collector utilizado para grabar los datos de cada ejecuci√≥n del modelo y los batch runners para automatizar m√∫ltiples ejecuciones con distintos par√°metros. Finalmente, los componentes de visualizaci√≥n se utilizan para mapear desde un objeto modelo a uno o m√°s representaciones visuales a trav√©s de una interfaz de servidor con el navegador web.\nModelando el COVID19 - Modelo SIR# Tiempo de pasar a un ejemplo concreto, y que mejor que aprovechar la popularidad de los modelos epidemiol√≥gicos que trajo la pandemia global del COVID19. Para este ejemplo en particular vamos a utilizar el modelo SIR; el cual es uno de los m√°s simples para captar las caracter√≠sticas t√≠picas de los brotes epid√©micos. El nombre del modelo proviene de las iniciales S (poblaci√≥n susceptible), I (poblaci√≥n infectada) y R (poblaci√≥n recuperada); relaciona las variaciones de las tres poblaciones (Susceptible, Infectada y Recuperada) a trav√©s de la tasa de infecci√≥n y el per√≠odo infeccioso promedio.\nVeamos como implementarlo para una poblaci√≥n de 10000 individuos.\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # Importando las librer√≠as que vamos a utilizar import pandas as pd import numpy as np import matplotlib.pyplot as plt from datetime import date as datemethod from datetime import datetime from mesa import Agent from mesa import Model from mesa.time import RandomActivation from mesa.space import NetworkGrid from mesa.datacollection import DataCollector from mesa_SIR import SIR import mesa_SIR.calculations_and_plots as c_p # graficos incrustados %matplotlib inline parametros = {\u0026#39;I0\u0026#39;:0.01, \u0026#39;ptrans\u0026#39;:0.25, \u0026#39;progression_period\u0026#39;:3, \u0026#39;progression_sd\u0026#39;:2, \u0026#39;population\u0026#39;:10000, \u0026#39;interactions\u0026#39;:6, \u0026#39;reinfection_rate\u0026#39;:0.00, \u0026#39;death_rate\u0026#39;:0.0200, \u0026#39;recovery_days\u0026#39;:15, \u0026#39;recovery_sd\u0026#39;:7, \u0026#39;severe\u0026#39;:0.18, \u0026#39;steps\u0026#39;:20} # Par√°metros ################################################################################################################### # ptrans = Probabilidad de transmisi√≥n. # population = Total de poblaci√≥n. # progression_period = N√∫mero de d√≠as hasta que se busca tratamiento. # progression_sd = Desvio estandar para buscar tratamiento. # interactions = N√∫mero de interacciones por persona (baja con distancia social) # reinfection_rate = Probalidad de volver a enfermar luego de recuperado. # I0 = Probalidad inicial de estar infectado. # death_rate = Probabilidad de muerte luego de ser infectado. # recovery_days = Promedio de d√≠as para recuperarse # recovery_sd = Desvio estandar de los d√≠as de recuperaci√≥n. # severe = Probabilidad de desarrollar s√≠ntomas severos. # steps = n√∫mero de d√≠as de la simulaci√≥n. #Agent class class humano(Agent): def __init__(self, unique_id, model): super().__init__(unique_id, model) self.pos = unique_id self.infected, self.susceptible, self.severe = self.model.SIR_instance.initial_infection() self.was_infected = False self.recovered = False self.alive = True self.day = 0 self.induced_infections = 0 self.infected_others = False def step(self): self.model.SIR_instance.interact(self) self.day += 1 class modelo_COVID(Model): def __init__(self): super().__init__(Model) self.susceptible = 0 self.dead = 0 self.recovered = 0 self.infected = 0 interactions = parametros[\u0026#39;interactions\u0026#39;] self.population = parametros[\u0026#39;population\u0026#39;] self.SIR_instance = SIR.Infection(self, ptrans = parametros[\u0026#39;ptrans\u0026#39;], reinfection_rate = parametros[\u0026#39;reinfection_rate\u0026#39;], I0= parametros[\u0026#34;I0\u0026#34;], severe = parametros[\u0026#34;severe\u0026#34;], progression_period = parametros[\u0026#34;progression_period\u0026#34;], progression_sd = parametros[\u0026#34;progression_sd\u0026#34;], death_rate = parametros[\u0026#34;death_rate\u0026#34;], recovery_days = parametros[\u0026#34;recovery_days\u0026#34;], recovery_sd = parametros[\u0026#34;recovery_sd\u0026#34;]) G = SIR.build_network(interactions, self.population) self.grid = NetworkGrid(G) self.schedule = RandomActivation(self) self.dead_agents = [] self.running = True for node in range(self.population): new_agent = humano(node, self) self.grid.place_agent(new_agent, node) self.schedule.add(new_agent) self.datacollector = DataCollector(model_reporters={\u0026#34;infectados\u0026#34;: lambda m: c_p.compute(m,\u0026#39;infected\u0026#39;), \u0026#34;recuperados\u0026#34;: lambda m: c_p.compute(m,\u0026#39;recovered\u0026#39;), \u0026#34;susceptibles\u0026#34;: lambda m: c_p.compute(m,\u0026#34;susceptible\u0026#34;), \u0026#34;muertes\u0026#34;: lambda m: c_p.compute(m, \u0026#34;dead\u0026#34;), \u0026#34;R0\u0026#34;: lambda m: c_p.compute(m, \u0026#34;R0\u0026#34;), \u0026#34;casos_severos\u0026#34;: lambda m: c_p.compute(m,\u0026#34;severe\u0026#34;)}) self.datacollector.collect(self) def step(self): self.schedule.step() self.datacollector.collect(self) if self.dead == self.schedule.get_agent_count(): self.running = False else: self.running = True # Ejecuci√≥n del modelo # Guardar salida del modelo today = datemethod.strftime(datetime.utcnow(), \u0026#39;%Y%m%dZ%H%M%S\u0026#39;) filename = \u0026#39;COVID_output_\u0026#39; + today + \u0026#39;.csv\u0026#39; output_path = \u0026#39;/home/raul/Documentos/\u0026#39; output_file = output_path + filename # iniciar modelo covid = modelo_COVID() dias=parametros[\u0026#34;steps\u0026#34;] for i in range(dias): covid.step() # generar salida. output_data = covid.datacollector.get_model_vars_dataframe() output_data infectados recuperados susceptibles muertes R0 casos_severos 0 94 0 9906 0.0 0.000000 12 1 1357 10 8633 0.0 2.791667 229 2 6748 88 3161 3.0 2.310806 1193 3 9322 311 355 12.0 2.119270 1651 4 9288 638 53 21.0 2.105173 1629 5 8860 1088 9 43.0 2.097285 1545 6 8265 1665 3 67.0 2.097300 1409 7 7572 2337 1 90.0 2.098090 1283 8 6731 3157 1 111.0 2.098667 1126 9 5839 4029 1 131.0 2.098212 968 10 4858 4997 0 145.0 2.097477 791 11 3826 6013 0 161.0 2.097345 609 12 2901 6929 0 170.0 2.097471 453 13 2108 7713 0 179.0 2.097772 323 14 1402 8417 0 181.0 2.097577 217 15 864 8952 0 184.0 2.097814 145 16 489 9324 0 187.0 2.098052 83 17 246 9564 0 190.0 2.098052 40 18 110 9699 0 191.0 2.098052 20 19 46 9763 0 191.0 2.098052 9 20 22 9787 0 191.0 2.098052 5 title = \u0026#39;Modelo COVID\u0026#39; plot1 = c_p.plot_SIR(output_data, title) c_p.plot_severe(output_data, title) Como vemos en este ejemplo simplificado, la infecci√≥n se expande a un velocidad muy considerable; en tan solo 3 d√≠as la mayor√≠a de la poblaci√≥n est√° infectada. Podemos jugar con los par√°metros, por por ejemplo con el de interacciones, para ver el efecto que puede tener reducir la circulaci√≥n social en la expansi√≥n de la infecci√≥n (el ya tan famoso aplanamiento de la curva en Argentina).\nLos modelos basados en agentes constituyen una herramienta m√°s del enorme arsenal que nos ofrecen las computadoras para asistirnos en el proceso de toma de decisiones. No deber√≠amos perder la oportunidad de explorarlos!\nSaludos!\nEste post fue escrito por Ra√∫l e. L√≥pez Briega utilizando Jupyter notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2020-08-09","id":0,"permalink":"/blog/2020/08/09/modelos-basados-en-agentes-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\n\u0026ldquo;Conocer la realidad implica construir sistemas en continua transformaci√≥n que se corresponden, m√°s o menos, a la realidad\u0026rdquo;\nJean Piaget\nIntroducci√≥n# A medida que el mundo se hace m√°s interconectado y complejo, nuestra habilidad para comprenderlo se hace menos efectiva. Los modelos simples que sol√≠amos utilizar ya no alcanzan para responder muchas de nuestras preguntas.","tags":["python","incertidumbre","matematica","pensamiento","modelos","inteligencia"],"title":"Modelos basados en Agentes con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, Analisis de datos y Python. El contenido esta bajo la licencia BSD.\n\u0026ldquo;No, no est√°s pensando\u0026hellip;solo est√°s siendo l√≥gico\u0026rdquo;\nNiels Bohr\nIntroducci√≥n# Cuando pensamos, estamos realizando un complejo proceso que se inicia con la creaci√≥n de im√°genes mentales en nuestro cerebro. Estas im√°genes las integramos, emparejamos, proyectamos o asociamos con nuestros conceptos o esquemas mentales que representan nuestra realidad en un proceso simb√≥lico que necesitamos estructurar en secuencias que sigan un orden racionalmente definido. Gracias a este proceso, podemos prever lo que suceder√°, evaluar las consecuencias de nuestros actos; y anticiparnos para evitar episodios desfavorables. Naturalmente, al ser seres humanos, este proceso de pensamiento puede verse afectado por nuestras emociones y otros factores f√≠sicos o sociales que prejuzgan nuestra manera de representar las cosas. Una de las herramientas que tenemos para evitar errores en nuestro pensamiento es la l√≥gica.\n¬øQu√© es la L√≥gica?# Una de las principales herramientas que poseemos para intentar comprender la realidad que nos rodea es la l√≥gica. M√°s formalmente la l√≥gica es un arte en la ciencia de la comprensi√≥n o el razonamiento. Se trata de avanzar en nuestra comprensi√≥n de una manera ordenada y correcta y que nos brinde facilidad. Se centra en el estudio de los procedimientos v√°lidos y no v√°lidos de pensamiento, es decir, en procesos como la demostraci√≥n, la inferencia o la deducci√≥n, as√≠ como en conceptos como las falacias, las paradojas y la verdad. El prop√≥sito de la l√≥gica es guiar de manera precisa los pensamientos que, de no ser exactos, fallar√≠an.\nL√≥gica y argumentaci√≥n# Los seres humanos tenemos la capacidad para establecer y verificar hechos, cambiar o justificar opiniones, y en general, dar sentido a las cosas. Hacemos esto a trav√©s de la raz√≥n; si bien todos somos capaces de razonar, alarmantemente solo un porcentaje peque√±o de nosotros lo hace realmente bien.\nLa l√≥gica es la ciencia que se usa para evaluar un argumento. Un argumento es un intento de persuadir a algui√©n de algo a trav√©s de darle razones para aceptar una conclusi√≥n dada. Utilizamos y escuchamos argumentos todos los d√≠as e incluso muchas veces sin darnos cuenta de ello. Si queremos reconocer un buen argumento de uno malo y ganar argumentaciones, entonces debemos dominar los mecanismos de la racionalidad pura, es decir, las leyes de la l√≥gica.\nUn argumento esta hecho de premisas y una conclusi√≥n. Las premisas suelen ser referidas tambi√©n como razones o evidencias; en esencia, son simples proposiciones o declaraciones que se presentan para ser aceptadas como v√°lidas. La conclusi√≥n no se m√°s que la proposici√≥n final del argumento. Si el argumento es v√°lido, las premisas implican la conclusi√≥n.\nL√≥gica inductiva y deductiva# En general, usamos dos tipos de l√≥gica, deductiva e inductiva. Las inferencias deductivas empiezan con un conocimiento general y predicen una observaci√≥n espec√≠fica; es decir que, en esta forma de razonamiento, la conclusi√≥n sigue necesariamente a las premisas. El ejemplo cl√°sico es:\npremisa 1: Todos los humanos son mortales. premisa 2: Socrates es humano. conclusi√≥n: Socrates es mortal. Las inferencias inductivas hacen lo contrario. Comienzan con observaciones de una experiencia particular y llegan a conclusiones generales. En la inducci√≥n, la conclusi√≥n se basa simplemente en una probabilidad. El ejemplo cl√°sico es:\npremisa 1: El sol ha salido todos los d√≠as hasta ahora. conclusi√≥n: Por lo tanto, el sol saldr√° ma√±ana. La soluci√≥n de problemas demasiado complicados para ser resueltos por medio del sentido com√∫n se logra mediante largas series de inferencias mezcladas, tanto inductivas como deductivas, que se entrelazan entre la experiencia observada y se formalizan como el m√©todo cient√≠fico. El prop√≥sito real del m√©todo cient√≠fico es asegurarnos de que la Naturaleza no nos ha inducido a pensar que sabemos algo que en realidad ignoramos. Un solo desliz l√≥gico y se derrumba todo un edificio cient√≠fico.\nFalacias# La importancia de conocer los mecanismos de la l√≥gica radica en tratar de comprender si un argumento que alguien dice es l√≥gicamente consistente; ya que si es inconsistente, implica que no deber√≠amos creer lo que esa persona nos dice. Lo m√°s importante a tener en cuenta es la verdad de sus premisas; es decir, si √©stas premisas se sustentan en evidencias y observaciones de la realidad.\nUna falacia es un error de razonamiento que invalida un argumento. Una de las caracter√≠sticas b√°sicas del pensamiento l√≥gico es la capacidad para detectar errores en las conclusiones o en las premisas de un determinado argumento para poder evitar los razonamientos falaces, ya que √©stos nos dificultan llegar a conocer la verdad de los hechos y nos hacen m√°s susceptibles a la manipulaci√≥n y la tergiversaci√≥n. Las falacia son, en definitiva, afirmaciones sin fundamento que a menudo se proclaman con una convicci√≥n que las hace sonar como si fueran hechos probados. Basta dedicar cinco minutos a escuchar las noticias o a leer la secci√≥n de pol√≠tica para encontrar cientos de ejemplos de falacias! Veamos algunas de las m√°s comunes:\nFalacia ‚Äúad hominem‚Äù: Los ataques personales son contrarios a los argumentos racionales. En l√≥gica y ret√≥rica, un ataque personal se llama ‚Äúad hominem‚Äù, que en lat√≠n significa ‚Äúcontra el hombre‚Äù. En lugar de avanzar en un buen razonamiento, una falacia ad hominem reemplaza la argumentaci√≥n l√≥gica con un lenguaje ofensivo no relacionado con la verdad del asunto. (Muy com√∫n en la pol√≠tica argentina).\nFalacia de la apelaci√≥n a la autoridad: Esta falacia argumentativa, tambi√©n denominada ‚Äúad verecundiam‚Äù, ocurre cuando hacemos mal uso de una autoridad. Este mal uso de la autoridad puede ocurrir de varias maneras; por ejemplo: podemos citar solo a las autoridades, alej√°ndonos convenientemente de otras pruebas comprobables y concretas como si la opini√≥n de los expertos fuera siempre correcta; o podemos citar autoridades irrelevantes, autoridades pobres o autoridades falsas.\nFalacia populista: Esta falacia, tambi√©n denominada argumento ‚Äúad populum‚Äù, supone que algo es cierto (o correcto o bueno) porque otras personas est√°n de acuerdo con la persona que lo afirma; esto es, se acepta algo que se dice porque es popular.\nFalacia de la generalizaci√≥n apresurada: Una generalizaci√≥n apresurada es una declaraci√≥n general sin evidencia suficiente para respaldarla. √âsta se produce a partir de la prisa por llegar a una conclusi√≥n, lo que lleva a la persona que argumenta a cometer alg√∫n tipo de suposici√≥n il√≥gica o a emitir estereotipos, conclusiones injustificadas o exageraciones.\nFalacia del falso dilema: Esta falacia argumentativa ocurre cuando fallamos al limitar las opciones a √∫nicamente dos, cuando de hecho hay m√°s opciones para elegir. A veces las opciones son entre una cosa, la otra, o ambas cosas juntas (no se excluyen entre s√≠). Y a veces hay una amplia gama de opciones.\nObviamente, esta es una muestra muy peque√±a de las trampas en las que puede caer nuestro pensamiento. Hay cientos de ellas!\nJuegos de l√≥gica# A m√≠ mam√° le gustan las revistas de pasatiempos, esas revistas que traen cientos de juegos entre los que se incluyen sopas de letras, palabras cruzadas, autodefinidos, etc. Si bien ella tiene una habilidad incre√≠ble para resolver todos los juegos relacionados con palabras\u0026hellip;siempre tuvo muuuchas dificultades con los del tipo Qui√©n es Qui√©n; en esos juegos, nos presentan un n√∫mero de afirmaciones y en base a ellas debemos aplicar l√≥gica deductiva para obtener la informaci√≥n faltante. Aqu√≠ va un ejemplo:\nTenemos la siguiente informaci√≥n sobre cuatro personas:\nEsteban tiene un auto azul La persona que tiene un gato vive en Espa√±a. Mat√≠as vive en Argentina La persona con el auto negro vive en Brasil. Juan tiene un gato. Alex vive en Brasil La persona que tiene un perro vive en Colombia. ¬øQui√©n tiene un conejo? El objetivo entonces es, a partir de estas afirmaciones, deducir qui√©n es el due√±o del conejo.\nAquellos lo suficientemente valientes para intentar resolver el enigma con sus propios medios, pueden dejar de leer y recurrir al viejo m√©todo de papel y l√°piz para arribar a la soluci√≥n\u0026hellip;los que prefieren aplicar la ley del menor esfuerzo (como es mi caso) pueden continuar leyendo para ver como con la ayuda de Python y la librer√≠a kanren, una librer√≠a que nos ayuda a implementar el paradigma de la programaci√≥n l√≥gica; podemos resolver este enigma l√≥gico y encontrar su soluci√≥n.\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # Importando las librer√≠as que vamos a utilizar from kanren import run, var, eq, membero from kanren.core import lall # Enigma de L√≥gica # Declaro la variable gente gente = var() # Defino las reglas reglas = lall( # hay 4 personas (eq, (var(), var(), var(), var()), gente), # Esteban tiene un auto azul (membero, (\u0026#39;Esteban\u0026#39;, var(), \u0026#39;azul\u0026#39;, var()), gente), # La persona que tiene un gato vive en Espa√±a. (membero, (var(), \u0026#39;gato\u0026#39;, var(), \u0026#39;Espa√±a\u0026#39;), gente), # Mat√≠as vive en Argentina (membero, (\u0026#39;Mat√≠as\u0026#39;, var(), var(), \u0026#39;Argentina\u0026#39;), gente), # La persona con el auto negro vive en Brasil. (membero, (var(), var(), \u0026#39;Negro\u0026#39;, \u0026#39;Brasil\u0026#39;), gente), # Juan tiene un gato. (membero, (\u0026#39;Juan\u0026#39;, \u0026#39;gato\u0026#39;, var(), var()), gente), # Alex vive en Brasil (membero, (\u0026#39;Alex\u0026#39;, var(), var(), \u0026#39;Brasil\u0026#39;), gente), # La persona que tiene un perro vive en Colombia. (membero, (var(), \u0026#39;perro\u0026#39;, var(), \u0026#39;Colombia\u0026#39;), gente), # ¬øQui√©n tiene un conejo? (membero, (var(), \u0026#39;conejo\u0026#39;, var(), var()), gente) ) # Ejecuto la soluci√≥n soluciones = run(0, gente, reglas) # Extraigo la respuesta due√±o = [nombre for nombre in soluciones[0] if \u0026#39;conejo\u0026#39; in nombre][0][0] # Imprimo los resultados. print(\u0026#39;\\n\u0026#39; + due√±o + \u0026#39; es el due√±o del conejo\u0026#39;) print(\u0026#39;\\n Detalle del enigma:\u0026#39;) atributos = [\u0026#39;Nombre\u0026#39;, \u0026#39;Mascota\u0026#39;, \u0026#39;Color de auto\u0026#39;, \u0026#39;Pa√≠s\u0026#39;] print(\u0026#39;\\n\u0026#39; + \u0026#39;\\t\\t\u0026#39;.join(atributos)) print(\u0026#39;=\u0026#39; * 60) for item in soluciones[0]: print(\u0026#39;\u0026#39;) print(\u0026#39;\\t\\t\u0026#39;.join([str(x) for x in item])) Mat√≠as es el due√±o del conejo Detalle del enigma: Nombre\tMascota\tColor de auto\tPa√≠s ============================================================ Esteban\tperro\tazul\tColombia Juan\tgato\t~_9\tEspa√±a Mat√≠as\tconejo\t~_11\tArgentina Alex\t~_13\tNegro\tBrasil Como podemos ver en el detalle del enigma, con la informaci√≥n proporcionada todav√≠a hay datos que no podemos deducir, pero a pesar de la informaci√≥n incompleta, el programa s√≠ pudo deducir la pregunta que est√°bamos buscando. Mat√≠as, que vive en Argentina, es el due√±o del conejo.\nAqu√≠ finaliza este art√≠culo. Si bien la l√≥gica es una herramienta muy √∫til dentro de la compleja maquinaria de nuestro pensamiento, y nos ayuda a pensar con m√°s claridad y menos errores; no por ello debemos confiar ciegamente en ella y dejar de seguir nuestros instintos. Obviamente este art√≠culo va dedicado a una de las personas que m√°s amo en el mundo, mi mam√°! y su constante lucha con los juegos de l√≥gica. Tal vez con alg√∫n d√≠a los pueda resolver\u0026hellip; üòÜ\nSaludos!\nEste post fue escrito por Ra√∫l e. L√≥pez Briega utilizando Jupyter notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2020-07-12","id":1,"permalink":"/blog/2020/07/12/introduccion-al-pensamiento-logico-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, Analisis de datos y Python. El contenido esta bajo la licencia BSD.\n\u0026ldquo;No, no est√°s pensando\u0026hellip;solo est√°s siendo l√≥gico\u0026rdquo;\nNiels Bohr\nIntroducci√≥n# Cuando pensamos, estamos realizando un complejo proceso que se inicia con la creaci√≥n de im√°genes mentales en nuestro cerebro. Estas im√°genes las integramos, emparejamos, proyectamos o asociamos con nuestros conceptos o esquemas mentales que representan nuestra realidad en un proceso simb√≥lico que necesitamos estructurar en secuencias que sigan un orden racionalmente definido.","tags":["python","matematica","incertidumbre","pensamiento","inteligencia","falacias","logica"],"title":"Introducci√≥n al pensamiento l√≥gico con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, Analisis de datos y Python. El contenido esta bajo la licencia BSD.\n\u0026ldquo;LLamamos caos al orden que todav√≠a no comprendemos\u0026rdquo;\nEdward Lorenz\nIntroducci√≥n# Vivimos en un mundo tan bello como complejo. El enfoque tradicional de la ciencia,que busca generalmente reducir a los sistemas complejos en cada una de sus partes, comprender cada parte, y luego entender todo el sistema, parece no ser suficiente. Cuando las partes comienzan a conectarse e interactuar entre s√≠, las bases cient√≠ficas de este enfoque comienzan a fallar, y se torna casi imposible predecir el comportamiento del sistema. Los simples supuestos como la linealidad, la independencia, o la distribuci√≥n normal parecen no ser de mucha utilidad a la hora de modelar los fen√≥menos incluso m√°s sencillos del mundo que nos rodea. Es a partir de esta realidad, que nuevos enfoques se comenzaron a desarrollar y surgieron nuevos campos de estudio multidisciplinarios como los sistemas din√°micos, la complejidad, o la teor√≠a del caos.\n¬øQu√© es un sistema complejo?# Es dif√≠cil definir rigurosamente que es un Sistema complejo, podemos decir que es un campo de investigaci√≥n interdisciplinario que busca explicar c√≥mo un gran n√∫mero de entidades relativamente simples (las cuales generalmente se denominan agentes) se organizan, sin el beneficio de ning√∫n controlador central, en un todo colectivo que crea patrones, usa informaci√≥n y, en algunos casos, evoluciona y aprende. Es el caso t√≠pico en que el todo es mucho m√°s que la suma de sus partes. Aunque no existe una buena definici√≥n de que es un Sistema complejo, si podemos describir algunas de las caracter√≠sticas principales que poseen estos sistemas, como ser:\nEl sistema contiene una colecci√≥n de muchos agentes que interact√∫an. Las interacciones entre estos agentes pueden surgir porque est√°n f√≠sicamente cerca el uno del otro, o porque son miembros de alg√∫n tipo de grupo, o porque comparten alguna informaci√≥n com√∫n. En la medida en que los agentes est√°n vinculados entre s√≠ a trav√©s de sus interacciones, tambi√©n se puede pensar que forman parte de una red o grafo. Por lo que el an√°lisis de redes es una herramienta importante para comprender estos sistemas.\nEl comportamiento de estos agentes se ve afectado por una memoria o retroalimentaci√≥n. Esto significa que algo del pasado afecta algo en el presente, o que algo que sucede en un lugar afecta lo que est√° sucediendo en otro, en otras palabras, una especie de efecto de arrastre. El resultado neto de que todos tengan tal memoria puede ser que el sistema como un todo tambi√©n recuerde. Por lo tanto, un patr√≥n o secuencia global particular puede manifestarse en el sistema.\nLos agentes pueden adaptar sus estrategias de acuerdo a su historia. Esto simplemente significa que un agente puede adaptar su comportamiento por s√≠ mismo, con la esperanza de mejorar su rendimiento.\nEl sistema es t√≠picamente abierto. Es decir, que el sistema puede ser influenciado por su entorno.\nAs√≠ mismo, en el comportamiento caracter√≠stico de los Sistemas complejos, podemos observar los siguientes rasgos:\nEl sistema parece estar vivo. El sistema evoluciona de una manera no trivial y, a menudo complicada, impulsada por una ecolog√≠a de agentes que interact√∫an y se adaptan bajo la influencia de la retroalimentaci√≥n.\nEl sistema exhibe fen√≥menos emergentes que generalmente son sorprendentes y pueden ser extremos. En terminolog√≠a cient√≠fica, el sistema est√° lejos de alcanzar un equilibrio. Esto b√°sicamente significa que cualquier cosa puede suceder, y si se espera lo suficiente, generalmente lo har√°. Tales fen√≥menos generalmente son inesperados en t√©rminos de cu√°ndo surgen, de ah√≠ su aspecto de sorpresa. Pero el sistema tambi√©n tender√° a exhibir fen√≥menos emergentes que a su vez son sorprendentes en el sentido de que no podr√≠an haber sido predichos en base al conocimiento de las propiedades de los agentes individuales.\nLos fen√≥menos emergentes suelen surgir en ausencia de cualquier tipo de mano invisible o controlador central. En otras palabras, un Sistema complejo puede evolucionar de una manera complicada por s√≠ mismo. Por esta raz√≥n, a menudo se considera que los Sistemas complejos son mucho m√°s que la suma de sus partes.\nEl sistema muestra una mezcla complicada de comportamiento ordenado y desordenado. De manera m√°s general, todos los Sistemas complejos parecen poder moverse entre el orden y el desorden por su propia cuenta.\n¬øQu√© es la teor√≠a del caos?# La teor√≠a del caos, m√°s que una teor√≠a, es un paradigma que supuso en su momento una gran revoluci√≥n cient√≠fica, al reflejar que muchos sistemas que eran considerados deterministas y previsibles ten√≠an severos l√≠mites en dicha previsibilidad. Es decir, que no eran tan √∫tiles como se cre√≠a a la hora de predecir eventos futuros. Iniciada por Henri Poincar√© y popularizada gracias al trabajo del matem√°tico y meteor√≥logo Edward Lorenz, la teor√≠a del caos se ha utilizado en campos como las matem√°ticas y la meteorolog√≠a para explicar la inexactitud y la dificultad para obtener resultados previsibles de la realidad.\nEl efecto mariposa# Esta teor√≠a es ampliamente conocida por lo que se denomina el efecto mariposa, seg√∫n el cual el d√©bil aleteo de una mariposa puede ser la causa de un hurac√°n a miles de kil√≥metros de distancia. Se indica de este modo que la existencia de una variable concreta puede provocar o alterar otras, influy√©ndose progresivamente hasta obtener un resultado fuera de lo esperado. En s√≠ntesis, podemos considerar que la teor√≠a del caos establece que peque√±os cambios en las condiciones iniciales crean grandes diferencias respecto al resultado final, con lo que una gran mayor√≠a de los sucesos y sistemas no resultan totalmente predecibles.\nEs importante tener en cuenta que a pesar de las apariencias, el caos al que se refiere esta teor√≠a no implica una falta de orden, sino que los hechos y la realidad no se ajustan a un modelo lineal.\nEl mapa log√≠stico# Para entender mejor qu√© es el caos exploremos el famoso mapa log√≠stico, sin duda una de las ecuaciones m√°s famosas en la ciencia de los sistemas din√°micos. El mapa log√≠stico representa un modelo sencillo para intentar explicar la din√°mica de una poblaci√≥n de la que se ha supuesto que tiene un crecimiento cada vez m√°s lento a medida que se acerca a una cantidad de individuos considerada como l√≠mite. El modelo simplifica al combinar los efectos de la tasa de natalidad y la tasa de mortalidad en un solo n√∫mero, llamado \\(R\\). El tama√±o de la poblaci√≥n se reemplaza por un concepto relacionado \u0026ldquo;fracci√≥n de la capacidad de carga\u0026rdquo;, llamado \\(x\\). Su expresi√≥n matem√°tica es la siguiente:\n$$x_{t+1} = Rx_t(1 - x_t)$$ Esta ecuaci√≥n define las reglas o din√°micas de nuestro sistema: \\(x\\) representa la poblaci√≥n en un momento dado \\(t\\), y \\(R\\) representa la tasa de crecimiento. En otras palabras, el nivel de poblaci√≥n en un momento dado es una funci√≥n del par√°metro de la tasa de crecimiento y el nivel de poblaci√≥n del intervalo de tiempo anterior. Si la tasa de crecimiento es demasiado baja, la poblaci√≥n se extinguir√°. Las tasas de crecimiento m√°s altas podr√≠an establecerse hacia un valor estable o fluctuar a trav√©s de una serie de auges y declives de la poblaci√≥n. Como podemos ver, es una ecuaci√≥n muy simple! la m√°s simple en capturar la esencia del caos!\nEl mapa log√≠stico se vuelve muy interesante a medida que vamos variando el valor de \\(R\\); veamos algunos ejemplos num√©ricos con la ayuda de Python y la librer√≠a pynamical.\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # Importando las librer√≠as que vamos a utilizar import pandas as pd import numpy as np from fractal import Fern, Lorentz from pynamical import simulate, bifurcation_plot, phase_diagram, phase_diagram_3d import matplotlib.pyplot as plt import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) # graficos incrustados %matplotlib inline # modelo logistico para 20 generaciones con 7 tasas de crecimiento entre 0.5 y 3.5. poblacion = simulate(num_gens=20, rate_min=0.5, rate_max=3.5, num_rates=7) poblacion.applymap(lambda x: \u0026#39;{:03.3f}\u0026#39;.format(x)) 0.5 1.0 1.5 2.0 2.5 3.0 3.5 0 0.500 0.500 0.500 0.500 0.500 0.500 0.500 1 0.125 0.250 0.375 0.500 0.625 0.750 0.875 2 0.055 0.188 0.352 0.500 0.586 0.562 0.383 3 0.026 0.152 0.342 0.500 0.607 0.738 0.827 4 0.013 0.129 0.338 0.500 0.597 0.580 0.501 5 0.006 0.112 0.335 0.500 0.602 0.731 0.875 6 0.003 0.100 0.334 0.500 0.599 0.590 0.383 7 0.002 0.090 0.334 0.500 0.600 0.726 0.827 8 0.001 0.082 0.334 0.500 0.600 0.597 0.501 9 0.000 0.075 0.333 0.500 0.600 0.722 0.875 10 0.000 0.069 0.333 0.500 0.600 0.603 0.383 11 0.000 0.065 0.333 0.500 0.600 0.718 0.827 12 0.000 0.060 0.333 0.500 0.600 0.607 0.501 13 0.000 0.057 0.333 0.500 0.600 0.716 0.875 14 0.000 0.054 0.333 0.500 0.600 0.610 0.383 15 0.000 0.051 0.333 0.500 0.600 0.713 0.827 16 0.000 0.048 0.333 0.500 0.600 0.613 0.501 17 0.000 0.046 0.333 0.500 0.600 0.711 0.875 18 0.000 0.044 0.333 0.500 0.600 0.616 0.383 19 0.000 0.042 0.333 0.500 0.600 0.710 0.827 # Graficando los resultados colores = [\u0026#39;yellow\u0026#39;, \u0026#39;green\u0026#39;, \u0026#39;blue\u0026#39;, \u0026#39;peru\u0026#39;, \u0026#39;black\u0026#39;, \u0026#39;red\u0026#39;, \u0026#39;grey\u0026#39;] for color, tasa in reversed(list(zip(colores, poblacion.columns))): ax = poblacion[tasa].plot(kind=\u0026#39;line\u0026#39;, figsize=[10, 6], linewidth=2.5, alpha=0.95, c=color) ax.grid(True) ax.set_ylim([0, 1]) ax.legend(title=\u0026#39;Tasa de Crecimiento\u0026#39;, loc=3, bbox_to_anchor=(1, 0.525)) ax.set_title(\u0026#39;Resultados del modelo logistico por tasa de crecimiento\u0026#39;) ax.set_xlabel(\u0026#39;Generacion\u0026#39;) ax.set_ylabel(\u0026#39;Populacion\u0026#39;) plt.show() Atractores# Hasta aqu√≠, el sistema parece tener un comportamiento bastante predecible y parece ser estable. En los sistemas din√°micos, un Atractor es un valor, o conjunto de valores hacia los que el sistema tienen a evolucionar en el tiempo y estabilizarse. En nuestro ejemplo, cuando el par√°metro de crecimiento es 0.5, el sistema tiene un Atractor de punto fijo en el nivel de poblaci√≥n 0 como se muestra en la l√≠nea amarilla. En otras palabras, el valor de la poblaci√≥n tiende a 0 con el tiempo a medida que el modelo va evolucionando. Cuando el par√°metro de tasa de crecimiento es 3.5, el sistema oscila entre cuatro valores, como se muestra en la l√≠nea gris. Este otro Atractor se llama de ciclo l√≠mite.\nLo verdaderamente interesante se produce cuando aumentamos el valor de la tasa de crecimiento a un valor superior a 4, en ese momento el sistema se vuelve ca√≥tico!\nDiagrama de bifurcaci√≥n# Para poder ver claramente como el sistema evoluciona hacia un comportamiento ca√≥tico, en los sistemas din√°micos se suele recurrir a la ayuda de un diagrama de bifurcaci√≥n. Analizando la evoluci√≥n de nuestra funci√≥n, podemos ver que en las diferentes iteraciones con los valores de \\(R\\) entre 0.5 y 4.0; el sistema fue evolucionando hacia Atractor de punto fijo, luego hacia otro de ciclo l√≠mite de 2 valores, un siguiente de 4, luego otro de 8, y as√≠ sucesivamente hasta llegar al caos. Cada uno de estos ciclos son llamadas bifurcaciones.\nEstas bifurcaciones a menudo se resumen en un llamado diagrama de bifurcaci√≥n que traza el Atractor en el que termina el sistema en funci√≥n del valor de un \u0026ldquo;par√°metro de control\u0026rdquo;, que en nuestro caso es el valor de la tasa de crecimiento \\(R\\).\n# Modelo con 100 generaciones haciendo crecer R en 1000 pasos. pops = simulate(num_gens=100, rate_min=0, rate_max=4, num_rates=1000, num_discard=1) #Graficando el diagrama de bifurcaci√≥n bifurcation_plot(pops) Como se puede ver en el gr√°fico, en alg√∫n punto m√°s all√° de un valor de \\(R\\) de 3.5 las bifurcaciones se vuelven ca√≥ticas y se vuelve imposible de predecir.\nEdward Lorenz, no linealidad y los atractores extra√±os# No se puede escribir un art√≠culo sobre caos, sin mencionar a Edward Lorenz quien fuera uno de sus pioneros. Lorenz se top√≥ con las particularidades del caos al trabajar en modelos matem√°ticos para intentar predecir el clima (algo que ni siquiera podemos hacer bien hoy en d√≠a!). Fue el primero en notar (se podr√≠a decir que por accidente) el fen√≥meno de la dependencia sensible a los valores de las condiciones iniciales de algunos sistemas. La caracter√≠stica principal de estos sistemas es su no linealidad. Un sistema es lineal cuando podemos comprender sus partes individualmente y luego unirlas en un todo coherente. Las relaciones lineales se pueden capturar con una l√≠nea recta en un gr√°fico; son f√°ciles de pensar: cuanto m√°s, mejor. Las ecuaciones lineales se pueden resolver, lo que las hace adecuadas para libros de texto. En cambio, un sistema no lineal es aquel en el que el todo es diferente de la suma de las partes; no se pueden resolver o expresar con claridad en un gr√°fico. Son la pesadilla de cualquier matem√°tico. Profundizando en el estudio de estos sistemas no lineales y su complejidad, Lorenz simplific√≥ algunas ecuaciones de din√°mica de fluidos (llamadas ecuaciones de Navier-Stokes) y termin√≥ con un conjunto de tres ecuaciones no lineales:\n$$\\frac{dx}{dt} = P(y-x)$$ $$\\frac{dy}{dt} = x(R-z) - y$$ $$\\frac{dz}{dt} = xy - cz$$ donde \\(P\\) es el n√∫mero de Prandtl que representa la relaci√≥n entre la viscosidad del fluido y su conductividad t√©rmica; \\(R\\) es el n√∫mero de Rayleigh y representa la diferencia de temperatura entre la parte superior e inferior del sistema, . Los valores que utiliz√≥ Lorenz fueron \\(P = 10\\), \\(R = 28\\) y \\(c = 8/3\\).\nEn la superficie, estas tres ecuaciones parecen f√°ciles de resolver. Sin embargo, representan un sistema din√°mico extremadamente complicado. Al graficar los resultados en un espacio f√°sico; nos encontramos con lo que hoy se conoce como el Atractor de Lorenz.\nEl Atractor de Lorenz es un ejemplo de un atractor extra√±o. Los atractores extra√±os son √∫nicos en comparaci√≥n a los otros Atractores mencionados, en el sentido en que uno no sabe exactamente en qu√© parte del Atractor estar√° el sistema. Dos puntos en el Atractor que est√°n cerca uno del otro al mismo tiempo estar√°n arbitrariamente separados en momentos posteriores. La √∫nica restricci√≥n es que el estado del sistema permanezca en el atractor. Los atractores extra√±os tambi√©n son √∫nicos porque nunca se cierran sobre s√≠ mismos y el movimiento del sistema nunca se repite. Los atractores extra√±os tienen una estructura fractal.\nEsta imagen m√°gica del Atractor de Lorenz, que se asemeja a las alas de una mariposa, se convirti√≥ en un emblema para los primeros exploradores del caos. Revel√≥ la fina estructura oculta dentro de un flujo desordenado de datos.\nEl juego del caos# Siempre me han fascinado el azar y los procesos aleatorios, tal cual refleja la frase de Lorenz que abre el art√≠culo; como a pesar de tener un comportamiento err√°tico y que desaf√≠a toda l√≥gica, parecen seguir un orden subyacente que no llegamos a captar del todo. Y nada mejor para captar ese sentimiento que el juego del caos de Barnsley.\nPara jugar al juego del caos solo se necesita papel, l√°piz y una moneda. Se elige un punto de partida en alg√∫n lugar del papel, no importa donde; y luego se inventan dos reglas, una regla de cara y una regla de cruz. Cada una de estas reglas nos dir√°n c√≥mo dibujar nuevos puntos. Por ejemplo \u0026ldquo;Mu√©vase dos cent√≠metros hacia el noreste\u0026rdquo; o \u0026ldquo;Mu√©vase 25 por ciento m√°s cerca del centro\u0026rdquo;. Ahora solo debemos comenzar a lanzar la moneda al aire y dibujar puntos en el papel, usando la regla de cara cuando la moneda sale cara y la regla de cruz cuando sale cruz. A medida que se van acumulando los puntos en el papel, vamos a poder observar que describen una cierta figura\u0026hellip;la cual se va haciendo m√°s clara a medida que aumenta el n√∫mero de puntos.\nLa idea sobre la que descansa el juego del caos, es hacer uso de 2 propiedades de los fractales (para conocer m√°s sobre fractales pueden visitar mi art√≠culo anterior). En primer lugar, que los fractales existen como un l√≠mite a un proceso aleatorio. Por analog√≠a, uno podr√≠a imaginar un mapa de Argentina dibujado con tiza en el piso de una habitaci√≥n. Un top√≥grafo con herramientas est√°ndar encontrar√≠a complicado medir el √°rea de estas formas inc√≥modas, con costas fractales. Pero supongamos que se arrojaran granos de arroz al aire uno por uno, lo que les permitir√≠a caer al suelo al azar y que luego se contaran los granos que caen dentro del mapa. A medida que pasa el tiempo, el resultado comienza a acercarse al √°rea de las formas, como el l√≠mite de un proceso aleatorio. En t√©rminos din√°micos, las formas de Barnsley demostraron ser Atractores. La segunda propiedad de los fractales que utiliza el juego del caos es esta cualidad de estar formados por peque√±as copias de la imagen principal. En este sentido, mientras m√°s fractal es la imagen, m√°s simples van a ser las reglas subyacentes para construirlas.\nPara demostrar como funciona el juego del caos, podemos ayudarnos de Python para generar el famoso helecho de Barnsley. Al utilizar primero 1000 puntos, podemos apreciar que parece generarse una imagen, pero su forma todav√≠a no es n√≠tida\u0026hellip;cuando utilizamos 60000 puntos ya podemos ver con claridad la forma de la hoja de un helecho.\n# Dibujar una hoja de helecho utilizando el juego del caos de Barnsley con 10000 puntos fern = Fern() fern.plot(1000) # Dibujar una hoja de helecho utilizando el juego del caos de Barnsley con 60000 puntos fern = Fern() fern.plot(60000) Con esto llegamos al final del art√≠culo. La ida central de las ciencias que estudian la naturaleza es hacer de lo maravilloso un lugar com√∫n! Mostrar que la complejidad, vista correctamente, es s√≥lo una m√°scara para lo simple!; y encontrar patrones escondidos en el aparente caos!\nSaludos!\nEste post fue escrito por Ra√∫l e. L√≥pez Briega utilizando Jupyter notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2020-03-26","id":2,"permalink":"/blog/2020/03/26/sistemas-dinamicos-complejidad-y-caos-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, Analisis de datos y Python. El contenido esta bajo la licencia BSD.\n\u0026ldquo;LLamamos caos al orden que todav√≠a no comprendemos\u0026rdquo;\nEdward Lorenz\nIntroducci√≥n# Vivimos en un mundo tan bello como complejo. El enfoque tradicional de la ciencia,que busca generalmente reducir a los sistemas complejos en cada una de sus partes, comprender cada parte, y luego entender todo el sistema, parece no ser suficiente.","tags":["python","sistemas dinamicos","incertidumbre","matematica","caos","fractales"],"title":"Sistemas din√°micos, Complejidad y Caos con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, Analisis de datos y Python. El contenido esta bajo la licencia BSD.\n\u0026ldquo;El cient√≠fico no estudia la naturaleza por la utilidad que le pueda reportar; la estudia por el gozo que le proporciona, y este gozo se debe a la belleza que hay en ella\u0026hellip;\u0026rdquo;\nHenri Poincar√©\n\u0026ldquo;Ni las nubes son esf√©ricas, ni las monta√±as c√≥nicas, ni las costas circulares, ni el tronco de un √°rbol cil√≠ndrico, ni un rayo viajan en l√≠nea recta\u0026hellip;\u0026rdquo;\nBeno√Æt Mandelbrot\nIntroducci√≥n# Para muchas personas, la palabra Geometr√≠a evoca c√≠rculos, cuadrados, cubos y otros objetos regulares o lisos. En nuestra vida cotiana, vemos edificios, muebles o autom√≥viles, que hacen un uso amplio de tales formas. Sin embargo, muchos fen√≥menos en la naturaleza y la ciencia son cualquier cosa menos regulares o suaves. Por ejemplo, un paisaje natural puede incluir arbustos, √°rboles, monta√±as y nubes, que son demasiado complejas para ser representadas por las formas geom√©tricas cl√°sicas.\nSorprendentemente, los objetos aparentemente complejos e irregulares a menudo se pueden describir en t√©rminos notablemente simples. La Geometr√≠a fractal proporciona un marco en el que un proceso simple, que implica una operaci√≥n b√°sica repetida muchas veces, puede dar lugar a un resultado altamente irregular. Las construcciones fractales pueden representar objetos naturales pero tambi√©n dan lugar a una gran variedad de otras formas, que pueden ser de extraordinaria complejidad. A menudo se escucha la frase \u0026ldquo;la belleza de los fractales\u0026rdquo;, una frase que refleja la complejidad interminable de los dise√±os fractales junto con la simplicidad que subyace en su forma siempre repetida.\nEl padre de los Fractales# Beno√Æt Mandelbrot, un cient√≠fico colorido y poco convencional, un matem√°tico polaco nacionalizado franc√©s y estadounidense; es generalmente referido como el \u0026ldquo;padre de los fractales\u0026rdquo; . En su libro de 1982 \u0026ldquo;La geometr√≠a fractal de la naturaleza\u0026rdquo;, argument√≥ que los objetos muy irregulares deben considerarse como algo muy com√∫n, en lugar de excepcional; y adem√°s, muchos fen√≥menos de la F√≠sica, la Biolog√≠a, las Finanzas y las Matem√°ticas tienen irregularidades de siguen un patr√≥n similar. Mandelbrot introdujo la palabra fractal como una descripci√≥n general de una gran clase de objetos irregulares, y destac√≥ la necesidad de desarrollar una matem√°tica fractal, o en algunos casos recuperarla de documentos olvidados.\nDesde la d√©cada de 1980, los fractales han atra√≠do un inter√©s generalizado. Pr√°cticamente todas las √°reas de la ciencia han sido examinadas desde un punto de vista fractal, con la Geometr√≠a fractal convirti√©ndose en un √°rea importante de las Matem√°ticas, tanto como un tema de inter√©s por derecho propio y como una herramienta para una amplia gama de aplicaciones.\nConstruyendo Fractales - La curva de Koch# Para apreciar la belleza de los fractales comencemos construyendo uno de ellos. Imaginemos una l√≠nea recta y divid√°mosla en tres partes iguales; luego borramos la pieza del medio y la reemplazamos por otros dos lados de un tri√°ngulo equil√°tero (es decir, de lados iguales) en la misma base. Esto nos dar√° una cadena de cuatro segmentos m√°s cortos, unidos y en l√≠nea recta como la siguiente figura:\nAhora hagamos exactamente lo mismo con cada una de estas cuatro piezas: quitamos los tercios medios y los reemplazamos por los otros dos lados de los tri√°ngulos equil√°teros en la misma base, para obtener una nueva cadena de 16 piezas rectas. De esta forma, podemos continuar repitiendo este proceso una y otra vez.\nAyud√©mosnos de Python para ver como crece en complejidad y hermosura esta figura al repitir un proceso tan simple una y otra vez.\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # Importando las librer√≠as que vamos a utilizar from fractal import Sierpinski, Vicsek, Tree, Dragon, Koch, Hilbert, Levy koch = Koch() koch.plot() Esta figura que acabamos de construir es conocida como Curva de Koch, y fue introducida y estudiada por el matem√°tico sueco Helge von Koch.\nPropiedades de los Fractales# Analizando esta construcci√≥n, podemos resumir las principales propiedades que exiben la mayor√≠a de los fractales\nEstructura fina: Al ampliar la curva, por mucho que la amplifiquemos, las irregularidades en su forma siempre son evidentes. Esta es una consecuencia directa de la construcci√≥n en la que los segmentos de l√≠nea muy peque√±os se trataron de la misma manera que el original pero en una escala mucho menor.\nAuto-similitud: La curva est√° compuesta de peque√±as copias de s√≠ misma.\nLos m√©todos cl√°sicos de geometr√≠a y matem√°ticas no son aplicables: La figura es demasiado irregular para ser descrito en el lenguaje geom√©trico tradicional y, a diferencia de las formas cl√°sicas, no puede expresarse mediante una f√≥rmula \u0026ldquo;simple\u0026rdquo;.\nEl \u0026ldquo;Tama√±o\u0026rdquo; depende de la escala a la que se mide: A diferencia de figuras tradicionales como el c√≠rculo, d√≥nde podemos tratar de medir la longitud tomando peque√±os tramos y multiplicando por el n√∫mero de pasos; tratar de medir la longitud de la curva de Koch dividi√©ndola en pasos cada vez m√°s cortos solo proporciona estimaciones cada vez mayores para su longitud.\nUna construcci√≥n recursiva simple: Si bien la curva parace un objeto complejo, en realidad se trata de una construcci√≥n recursiva que consiste en aplicar unos pasos simples una y otra vez.\nUna apariencia natural: Con un poco de imaginaci√≥n, la mayor√≠a de los objetos fractales toman formas que recuerdan a la Naturaleza.\nOtros Ejemplos de Fractales# Algunos otros ejemplos de fractales son:\nLa curva de Levy C# Esta curva lleva el nombre por el matem√°tico franc√©s Paul Pierre L√©vy quien, en 1938, fue el primero en exhibir sus propiedades de autosimilaridad y proveer una construcci√≥n geom√©trica.\nlevy = Levy() levy.plot() La curva de Hilber# Este fractal, descripto por David Hilbert es una variante de las curvas de Peanno que recubren todo el plano.\nhilbert = Hilbert() hilbert.plot() La curva del Dargon# Este fractal hizo su aparici√≥n por primera vez en la columna de juegos matematicos de Mart√≠n Gardner en Scientific American.\ndragon = Dragon() dragon.plot() El triangulo de Sierpinski# Propuesto por el matem√°tico polaco Wac≈Çaw Sierpi≈Ñski. Este fractal se puede construir a partir de cualquier tri√°ngulo.\nsierpinski = Sierpinski() sierpinski.plot() El Fractal de Vicsek# Es un fractal propuesto por Tam√°s Vicsek. Es utilizado en el dise√±o de antenas compactas, particularmente para tel√©fonos celulares.\nvicsek = Vicsek() vicsek.plot() Fractales en el mundo real# Los fractales que vimos hasta aqu√≠ viven en el mundo idealizado de las Matem√°ticas, donde es te√≥ricamente posible repetir cada paso de construcci√≥n en forma indefinida, o ver un objeto a escalas arbitrariamente peque√±as. Por supuesto, el \u0026ldquo;mundo real\u0026rdquo; no es as√≠, en la realidad, solo encontramos fractales aproximados. Si nos acercamos demasiado a un objeto real, se perder√° cualquier auto-similitud, y eventualmente nos encontraremos con una estructura molecular o at√≥mica. Sin embargo, puede ser muy √∫til considerar los objetos naturales como fractales si exhiben irregularidades o auto-similitudes cuando se ven en un rango significativo de escalas.\nAlgunos casos de fractales que podemos encontrar en la Madre Naturaleza son:\nCostas y paisajes: Las formas geogr√°ficas, como las costas, los paisajes y los causes de los r√≠os muestran muchas caracter√≠sticas fractales.\nNubes: La forma de las nubes es complicada e irragular, tomando muchas de las caracter√≠sticas de los fractales.\nEl br√≥coli romanesco: Su estructura general est√° compuesta por una serie de conos repetidos a escalas cada vez m√°s peque√±as.\nRamas de los √°rboles: Las ramas de los √°rboles crecen y se bifurcan siguiendo un patr√≥n de autosimilitud como el de los fractales. tree = Tree() tree.plot() Aqu√≠ concluye este fascinante paseo por el mundo de los fractales. Espero que hayan disfrutado del art√≠culo.\nSaludos!\nEste post fue escrito por Ra√∫l e. L√≥pez Briega utilizando Jupyter notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2020-03-24","id":3,"permalink":"/blog/2020/03/24/fractales-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, Analisis de datos y Python. El contenido esta bajo la licencia BSD.\n\u0026ldquo;El cient√≠fico no estudia la naturaleza por la utilidad que le pueda reportar; la estudia por el gozo que le proporciona, y este gozo se debe a la belleza que hay en ella\u0026hellip;\u0026rdquo;\nHenri Poincar√©\n\u0026ldquo;Ni las nubes son esf√©ricas, ni las monta√±as c√≥nicas, ni las costas circulares, ni el tronco de un √°rbol cil√≠ndrico, ni un rayo viajan en l√≠nea recta\u0026hellip;\u0026rdquo;","tags":["python","matematica","fractales","complejidad"],"title":"Fractales con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, Analisis de datos y Python. El contenido esta bajo la licencia BSD.\n\u0026ldquo;Tengo dos problemas para jugar al f√∫tbol. Uno es la pierna izquierda. El otro es la pierna derecha.\u0026rdquo;\nRoberto Fontarrosa\nIntroducci√≥n# Futbol\u0026hellip;Fubol\u0026hellip;o Fuchibol\u0026hellip;sin dudas uno de los deportes m√°s hermosos del mundo y una pasi√≥n de multitudes! Cada vez que la pelota rueda sobre el verde cesped, los corazones se aceleran, el aire se entrecorta y la tensi√≥n se acumula en todos los estadios\u0026hellip;\nQui√©n no ha so√±ado alguna vez con convertirse en un astro del balonpi√©\u0026hellip;para ver luego como ese sue√±o se hace a√±icos y terminar rememorando los viejos tiempos en que las rodillas no dolian, o la panza no hac√≠a de contrapeso y pod√≠amos deslizarnos por el campo eludiendo rivales con la pelota entre los pies!\u0026hellip;O para aquellos que no fuimos tan agraciados con un talento natural y √©ramos m√°s r√∫sticos\u0026hellip;el √©xito resid√≠a en no dejar pasar al rival a como d√© lugar\u0026hellip;aplicando tal vez un par de patadas si la situaci√≥n lo requer√≠a.\nSiempre que comienza una nueva competici√≥n futbol√≠stica, y sobre todo cuando juega la selecci√≥n nacional, que representa los colores de todo un pa√≠s\u0026hellip;la pasi√≥n y el entusiasmo se renuevan para volver a alentar al equipo en la b√∫squeda de la gloria!\nComo sabemos, ya falta muy poco para que comience la Copa Am√©rica, torneo en que las selecciones de sudam√©rica se mediran para determinar quien es el mejor del continente. En Argentina, donde a la mayor√≠a nos encanta discutir sobre futbol y hacer predicciones, cada vez que empieza un torneo como este, solemos jugar al prode\u0026hellip;juego que consiste en predecir los resultados que se van a dar en la competici√≥n en cuesti√≥n.\nY que mejor que aplicar un poco de estad√≠stica y ayudarnos de Python para intentar predecir los resultados y ver que suerte acompa√±ar√° a la gloriosa selecci√≥n albiceleste!\nPredicci√≥n de resultados de la Copa Am√©rica# Para intentar predecir los resultados de la Copa Am√©rica voy a utilizar un modelo simple de Regresi√≥n Log√≠stica que adapt√© de Kaggle; en d√≥nde las principales variales para predecir los resultados ser√°n el ranking FIFA de cada equipo, el talento de sus jugadores y el historial de enfrentamientos.\nLos datos# Los datos que utilic√© los arm√© en base a otros datasets que obtuve de Kaggle. Se los pueden descargar de los siguientes links:\nequipos: Este dataset contiene los datos de los jugadores convocados en cada selecci√≥n.\npartidos: Este dataset contine el historial de partidos en que se enfrentaron las selecciones que participan de la copa.\ncopa: Este dataset contine la composici√≥n de los grupos de la Copa Am√©rica.\nComencemos con el modelo\u0026hellip;\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # Importando las librer√≠as que vamos a utilizar import pandas as pd import numpy as np from pandasql import sqldf import matplotlib.pyplot as plt from sklearn import linear_model from sklearn import ensemble from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score from sklearn.pipeline import Pipeline from sklearn.preprocessing import PolynomialFeatures #utilizar SQL para filtrar pandas df query = lambda q: sqldf(q, globals()) # graficos incrustados %matplotlib inline # importando los datos # jugadores de las selecciones equipos = pd.read_csv(\u0026#39;data/selecciones.csv\u0026#39;) # completando el overall de jugadores faltantes equipos[\u0026#39;Overall\u0026#39;] = equipos[\u0026#39;Overall\u0026#39;].fillna(65) # Historial de partidos entre las selecciones partidos = pd.read_csv(\u0026#39;data/partidos.csv\u0026#39;) # Grupos Copa Am√©rica Brasil 2019 copa = pd.read_csv(\u0026#39;data/CopaAmerica.csv\u0026#39;) Hay quienes dicen que los mundiales de futbol son un gran metegol con el que juegan los Dioses\u0026hellip;aplicando esa analog√≠a yo tom√© las estad√≠sticas de los jugadores del videojuego FIFA19 para determinar el talento general que le corresponde a cada selecci√≥n y ese n√∫mero se ve reflejado en la columna Overal del dataset de equipos\u0026hellip;agreguemos tambi√©n ese dato a los dataset de partidos y de copa para facilitar el acceso a ese dato.\n# Agregando el pontencial del equipo al dataset de partidos potencial = equipos.groupby(\u0026#39;selecci√≥n\u0026#39;).mean()[\u0026#39;Overall\u0026#39;] partidos = partidos.merge(potencial, left_on=[\u0026#39;local\u0026#39;], right_on=[\u0026#39;selecci√≥n\u0026#39;]) partidos = partidos.merge(potencial, left_on=[\u0026#39;visitante\u0026#39;], right_on=[\u0026#39;selecci√≥n\u0026#39;], suffixes=(\u0026#39;_local\u0026#39;, \u0026#39;_visitante\u0026#39;)) # Agregar diferencia de potencial entre los equipos partidos[\u0026#39;dif_potencial\u0026#39;] = partidos[\u0026#39;Overall_local\u0026#39;] - partidos[\u0026#39;Overall_visitante\u0026#39;] # Agregando el pontencial del equipo al dataset de copa copa = copa.merge(potencial, left_on=[\u0026#39;equipo\u0026#39;], right_on=[\u0026#39;selecci√≥n\u0026#39;]) copa = copa.set_index([\u0026#39;equipo\u0026#39;]) # Graficando el potencial de cada seleccion # de acuerdo a la jerarqu√≠a de sus jugadores plot = potencial.sort_values( ascending=False).plot(kind=\u0026#39;bar\u0026#39;) Como vemos en este gr√°fico\u0026hellip;los grandes candidatos de acuerdo a la calidad de sus jugadores son Brasil, Argentina y Uruguay..ahora armemos el modelo\nModelo# # Armado del modelo de regresi√≥n lineal para predecir los resultados # los predictores a utilizar seran la diferencia del ranking fifa y el potencial # de los jugadores de las selecciones X, y = partidos.loc[:,[\u0026#39;ranking_local\u0026#39;, \u0026#39;dif_ranking\u0026#39;, \u0026#39;dif_potencial\u0026#39;]], partidos[\u0026#39;gana_local\u0026#39;] X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42) logreg = linear_model.LogisticRegression(solver=\u0026#39;lbfgs\u0026#39;, C=1e-5) features = PolynomialFeatures(degree=2) model = Pipeline([ (\u0026#39;polynomial_features\u0026#39;, features), (\u0026#39;logistic_regression\u0026#39;, logreg) ]) model = model.fit(X_train, y_train) # Graficos fpr, tpr, _ = roc_curve(y_test, model.predict_proba(X_test)[:,1]) plt.figure(figsize=(15,5)) ax = plt.subplot(1,3,1) ax.plot([0, 1], [0, 1], \u0026#39;k--\u0026#39;) ax.plot(fpr, tpr) ax.set_title(\u0026#39;AUC score es {0:0.2}\u0026#39;.format(roc_auc_score(y_test, model.predict_proba(X_test)[:,1]))) ax.set_aspect(1) ax = plt.subplot(1,3,2) cm = confusion_matrix(y_test, model.predict(X_test)) ax.imshow(cm, cmap=\u0026#39;Blues\u0026#39;, clim = (0, cm.max())) ax.set_xlabel(\u0026#39;Predicci√≥n\u0026#39;) ax.set_title(\u0026#39;Performance en el Test set\u0026#39;) ax = plt.subplot(1,3,3) cm = confusion_matrix(y_train, model.predict(X_train)) ax.imshow(cm, cmap=\u0026#39;Blues\u0026#39;, clim = (0, cm.max())) ax.set_xlabel(\u0026#39;Predicci√≥n\u0026#39;) ax.set_title(\u0026#39;Performance en el Training set\u0026#39;) plot = plt.show() El modelo tiene una precisi√≥n del 72% no es un gran resultado\u0026hellip;pero es decente teniendo en cuenta que los resultados del futbol no se caracterizan por ser f√°ciles de predecir\u0026hellip;si hay algo hermoso que tiene este deporte\u0026hellip;es que siempre te da sorpresas!\ncontinuemos ahora con la simulaci√≥n de la fase de grupos\nSimulaci√≥n de fase de grupos# # Simulando los partidos de la fase de grupos from itertools import combinations # margen de error para definir empate en casos parejos margin = 0.05 copa[\u0026#39;puntos\u0026#39;] = 0 copa[\u0026#39;total_prob\u0026#39;] = 0 for grupo in sorted(set(copa[\u0026#39;grupo\u0026#39;])): print(\u0026#39;___Grupo {}:___\u0026#39;.format(grupo)) for local, visitante in combinations(copa.query(\u0026#39;grupo == \u0026#34;{}\u0026#34;\u0026#39;.format(grupo)).index, 2): print(\u0026#34;{} vs. {}: \u0026#34;.format(local, visitante), end=\u0026#39;\u0026#39;) row = pd.DataFrame(np.array([[np.nan, np.nan, np.nan]]), columns=X_test.columns) row[\u0026#39;ranking_local\u0026#39;] = copa.loc[local, \u0026#39;ranking FIFA\u0026#39;] opp_rank = copa.loc[visitante, \u0026#39;ranking FIFA\u0026#39;] local_pot = copa.loc[local, \u0026#39;Overall\u0026#39;] opp_pot = copa.loc[visitante, \u0026#39;Overall\u0026#39;] row[\u0026#39;dif_ranking\u0026#39;] = row[\u0026#39;ranking_local\u0026#39;] - opp_rank row[\u0026#39;dif_potencial\u0026#39;] = local_pot - opp_pot local_win_prob = model.predict_proba(row)[:,1][0] copa.loc[local, \u0026#39;total_prob\u0026#39;] += local_win_prob copa.loc[visitante, \u0026#39;total_prob\u0026#39;] += 1-local_win_prob # Asignando los puntos en a los equipos en el grupo points = 0 if local_win_prob \u0026lt;= 0.5 - margin: print(\u0026#34;{} gana con prob de {:.3f}\u0026#34;.format(visitante, 1-local_win_prob)) copa.loc[visitante, \u0026#39;puntos\u0026#39;] += 3 if local_win_prob \u0026gt; 0.5 - margin: points = 1 if local_win_prob \u0026gt;= 0.5 + margin: points = 3 copa.loc[local, \u0026#39;puntos\u0026#39;] += 3 print(\u0026#34;{} gana con prob de {:.3f}\u0026#34;.format(local, local_win_prob)) if points == 1: print(\u0026#34;empatan\u0026#34;) copa.loc[local, \u0026#39;puntos\u0026#39;] += 1 copa.loc[visitante, \u0026#39;puntos\u0026#39;] += 1 ___Grupo A:___ Brasil vs. Bolivia: Brasil gana con prob de 0.785 Brasil vs. Venezuela: Brasil gana con prob de 0.622 Brasil vs. Peru: Brasil gana con prob de 0.591 Bolivia vs. Venezuela: Venezuela gana con prob de 0.639 Bolivia vs. Peru: Peru gana con prob de 0.667 Venezuela vs. Peru: empatan ___Grupo B:___ Argentina vs. Colombia: empatan Argentina vs. Paraguay: Argentina gana con prob de 0.637 Argentina vs. Qatar: Argentina gana con prob de 0.765 Colombia vs. Paraguay: Colombia gana con prob de 0.573 Colombia vs. Qatar: Colombia gana con prob de 0.687 Paraguay vs. Qatar: Paraguay gana con prob de 0.655 ___Grupo C:___ Uruguay vs. Ecuador: Uruguay gana con prob de 0.681 Uruguay vs. Japon: Uruguay gana con prob de 0.623 Uruguay vs. Chile: empatan Ecuador vs. Japon: empatan Ecuador vs. Chile: Chile gana con prob de 0.675 Japon vs. Chile: Chile gana con prob de 0.586 # ver posiciones de grupos copa = copa.sort_values(by=[\u0026#39;grupo\u0026#39;, \u0026#39;puntos\u0026#39;, \u0026#39;total_prob\u0026#39;], ascending=[True, False, False]).reset_index() # Seleccionar quienes pasan de ronda next_round = copa.groupby(\u0026#39;grupo\u0026#39;).nth([0, 1, 2]).reset_index() # primeros 3 next_round[[\u0026#39;grupo\u0026#39;,\u0026#39;equipo\u0026#39;, \u0026#39;puntos\u0026#39;]] grupo equipo puntos 0 A Brasil 9 1 A Peru 4 2 A Venezuela 4 3 B Argentina 7 4 B Colombia 7 5 B Paraguay 3 6 C Uruguay 7 7 C Chile 7 8 C Japon 1 Ahora que ya tenemos las predicciones de quienes superar√°n la primera fase y continuar√°n compitiendo para alzar la copa y la gloria\u0026hellip;simulemos la fase final y veamos quien se convierte en campe√≥n!\nSimulando fase final# # Armo los cruces...el peor 3ro queda afuera de cuartos cruces = [0,5,1,4,3,7,6,2] next_round = next_round.loc[cruces] next_round = next_round.set_index(\u0026#39;equipo\u0026#39;) # Simulo rondas finales finales = [\u0026#39;cuartos de final\u0026#39;, \u0026#39;semifinal\u0026#39;, \u0026#39;final\u0026#39;] copa = copa.set_index([\u0026#39;equipo\u0026#39;]) for f in finales: print(\u0026#34;___Ronda {}___\u0026#34;.format(f)) iterations = int(len(next_round) / 2) winners = [] for i in range(iterations): local = next_round.index[i*2] visitante = next_round.index[i*2+1] print(\u0026#34;{} vs. {}: \u0026#34;.format(local, visitante), end=\u0026#39;\u0026#39;) row = pd.DataFrame(np.array([[np.nan, np.nan, np.nan]]), columns=X_test.columns) row[\u0026#39;ranking_local\u0026#39;] = copa.loc[local, \u0026#39;ranking FIFA\u0026#39;] opp_rank = copa.loc[visitante, \u0026#39;ranking FIFA\u0026#39;] local_pot = copa.loc[local, \u0026#39;Overall\u0026#39;] opp_pot = copa.loc[visitante, \u0026#39;Overall\u0026#39;] row[\u0026#39;dif_ranking\u0026#39;] = row[\u0026#39;ranking_local\u0026#39;] - opp_rank row[\u0026#39;dif_potencial\u0026#39;] = local_pot - opp_pot local_win_prob = model.predict_proba(row)[:,1][0] copa.loc[local, \u0026#39;total_prob\u0026#39;] += local_win_prob copa.loc[visitante, \u0026#39;total_prob\u0026#39;] += 1-local_win_prob local_win_prob = model.predict_proba(row)[:,1][0] if model.predict_proba(row)[:,1] \u0026lt;= 0.5: print(\u0026#34;{0} gana con prob de {1:.3f}\u0026#34;.format(visitante, 1-local_win_prob)) winners.append(visitante) else: print(\u0026#34;{0} gana con prob de {1:.3f}\u0026#34;.format(local, local_win_prob)) winners.append(local) next_round = next_round.loc[winners] print(\u0026#34;\\n\u0026#34;) ___Ronda cuartos de final___ Brasil vs. Paraguay: Brasil gana con prob de 0.642 Peru vs. Colombia: Colombia gana con prob de 0.540 Argentina vs. Chile: Argentina gana con prob de 0.532 Uruguay vs. Venezuela: Uruguay gana con prob de 0.591 ___Ronda semifinal___ Brasil vs. Colombia: Brasil gana con prob de 0.541 Argentina vs. Uruguay: Uruguay gana con prob de 0.503 ___Ronda final___ Brasil vs. Uruguay: Brasil gana con prob de 0.512 Si bien el modelo es simple\u0026hellip;y seguramente se puede armar un poco m√°s sofisticado\u0026hellip;los resultados se ven bastante l√≥gicos de acuerdo a la tradici√≥n y el presente futbol√≠stico de cada selecci√≥n.\nComo vemos, de acuerdo a este modelo\u0026hellip;la albiceleste pierde en semifinales en forma re√±ida contra los charr√∫as\u0026hellip;el campe√≥n termina siendo Brasil\u0026hellip;esperemos que los resultados esten errados y en esta ocasi√≥n Messi pueda lograr el tan ansiado t√≠tulo con la selecci√≥n!\nSaludos!\nEste post fue escrito por Ra√∫l e. L√≥pez Briega utilizando Jupyter notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2019-06-09","id":4,"permalink":"/blog/2019/06/09/jugando-la-copa-america-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, Analisis de datos y Python. El contenido esta bajo la licencia BSD.\n\u0026ldquo;Tengo dos problemas para jugar al f√∫tbol. Uno es la pierna izquierda. El otro es la pierna derecha.\u0026rdquo;\nRoberto Fontarrosa\nIntroducci√≥n# Futbol\u0026hellip;Fubol\u0026hellip;o Fuchibol\u0026hellip;sin dudas uno de los deportes m√°s hermosos del mundo y una pasi√≥n de multitudes! Cada vez que la pelota rueda sobre el verde cesped, los corazones se aceleran, el aire se entrecorta y la tensi√≥n se acumula en todos los estadios\u0026hellip;","tags":["python","programacion","analisis de datos","machine learning","matematica","futbol"],"title":"Jugando la Copa Am√©rica con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, Analisis de datos y Python. El contenido esta bajo la licencia BSD.\n\u0026ldquo;S√≥lo hay 10 tipos de personas en el mundo: Los que entienden binario y los que no lo hacen.\u0026rdquo;\nIntroducci√≥n# El sistema de n√∫meros binarios es el m√°s fundamental sistema num√©rico que se utiliza en todas las computadoras. Cualquier dispositivo digital que vemos hoy en d√≠a, desde celulares hasta Smart TVs utilizan el sistema binario! Este sistema sigue reglas muy similares al sistema decimal (el que usamos diariamente) pero en lugar de utilizar una base decimal de 10 n√∫meros, se utiliza una base de 2 n√∫meros, cero y uno. Es decir que los n√∫meros van a ser expresados como una cadena de 1s y 0s.\nUna de las formas m√°s sencillas de entender este sistema de numeraci√≥n, es compar√°ndolo con algo que ya conocemos y que manejamos cotidianamente como es el sistema decimal. Comencemos viendo como convertir los n√∫meros entre estos dos sistemas.\nConvertir un n√∫mero Binario a Decimal# Como en cualquier sistema de numeraci√≥n, en los n√∫meros binarios cada d√≠gito tiene distinto valor dependiendo de la posici√≥n en que se encuentre dentro de la cadena. Para convertir un n√∫mero binario en su equivalente en decimal, solamente debemos conocer las potencias de 2 y luego es cuesti√≥n de contar las posiciones. Algo a tener en cuenta es que al trabajar con binarios los algoritmos en general se aplican de derecha a izquierda. Entonces, vamos a empezar a contar desde la derecha y a cada posici√≥n le vamos a asignar una potencia de 2 empezando por el 0. Por ejemplo, si queremos convertir el 10011000 al sistema decimal, deber√≠amos hacer lo siguiente:\nLe asignamos las potencias a cada posici√≥n y luego multiplicamos los resultados de las potencias por los 1 y 0; por √∫ltimo sumamos los resultados y obtenemos la conversi√≥n en el sistema decimal 128 + 16 + 8 = 152.\nConvertir un n√∫mero Decimal a Binario# Realizar el proceso inverso y convertir un n√∫mero decimal a un n√∫mero binario tambi√©n es bastante sencillo. Como el sistema binario solo tiene 2 alternativas 0 y 1; el n√∫mero 2 se vuelve fundamental. Para convertir un n√∫mero decimal a su equivalente en binario entonces tenemos que comenzar a dividir por 2 hasta obtener restos de 1 y 0. Por ejemplo si queremos convertir el n√∫mero 152, debemos ir dividiendo el mismo por 2 como podemos ver en el siguiente cuadro:\nUna vez que completamos este proceso, simplemente tomamos los restos desde abajo hacia arriba y obtenemos el n√∫mero binario equivalente, que en este caso es 10011000.\nSuma de n√∫meros Binarios# La suma de n√∫meros binarios es bastante f√°cil, solo hay que tener en cuenta la siguiente tabla y luego es un proceso simple y mec√°nico:\nEs decir 0 + 0 = 0; 0 + 1 = 1; 1 + 0 = 1 y 1 + 1 = 10. En este √∫ltimo caso en realidad el resultado de 1 + 1 es 0 y se arrastra un 1 a la izquierda.\nPor ejemplo, para sumar los siguientes n√∫meros: 10011000 (que representa al n√∫mero 152 en el sistema decimal) y 10101 (que representa al n√∫mero 21), deber√≠amos aplicar las reglas del cuadro de la siguiente forma: El proceso siempre se empieza de la derecha hacia la izquierda. Tener en cuenta que en la tercera posici√≥n, el resultado de 0 + 0 es 1 porque se arrastr√≥ el valor de la suma anterior de 1 + 1 = 0. El resultado final que obtuvimos fue 10101101 el cual representa al 173 en el sistema decimal!.\nResta de n√∫meros Binarios# Para la resta de n√∫meros binarios el algoritmo es tambi√©n muy f√°cil y similar al que se utiliza en el sistema decimal. El cuadro a tener en cuenta en este caso es:\nEs decir 0 - 0 = 0; 1 - 0 = 1; 1 - 1 = 0 y 0 - 1 = 1. En este √∫ltimo caso en realidad como no se puede restar 1 a 0, se pide prestado una unidad de la posici√≥n siguiente como hacemos en la resta en el sistema decimal.\nPor ejemplo, para restar los siguientes n√∫meros: 11001001 (que representa al 201 en decimales) y 1000011 (que representa al 67), deber√≠amos aplicar las reglas del cuadro de la siguiente forma:\nNuevamente el algoritmo se aplica de derecha a izquierda, comenzamos con 1 ‚Äì 1 que da 0, luego 0 ‚Äì 1 que da 1 y arrastramos a 1 a la izquierda, por lo que la siguiente columna vuelve a ser 0 ‚Äì 1 que da 1, como volvimos a arrastrar la siguiente es 1 ‚Äì 1 igual a 0 y despu√©s ya es m√°s simple porque ya no tenemos arrastres. El resultado final es 10000110 que representa al n√∫mero 134 en el sistema decimal!\nMultiplicaci√≥n de n√∫meros Binarios# El algoritmo de multiplicaci√≥n es similar al que utilizamos en el sistema decimal pero es mucho m√°s sencillo ya que solo tenemos que multiplicar por 1 y 0! La tabla que utilizamos para los c√°lculos es la siguiente:\nPara poder realizar la multiplicaci√≥n al igual que como ocurre con el sistema decimal, tenemos que saber sumar. Por ejemplo, si queremos multiplicar 10110 (22 en el sistema decimal) por 1001 ( que representa al 9) el proceso ser√≠a el siguiente:\nVamos multiplicando d√≠gito a d√≠gito y luego sumamos los resultados; el resultado final es 11000110 que equivale a 198 en el sistema decimal.\nDivisi√≥n de n√∫meros Binarios# El algoritmo de divisi√≥n de n√∫meros binarios es el mismo que se utiliza para el sistema decimal con la salvedad que solo podemos trabajar con 1s y 0s y que las restas y multiplicaciones se deben realizar tambi√©n en binario.\nSi por ejemplo queremos dividir 101010 (que representa al 42 en el sistema decimal) entre 110 (que representa al 6); el proceso ser√≠a el siguiente:\nTomamos los primeros 3 d√≠gitos 101 como 101 (5 en decimal) es m√°s chico que 110 (6 en decimal) no nos sirve, entonces tomamos un d√≠gito m√°s 1010. Como es m√°s grande entonces entra 1 vez en 110, por lo que ya tenemos el primer d√≠gito del resultado. Multiplicamos 1 por 110 y lo restamos a 1010, el resultado es 100, como 100 es menor que 110 bajamos el siguiente d√≠gito, por lo que nos queda 1001 para dividir entre 110.\nComo 1001 es mayor que 110, volvemos a aplicar el procedimiento y restar 110.\nComo 110 entra una vez en 110, otra vez obtenemos 1 y volvemos a aplicar el procedimiento.\nDe esta forma arribamos al resultado final de 111 ( 7 en el sistema decimal).\nComo la divisi√≥n es un poco m√°s dif√≠cil que las otras operaciones, realicemos un ejemplo m√°s, en este caso vamos a dividir 101000 (40 en decimal) entre 1000 (8 en decimal).\nPartimos de los primeros 4 d√≠gitos, como 1010 es mayor que 1000, el primer d√≠gito de nuestro resultado es 1.\nLuego de restar 1000 a 1010, el resultado es 10, como es m√°s chico que 1000 bajamos el siguiente d√≠gito; como 100 sigue siendo m√°s chico que 1000; el segundo d√≠gito de nuestro resultado es 0.\nContinuamos aplicando el mismo procedimiento; como luego de la resta, 100 sigue siendo m√°s chico que 1000, bajamos el siguiente d√≠gito. Ahora como 1000 entra una vez en 1000, podemos obtener el siguiente d√≠gito del resultado y arribar al resultado final!\nComo vemos, el resultado final es 101 que equivale a 5 en el sistema decimal.\nN√∫meros Binarios con Python# Ahora que ya conocemos como trabajar con el sistema de n√∫meros binarios en forma manual; podemos simplificar las cosas ayud√°ndonos de Python y su interprete interactivo. En Python podemos convertir un n√∫mero decimal en su equivalente en binario utilizando la funci√≥n bin()\n# convertir decimal a binario bin(152) '0b10011000' # Convertir binario a decimal int(0b10011000) 152 # suma de binarios bin(0b10011000 + 0b10101) '0b10101101' # resta de binarios bin(0b11001001 - 0b1000011) '0b10000110' # multiplicacion bin(0b10110 * 0b1001) '0b11000110' # divisi√≥n bin(0b101010 // 0b110) '0b111' # divisi√≥n bin(0b101000 // 0b1000) '0b101' Como vemos, las operaciones entre los sistema de numeraci√≥n son equivalentes, ya que lo que cambia es la forma en que representamos los n√∫meros y no la esencia de la operaci√≥n.\nAqu√≠ concluye este art√≠culo, como podemos ver el sistema de numeraci√≥n binario posee una simpleza muy bella que lo hace extremadamente √∫til; con tan solo dos estados\u0026hellip;uno y cero, podemos representar cosas extremadamente complejas como son todos los productos de la revoluci√≥n digital que utilizamos hoy en d√≠a!\nEste post fue escrito por Ra√∫l e. L√≥pez Briega utilizando Jupyter notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2019-03-09","id":5,"permalink":"/blog/2019/03/09/el-sistema-de-numeracion-binario/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, Analisis de datos y Python. El contenido esta bajo la licencia BSD.\n\u0026ldquo;S√≥lo hay 10 tipos de personas en el mundo: Los que entienden binario y los que no lo hacen.\u0026rdquo;\nIntroducci√≥n# El sistema de n√∫meros binarios es el m√°s fundamental sistema num√©rico que se utiliza en todas las computadoras. Cualquier dispositivo digital que vemos hoy en d√≠a, desde celulares hasta Smart TVs utilizan el sistema binario!","tags":["python","matematica","binario","numeros","decimales"],"title":"El sistema de numeraci√≥n Binario"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\n\u0026ldquo;La informaci√≥n es la resoluci√≥n de la incertidumbre\u0026rdquo;\nClaude Shannon\n\u0026ldquo;Lo que est√° en el coraz√≥n de cada ser vivo no es un fuego, ni un aliento c√°lido, ni una chispa de vida. Es informaci√≥n, palabras, instrucciones \u0026hellip; Si quieres entender la vida\u0026hellip; piensa en la tecnolog√≠a de la informaci√≥n\u0026rdquo;\nRichard Dawkins\nIntroducci√≥n# Muchas veces hemos escuchado decir que vivimos en la era de la informaci√≥n. La informaci√≥n parece estar en todo los que nos rodea. Ya sea que consideremos a las computadoras, la evoluci√≥n, la f√≠sica, la inteligencia artificial, o nuestro cerebro; podemos llegar a la conclusi√≥n de que su comportamiento esta principalmente determinado por la forma en que procesan la informaci√≥n.\nLa idea de la informaci√≥n naci√≥ del antiguo arte de la codificaci√≥n y decodificaci√≥n de c√≥digos. Los encargados de esconder los secretos de estado durante la segunda guerra mundial utilizaban, en esencia, m√©todos para ocultar informaci√≥n y transmitirla de un lugar a otro. Cuando el arte de quebrar estos c√≥digos se combin√≥ con la ciencia de la Termodin√°mica, la rama de la f√≠sica encargada del estudio de la interacci√≥n entre el calor y otras manifestaciones de la energ√≠a; surgi√≥ lo que hoy conocemos como Teor√≠a de la informaci√≥n. Esta teor√≠a fue una idea revolucionaria que inmediatamente transform√≥ el campo de las comunicaciones y prepar√≥ el camino para la era de las computadoras. Pero las ideas de la Teor√≠a de la informaci√≥n no solo gobiernan las comunicaciones y los bits y bytes de las computadoras modernas; sino que tambi√©n describen el comportamiento del mundo subat√≥mico, e incluso de toda la vida en la Tierra.\n¬øQu√© es la informaci√≥n?# Hasta no hace no tanto tiempo atr√°s, nuestro conocimiento de la informaci√≥n era bastante vago y limitado. En 1948, Claude Shannon public√≥ un art√≠culo titulado \u0026ldquo;Una teor√≠a matem√°tica de la comunicaci√≥n\u0026rdquo;, el cual transform√≥ para siempre la forma en que entendemos la informaci√≥n. La Teor√≠a de la informaci√≥n de Shannon proporciona una definici√≥n matem√°tica de informaci√≥n y describe con precisi√≥n cu√°nta informaci√≥n se puede comunicar entre los diferentes elementos de un sistema. La teor√≠a de Shannon respalda nuestra comprensi√≥n de c√≥mo se relacionan las se√±ales y el ruido, y por qu√© existen l√≠mites definidos para la velocidad a la que se puede comunicar la informaci√≥n dentro de cualquier sistema, ya sea creado por el hombre o biol√≥gico. La habilidad de separar la se√±al del ruido, para extraer la informaci√≥n en los datos, se ha vuelto crucial en las telecomunicaciones modernas.\nLa Teor√≠a de la informaci√≥n es tan poderosa porque la informaci√≥n es f√≠sica. La informaci√≥n no es solo un concepto abstracto, y no solo son hechos o figuras, fechas o nombres. Es una propiedad concreta de la materia y la energ√≠a que es cuantificable y mensurable. Es tan real como el peso de un trozo de plomo o la energ√≠a almacenada en una ojiva at√≥mica, y al igual que la masa y la energ√≠a, la informaci√≥n est√° sujeta a un conjunto de leyes f√≠sicas que dictan c√≥mo puede comportarse, c√≥mo la informaci√≥n puede ser manipulada, transferida, duplicada, borrada o destruida. Y todo en el universo debe obedecer las leyes de la informaci√≥n, porque todo en el universo est√° formado por la informaci√≥n que contiene.\nSeg√∫n la perspectiva de la informaci√≥n de Shannon, el significado no es importante, sino que lo que importa es cu√°nta informaci√≥n es transmitida por un mensaje. Una de las grandes intuiciones que tuvo Shannon fue darse cuenta que cualquier pregunta que tenga una respuesta finita puede ser respondida por una cadena de preguntas por s√≠ o por no. As√≠ es como surge el concepto de Bit.\nBits# Un Bit es la unidad fundamental en la que podemos medir la informaci√≥n y nos permite decidir entre dos alternativas igualmente probables. La palabra Bit deriva de binary digit, o sea d√≠gito binario, los cuales son representados por 1s y 0s. Pero si bien la palabra Bit deriva de binary digit no debemos confundirlos, ya que representan entidades distintas. Un Bit representa una cantidad de informaci√≥n definitiva. En cambio, un d√≠gito binario es el valor de una variable binaria, el cual, como ya dijimos, puede ser 0 o 1; pero un d√≠gito binario no representa informaci√≥n en s√≠ misma.\nAhora bien, volviendo a las preguntas por s√≠ o por no que mencionamos antes; responder cada una de estas preguntas requiere un Bit de informaci√≥n. S√≥lo necesitamos un Bit para responder una pregunta como ¬øsos un hombre o una mujer?; el 0 puede significar hombre y el 1 mujer. Con simplemente transmitir ese d√≠gito en el mensaje, estamos transmitiendo la respuesta. Pero aqu√≠ viene otra de las grandes intuiciones de Shannon; tampoco importa la forma que tome el mensaje, puede ser una luz roja versus una luz verde; o una bandera blanca y otra roja; realmente no importa el medio que se utilice, el mensaje siempre contiene un Bit de informaci√≥n.\nY ¬øqu√© pasa con otro tipo de preguntas? preguntas como adivinar un n√∫mero entero entre 1 y 1000, o como, ¬øcu√°l es la capital de Islandia? Estas preguntas tambi√©n pueden ser respondidas con una cadena de Bits. El lenguaje no es m√°s que una cadena de s√≠mbolos y cualquier s√≠mbolo puede ser representado con una cadena de Bits. Por lo tanto, cualquier respuesta que pueda ser escrita en un lenguaje puede ser representada con una cadena de Bits, de 1s y 0s. Los Bits son el medio fundamental de la informaci√≥n.\nEsta realizaci√≥n, que cualquier informaci√≥n, cualquier respuesta, puede ser codificada en una cadena de Bits, nos abre la puerta para pensar que entonces debe existir una forma de medir cu√°nta informaci√≥n hay en un mensaje.¬øCu√°l es la m√≠nima cantidad de Bits para codificar un mensaje? Por ejemplo, para responder la pregunta planteada anteriormente de adivinar un n√∫mero entero entre 1 y 1000, no se necesitan m√°s que 10 Bits!. Shannon encontr√≥ que una pregunta con \\(N\\) posibles resultados puede ser respondida con una cadena de Bits de \\(log_2 N\\) Bits; es decir que solo necesitamos \\(log_2 N\\) Bits de informaci√≥n para distinguir entre \\(N\\) posibilidades. Si no me creen, m√°s abajo les dejo un bot√≥n para jugar a adivinar el n√∫mero. (Si les consume m√°s de 10 bits llegar a la respuesta correcta, no est√°n utilizando la estrategia correcta!).\nTodo esto √∫ltimo relacionado a c√≥mo medir cu√°nta informaci√≥n contiene un mensaje nos lleva a otro de los conceptos fundamentales de la Teor√≠a de la informaci√≥n, el concepto de Entrop√≠a.\nJugar a Adivinar el n√∫mero! Entrop√≠a# La idea central de la Teor√≠a de la informaci√≥n de Shannon es la Entrop√≠a. La informaci√≥n y la Entrop√≠a est√°n intimimamente relacionadas, ya que esta √∫ltima es en s√≠ misma una medida de informaci√≥n. Cuando Shannon comenz√≥ a desarrollar su teor√≠a, encontr√≥ una formula que le permit√≠a analizar la informaci√≥n en un mensaje en t√©rminos de Bits. Esta formula que encontr√≥ mide, a grandes rasgos, cuan poco predecible es una cadena de Bits. Mientras menos predecible, existen menos probabilidades de poder generar el mensaje completo desde una cadena m√°s peque√±a de Bits. Es decir, que al intentar medir cuan poco predecible es una cadena de Bits, Shannon esperaba poder encontrar cu√°nta informaci√≥n conten√≠a el mensaje. Ahora bien, ¬øcu√°l es la cadena de 0s y 1s menos probables? Pues aquella que sea completamente aleatoria, como cuando lanzamos una moneda al aire y tenemos 50% de probabilidades de obtener cara o seca. Mientras m√°s aleatoria es una cadena de s√≠mbolos, es menos predecible y menos redundante; y por lo tiende a contener una mayor cantidad de informaci√≥n por s√≠mbolo. Si bien esto parece una paradoja, ¬øc√≥mo algo que es completamente aleatorio contiene m√°s informaci√≥n que algo que no lo es? Acoso, ¬ølo aleatorio no es no lo contrario de informaci√≥n? Parece ser contra intuitivo, pero en realidad no lo es. Se puede observar fac√≠lmente con un ejemplo. Supongamos que arrojamos una moneda al aire 16 veces y representamos a la cara con un 1 y a la seca con un 0. Podemos obtener una cadena como la siguiente: 1101001110001011. Esta cadena es aleatoria y por lo tanto no podemos encontrar ning√∫n patr√≥n en ella que nos diga cu√°l va a ser el pr√≥ximo valor que podemos obtener m√°s alla de la chance de 50% habitual, por tal motivo, no podemos comprimir la cadena y cada s√≠mbolo contiene un Bit de informaci√≥n. Ahora supongamos que la moneda esta sesgada y que siempre sale cara; en este caso nuestra cadena ser√° la siguiente: 1111111111111111. Esta cadena es sumamente predecible, no nos aporta ninguna sorpresa y tenemos una probabilidad de 100% de adivinar que el siguiente d√≠gito tambi√©n ser√° un 1. Es totalmente redundante y por lo tanto no nos aporta ninguna informaci√≥n. Cada s√≠mbolo contiene un Bit de informaci√≥n. Sin sorpresa, no hay informaci√≥n.\nLa f√≥rmula matem√°tica que encontr√≥ Shannon para medir la Entrop√≠a de un mensaje es muy similar a la que se utiliza en Termodin√°mica para medir el grado de desorden de un sistema. Es la siguiente:\n$$ H(x) = - \\sum_{i} p(i) log_2 p(i) $$ Cuando Shannon se dio cuenta de que la Entrop√≠a de una secuencia de s√≠mbolos estaba relacionada con la cantidad de informaci√≥n que la cadena de s√≠mbolos tiende a contener, de repente tuvo una herramienta para cuantificar la informaci√≥n y la redundancia en un mensaje. Fue capaz de demostrar, matem√°ticamente, cu√°nta informaci√≥n puede transmitirse en cualquier medio y que existe un l√≠mite fundamental para la cantidad de informaci√≥n que puede transmitir con un equipo determinado.\nVeamos algunos ejemplos de como calcular la Entrop√≠a con la ayuda de Python:\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; import matplotlib.pyplot as plt import numpy as np import warnings # ingnorar mensajes de advertencias en el notebook warnings.filterwarnings(\u0026#39;ignore\u0026#39;) # graficos en el notebook %matplotlib inline def entropia(X): \u0026#34;\u0026#34;\u0026#34;Devuelve el valor de entropia de una muestra de datos\u0026#34;\u0026#34;\u0026#34; probs = [np.mean(X == valor) for valor in set(X)] return round(np.sum(-p * np.log2(p) for p in probs), 3) def entropia_prob_pq(x): \u0026#34;\u0026#34;\u0026#34;Devuelve la entropia de una probabilidad de dos posibilidades\u0026#34;\u0026#34;\u0026#34; return round((-x * np.log2(x)) + (-(1 - x ) * np.log2((1 - x))), 3) def entropia_posibilidades(x): \u0026#34;\u0026#34;\u0026#34;Devuelve la entrop√≠a para la cantidad de posibilidades independientes x\u0026#34;\u0026#34;\u0026#34; return round(np.log2(x), 3) # Graficando la informaci√≥n como sorpresa # Mientras menos probable, m√°s sorpresa y m√°s informaci√≥n contiene. vent = np.vectorize(entropia_posibilidades) X = np.linspace(0, 1, 11) plt.plot(X, vent(X)*-1) plt.title(\u0026#34;Informaci√≥n como sorpresa\u0026#34;) plt.grid(color=\u0026#39;b\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=.3) plt.xlabel(r\u0026#39;Probabilidades $p(x)$\u0026#39;) plt.ylabel(r\u0026#39;sorpresa $H(x) = log_2 1/p(x)$\u0026#39;) plt.show() # Graficando la entropia en el caso de 2 posibilidades con # probabilidad p y (1- p) # vectorizar la funci√≥n para poder pasarle un vector de par√°metro vent = np.vectorize(entropia_prob_pq) X = np.linspace(0, 1, 11) plt.plot(X, vent(X)) plt.title(\u0026#34;Entropia para 2 posibilidades con probabilidad p\u0026#34;) plt.grid(color=\u0026#39;b\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=.3) plt.xlabel(\u0026#39;Probabilidades p\u0026#39;) plt.ylabel(\u0026#39;Bits\u0026#39;) plt.show() # La entropia de una muestra de 2 posibilidades completamente # aleatorias, en la que cualquiera de los 2 valores tiene la # misma probabilidad (p=0.5) de ser seleccionada es de 1 bit # Muestra de 10000 valores aleatorios entre 0 y 1 X = np.random.randint(0, 2, size=10000) entropia(X), entropia_posibilidades(2) (1.0, 1.0) # La entropia de una muestra de 8 posibilidades completamente # aleatorias es igual a 3 bits. # Muestra de 10000 valores aleatorios entre 0 y 7 X = np.random.randint(0, 8, size=10000) entropia(X), entropia_posibilidades(8) (3.0, 3.0) Redundancia# Otro de los conceptos fundamentales de la Teor√≠a de la informaci√≥n es el de Redundancia. La Redundancia son esas pistas adicionales en una sentencia o mensaje que nos permiten entender su significado incluso si el mensaje esta incompleto o distorsionado; son esos caracteres extra en una cadena de s√≠mbolos, la parte predecible que nos permite completar la informaci√≥n faltante. Cualquier sentencia de cualquier lenguaje es altamente redundante. Todo sentencia nos proporciona informaci√≥n adicional para que podemos descifrarla. Esta Redundancia es f√°cil de ver, simplemente tr-t- d- l\u0026ndash;r -st- m-ns-j-. A pesar de que quitemos todas la vocales, igualmente se podemos entender la sentencia.\nPara nosotros, la redundancia del lenguaje es algo bueno, porque hace que un mensaje sea m√°s f√°cil de comprender incluso cuando el mensaje est√° parcialmente modificado por el entorno. Podemos entender a un amigo hablando en un restaurante abarrotado de gente o hablando con un tel√©fono celular con mucha est√°tica gracias a la Redundancia . La Redundancia es un mecanismo de seguridad; nos asegura que el mensaje se transmita incluso si se da√±a levemente en el camino. Todos los idiomas tienen estas redes de seguridad integradas compuestas de patrones, estructuras y un conjunto de reglas que los hacen redundantes. Usualmente no estamos al tanto de esas reglas, pero nuestro cerebro las usa inconscientemente mientras leemos, hablamos, escuchamos y escribimos.\nCuando eliminamos toda la redundancia en una cadena de s√≠mbolos, lo que queda es su n√∫cleo concreto y cuantificable. Eso es la informaci√≥n, ese algo central e irreductible que se encuentra en la esencia de toda sentencia.\nPara explorar en carne propia como la informaci√≥n es una medida de sorpresa y como la mayor√≠a de los mensajes contienen bastantes Bits redundantes, les dejo otro juego; la idea es adivinar nombres que empiezan con \u0026ldquo;R\u0026rdquo; de Ra√∫l a medida que se van descubriendo nuevas letras. Les garantizo que podr√°n descubrir los nombres sin tener que llegar que se descubra la √∫ltima letra!\nJugar a Adivinar el nombre! Informaci√≥n e incertidumbre# Nuestra experiencia del mundo nos lleva a concluir que muchos eventos son impredecibles y algunas veces bastante inesperados. Estos pueden variar desde el resultado de simples juegos de azar como arrojar una moneda e intentar adivinar si ser√° cara o cruz, al colapso repentino de los gobiernos, o la ca√≠da dram√°tica de los precios de las acciones en el mercado burs√°til. Cuando tratamos de interpretar tales eventos, es probable que tomemos uno de dos enfoques: nos encogeremos de hombros y diremos que fue por casualidad o argumentaremos que podr√≠amos haber sido m√°s capaces de predecir, por ejemplo, el colapso del gobierno si hubi√©ramos tenido m√°s informaci√≥n sobre las acciones de ciertos ministros. En cierto sentido, podemos decir que estos dos conceptos de informaci√≥n e incertidumbre est√°n m√°s estrechamente relacionados de lo que podr√≠amos pensar. De hecho, cuando nos enfrentamos a la incertidumbre, nuestra tendencia natural es buscar informaci√≥n que nos ayude a reducir esa incertidumbre en nuestras mentes. Las herramientas que nos proporciona la Teor√≠a de la informaci√≥n est√°n en las bases de todos los modelos que desarrollamos para intentar predecir y lidiar con la incertidumbre del futuro.\nAqu√≠ concluye esta introducci√≥n, el trabajo de Shannon habri√≥ un campo enorme del conocimiento cient√≠fico. Por a√±os, cript√≥grafos hab√≠an intentado esconder informaci√≥n y reducir la redundancia sin siquiera saber como medirlas; o los ingenieros trataron de dise√±ar maneras eficientes de transmitir mensajes sin conocer los l√≠mites que la Naturaleza pon√≠a a su eficiencia. La Teor√≠a de la informaci√≥n de Shannon revolucion√≥ la criptograf√≠a, el procesamiento de se√±ales, las ciencias de la computaci√≥n, la f√≠sica, y un gran n√∫mero de otros campos.\nPara cerrar, les dejo la implementaci√≥n de los juegos utilizados en el art√≠culo utilizando Python! :)\nSaludos!\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; import random random.seed(1982) def adivinar_numero(): mi_numero = random.randint(1, 1000) bits = 1 tu_numero = int(input(\u0026#34;Adivine un n√∫mero entero entre 1 y 1000\\nIngrese un n√∫mero entre 1 y 1000: \u0026#34;)) while tu_numero != mi_numero: if tu_numero \u0026lt; mi_numero: tu_numero = int(input(\u0026#34;Su n√∫mero es muy bajo!\\nIngrese otro n√∫mero entre 1 y 1000:\u0026#34;)) else: tu_numero = int(input(\u0026#34;Su n√∫mero es muy alto!\\nIngrese otro n√∫mero entre 1 y 1000:\u0026#34;)) bits += 1 print(\u0026#34;Felicidades el n√∫mero es {0} y ha utilizado {1} bits!\u0026#34;.format(mi_numero, bits)) Ver C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; def adivinar_nombre(): nombres = [ \u0026#34;ramses\u0026#34;, \u0026#34;rodolfo\u0026#34;, \u0026#34;regina\u0026#34;, \u0026#34;ruth\u0026#34;, \u0026#34;ramiro\u0026#34;, \u0026#34;ramon\u0026#34;, \u0026#34;roxana\u0026#34;, \u0026#34;rebeca\u0026#34;, \u0026#34;raquel\u0026#34;, \u0026#34;ruben\u0026#34;, \u0026#34;rosario\u0026#34;, \u0026#34;renata\u0026#34;, \u0026#34;raul\u0026#34;, \u0026#34;romina\u0026#34;, \u0026#34;roberto\u0026#34;, \u0026#34;ricardo\u0026#34;, \u0026#34;rafael\u0026#34;, \u0026#34;rosa\u0026#34;, \u0026#34;rodrigo\u0026#34;, \u0026#34;rocio\u0026#34; ] index = random.randint(0, 19) mi_nombre = nombres[index] tu_nombre = input(\u0026#34;Adivina el nombre! Empieza con R y tiene {} letras: \u0026#34;.format(len(mi_nombre))) letras = 2 bits = 1 while tu_nombre.lower() != mi_nombre: mi_nombre_parcial = mi_nombre[:letras] if mi_nombre_parcial == mi_nombre: break tu_nombre = input(\u0026#34;Int√©ntalo otra vez! Empieza con {0} y tiene {1} letras:\u0026#34;.format(mi_nombre_parcial, letras)) bits += 1 letras += 1 print(\u0026#34;El nombre es {0} y has utilizado {1} bits! Los restantes {2} son redundantes!\u0026#34;.format(mi_nombre.upper(), bits, len(mi_nombre) - bits)) adivinar_numero() adivinar_nombre() Este post fue escrito utilizando Jupyter notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2018-03-30","id":6,"permalink":"/blog/2018/03/30/introduccion-a-la-teoria-de-la-informacion-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\n\u0026ldquo;La informaci√≥n es la resoluci√≥n de la incertidumbre\u0026rdquo;\nClaude Shannon\n\u0026ldquo;Lo que est√° en el coraz√≥n de cada ser vivo no es un fuego, ni un aliento c√°lido, ni una chispa de vida. Es informaci√≥n, palabras, instrucciones \u0026hellip; Si quieres entender la vida\u0026hellip; piensa en la tecnolog√≠a de la informaci√≥n\u0026rdquo;","tags":["informacion","matematica","incertidumbre","entropia","redundancia","bit"],"title":"Introducci√≥n a la Teor√≠a de la informaci√≥n con python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, Analisis de datos y Python. El contenido esta bajo la licencia BSD.\n\u0026ldquo;El lenguaje sirve no s√≥lo para expresar el pensamiento, sino para hacer posibles pensamientos que no podr√≠an existir sin √©l.\u0026rdquo;\nBertrand Russell\nIntroducci√≥n# El lenguaje es una de las herramientas centrales en nuestra vida social y profesional. Entre otras cosas, act√∫a como un medio para transmitir ideas, informaci√≥n, opiniones y sentimientos; as√≠ como para persuadir, pedir informaci√≥n, o dar ordenes. Asimismo, el lenguaje humano es algo que esta en constante cambio y evoluci√≥n; y que puede llegar a ser muy ambiguo y variable. Tomemos por ejemplo la frase \u0026ldquo;com√≠ una pizza con amigos\u0026rdquo; comparada con \u0026ldquo;com√≠ una pizza con aceitunas\u0026rdquo;; su estructura es la misma, pero su significado es totalmente distinto. De la misma manera, un mismo mensaje puede ser expresado de formas diferentes; \u0026ldquo;com√≠ una pizza con amigos\u0026rdquo; puede tambi√©n ser expresado como \u0026ldquo;compart√≠ una pizza con amigos\u0026rdquo;.\nLos seres humanos somos muy buenos a la hora de producir e interpretar el lenguaje, podemos expresar, percibir e interpretar significados muy elaborados en fracci√≥n de segundos casi sin dificultades; pero al mismo tiempo, somos tambi√©n muy malos a la hora de entender y describir formalmente las reglas que lo gobiernan. Por este motivo, entender y producir el lenguaje por medio de una computadora es un problema muy dif√≠cil de resolver. √âste problema, es el campo de estudio de lo que en inteligencia artificial se conoce como Procesamiento del Lenguaje Natural o NLP por sus siglas en ingl√©s.\n¬øQu√© es el Procesamiento del Lenguaje Natural?# El Procesamiento del Lenguaje Natural o NLP es una disciplina que se encuentra en la intersecci√≥n de varias ciencias, tales como las Ciencias de la Computaci√≥n, la Inteligencia Artificial y Psicolog√≠a Cognitiva. Su idea central es la de darle a las m√°quinas la capacidad de leer y comprender los idiomas que hablamos los humanos. La investigaci√≥n del Procesamiento del Lenguaje Natural tiene como objetivo responder a la pregunta de c√≥mo las personas son capaces de comprender el significado de una oraci√≥n oral / escrita y c√≥mo las personas entienden lo que sucedi√≥, cu√°ndo y d√≥nde sucedi√≥; y las diferencias entre una suposici√≥n, una creencia o un hecho.\nEn general, en Procesamiento del Lenguaje Natural se utilizan seis niveles de comprensi√≥n con el objetivo de descubrir el significado del discurso. Estos niveles son:\nNivel fon√©tico: Aqu√≠ se presta atenci√≥n a la fon√©tica, la forma en que las palabras son pronunciadas. Este nivel es importante cuando procesamos la palabra hablada, no as√≠ cuando trabajamos con texto escrito.\nNivel morfol√≥gico: Aqu√≠ nos interesa realizar un an√°lisis morfol√≥gico del discurso; estudiar la estructura de las palabras para delimitarlas y clasificarlas.\nNivel sint√°ctico: Aqu√≠ se realiza un an√°lisis de sintaxis, el cual incluye la acci√≥n de dividir una oraci√≥n en cada uno de sus componentes.\nNivel sem√°ntico: Este nivel es un complemente del anterior, en el an√°lisis sem√°ntico se busca entender el significado de la oraci√≥n. Las palabras pueden tener m√∫ltiples significados, la idea es identificar el significado apropiado por medio del contexto de la oraci√≥n.\nNivel discursivo: El nivel discursivo examina el significado de la oraci√≥n en relaci√≥n a otra oraci√≥n en el texto o p√°rrafo del mismo documento.\nNivel pragm√°tico: Este nivel se ocupa del an√°lisis de oraciones y c√≥mo se usan en diferentes situaciones. Adem√°s, tambi√©n c√≥mo su significado cambia dependiendo de la situaci√≥n.\nTodos los niveles descritos aqu√≠ son inseparables y se complementan entre s√≠. El objetivo de los sistemas de NLP es incluir estas definiciones en una computadora y luego usarlas para crear una oraci√≥n estructurada y sin ambig√ºedades con un significado bien definido.\nAplicaciones del Procesamiento del Lenguaje Natural# Los algoritmos de Procesamiento del Lenguaje Natural suelen basarse en algoritmos de aprendizaje autom√°tico. En lugar de codificar manualmente grandes conjuntos de reglas, el NLP puede confiar en el aprendizaje autom√°tico para aprender estas reglas autom√°ticamente analizando un conjunto de ejemplos y haciendo una inferencia estad√≠stica. En general, cuanto m√°s datos analizados, m√°s preciso ser√° el modelo. Estos algoritmos pueden ser utilizados en algunas de las siguientes aplicaciones:\nResumir texto: Podemos utilizar los modelos de NLP para extraer las ideas m√°s importantes y centrales mientras ignoramos la informaci√≥n irrelevante.\nCrear chatbots: Podemos utilizar las t√©cnicas de NLP para crear chatbots que puedan interactuar con las personas.\nGenerar autom√°ticamente etiquetas de palabras clave: Con NLP tambi√©n podemos realizar un an√°lisis de contenido aprovechando el algoritmo de LDA para asignar palabras claves a p√°rrafos del texto.\nReconocer entidades: Con NLP podemos identificar a las distintas entidades del texto como ser una persona, lugar u organizaci√≥n.\nAn√°lisis de sentimiento: Tambi√©n podemos utilizar NLP para identificar el sentimiento de una cadena de texto, desde muy negativo a neutral y a muy positivo.\nLibrer√≠as de Python para Procesamiento del Lenguaje Natural# Actualmente, Python es uno de los lenguajes m√°s populares para trabajar en el campo la Inteligencia Artificial. Para abordar los problemas relacionados con el Procesamiento del Lenguaje Natural Python nos proporciona las siguientes librer√≠as:\nNLTK: Es la librer√≠a l√≠der para el Procesamiento del Lenguaje Natural. Proporciona interfaces f√°ciles de usar a m√°s de 50 corpus y recursos l√©xicos, junto con un conjunto de bibliotecas de procesamiento de texto para la clasificaci√≥n, tokenizaci√≥n, el etiquetado, el an√°lisis y el razonamiento sem√°ntico.\nTextBlob: TextBlob simplifica el procesamiento de texto proporcionando una interfaz intuitiva a NLTK. Posee una suave curva de aprendizaje al mismo tiempo que cuenta con una sorprendente cantidad de funcionalidades.\nStanford CoreNLP: Paquete desarrollado por la universidad de Stanford, para muchos constituye el estado del arte sobre las t√©cnicas tradicionales de Procesamiento del Lenguaje Natural. Si bien esta escrita en Java, posee una interface con Python.\nSpacy: Es una librer√≠a relativamente nueva que sobresale por su facilidad de uso y su velocidad a la hora de realizar el procesamiento de texto.\nTextacy: Esta es una librer√≠a de alto nivel dise√±ada sobre Spacy con la idea de facilitar aun m√°s las tareas relacionadas con el Procesamiento del Lenguaje Natural.\nGensim: Es una librer√≠a dise√±ada para extraer autom√°ticamente los temas sem√°nticos de los documentos de la forma m√°s eficiente y con menos complicaciones posible.\npyLDAvis: Esta librer√≠a est√° dise√±ado para ayudar a los usuarios a interpretar los temas que surgen de un an√°lisis de t√≥picos. Nos permite visualizar en forma muy sencilla cada uno de los temas incluidos en el texto.\nComo veremos, el NLP tambi√©n se est√° sumando a la popularidad del Deep Learning, por lo que muchos de los frameworks que se utilizan en Deep Learning pueden ser aplicados para realizar modelos de NLP.\nCorpus ling√º√≠stico# Hoy en d√≠a, es indispensable el uso de buenos recursos ling√º√≠sticos para el desarrollo de los sistemas de NLP. Estos recursos son esenciales para la creaci√≥n de gram√°ticas, en el marco de aproximaciones simb√≥licas; o para llevar a cabo la formaci√≥n de m√≥dulos basados en el aprendizaje autom√°tico.\nUn corpus ling√º√≠stico es un conjunto amplio y estructurado de ejemplos reales de uso de la lengua. Estos ejemplos pueden ser textos (los m√°s comunes), o muestras orales (generalmente transcritas). Un corpus ling√º√≠stico es un conjunto de textos relativamente grande, creado independientemente de sus posibles formas o usos. Es decir, en cuanto a su estructura, variedad y complejidad, un corpus debe reflejar una lengua, o su modalidad, de la forma m√°s exacta posible; en cuanto a su uso, preocuparse de que su representaci√≥n sea real. La idea es que representen al lenguaje de la mejor forma posible para que los modelos de NLP puedan aprender los patrones necesarios para entender el lenguaje. Encontrar un buen corpus sobre el cual trabajar no suele ser una tarea sencilla; un corpus que se suele utilizar para entrenar modelos es el que incluye la informaci√≥n extra√≠da de wikipedia.\nProcesamiento del Lenguaje Natural con Python# Hasta aqu√≠ la introducci√≥n, ahora lleg√≥ el momento de ensuciarse un poco las manos y comenzar a explorar algunos ejemplos de las herramientas que nos ofrece Python para trabajar con problemas de Procesamiento del Lenguaje Natural. Comencemos por ejemplo, descargando el corpus de wikipedia en espa√±ol. Esto lo podemos hacer f√°cilmente utilizando Textacy.\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # Importando las librer√≠as que vamos a utilizar import pandas as pd import numpy as np import matplotlib.pyplot as plt import textacy from textacy.datasets import Wikipedia from collections import Counter, defaultdict import warnings; warnings.simplefilter(\u0026#39;ignore\u0026#39;) # graficos incrustados %matplotlib inline # funci√≥n auxiliar def leer_texto(texto): \u0026#34;\u0026#34;\u0026#34;Funcion auxiliar para leer un archivo de texto\u0026#34;\u0026#34;\u0026#34; with open(texto, \u0026#39;r\u0026#39;) as text: return text.read() # Descargando copus de wikipedia wp = Wikipedia(data_dir=\u0026#39;/home/raul/Documents/data\u0026#39;, lang=\u0026#39;es\u0026#39;, version=\u0026#39;latest\u0026#39;) wp.download() # Chequeando la informaci√≥n descargada wp.info {'data_dir': '/home/raul/Documents/data', 'description': 'All articles for a given language- and version-specific Wikipedia site snapshot.', 'name': 'wikipedia', 'site_url': 'https://meta.wikimedia.org/wiki/Data_dumps'} for text in wp.texts(min_len=1000, limit=2): print(text[:375], \u0026#34;\\n\u0026#34;) Andorra Andorra, oficialmente Principado de Andorra , es un peque√±o pa√≠s soberano del suroeste de Europa. Constituido en Estado independiente, de derecho, democr√°tico y social, cuya forma de gobierno es el coprincipado parlamentario. Su territorio est√° organizado en siete parroquias, con una poblaci√≥n total en 2016 de 78.264 habitantes. Su capital es Andorra la Vieja. Ti Argentina La Rep√∫blica Argentina, conocida simplemente como Argentina, es un pa√≠s soberano de Am√©rica del Sur, ubicado en el extremo sur y sudeste de dicho subcontinente. Adopta la forma de gobierno republicana, representativa y federal. El Estado argentino es un Estado federal descentralizado, integrado por un Estado nacional y veintitr√©s estados provinciales aut√≥nomos Como podemos ver, con la ayuda de Textacy es muy f√°cil descargar la informaci√≥n de wikipedia para luego poder utilizarla de base para nuestro corpus. Veamos otros problemas que tambi√©n podemos resolver con la ayuda de Textacy; como ser los casos de detectar el idioma o de procesar todo un texto y analizarlo.\n# Detectando el idioma con taxtacy saludos = [\u0026#34;Hola\u0026#34;, \u0026#34;Hello\u0026#34;, \u0026#34;Bonjour\u0026#34;, \u0026#34;Guten Tag\u0026#34;, \u0026#34;Buon giorno\u0026#34;, \u0026#34;Bom dia\u0026#34;] for saludo in saludos: print(textacy.text_utils.detect_language(saludo)) es en fr de it pt # Cargando el modelo en espa√±ol de spacy nlp = textacy.data.spacy.load(\u0026#39;es_core_web_md\u0026#39;) # detalle de stop words # las stop words son las palabras m√°s comunes de un corpus que en general # queremos eliminar porque no son significativas para un an√°lisis. # Ocurren muchas veces, pero aportan muy poca informaci√≥n stop = list(textacy.data.spacy.es.STOP_WORDS) stop[:15] ['cuanto', 'sera', 'trabajo', 'tan', 'ya', 'claro', 'encuentra', 'arriba', 'despacio', 'primero', 'pocas', 'tiempo', 'aqu√©llos', 'd√≠as', 'enseguida'] # Procesando un texto # Procesando 1984 de George Orwell - mi novela favorita texto = leer_texto(\u0026#39;ORWELL_1984.txt\u0026#39;) texto_procesado = nlp(texto) # Cu√°ntas sentencias hay en el texto? sentencias = [s for s in texto_procesado.sents] print(len(sentencias)) 8114 # imprimir las primeras 10 sentencias para verificar el texto print(sentencias[1:11]) [Winston Smith, con la barbilla clavada en el pecho en su esfuerzo por burlar el molest√≠simo viento, se desliz√≥ r√°pidamente por entre las puertas de cristal de las Casas de la Victoria, aunque, no con la suficiente rapidez para evitar que una r√°faga polvorienta se colara con √©l. , El vest√≠bulo ol√≠a a legumbres cocidas y a esteras viejas., Al fondo, un cartel de colores, demasiado grande para hallarse en un interior, estaba pegado a la pared., Representaba s√≥lo un enorme rostro de m√°s de un metro de anchura: la cara de un hombre de unos cuarenta y cinco a√±os con un gran bigote negro y facciones hermosas y endurecidas., Winston se dirigi√≥ hacia las escaleras., Era in√∫til intentar subir en el ascensor., No funcionaba con frecuencia y en esta √©poca la corriente se cortaba durante las horas de d√≠a., Esto era parte de las restricciones con que se preparaba la Semana del Odio., Winston ten√≠a que subir a un s√©ptimo piso.] # sentencias con las que aparece el Gran Hermano [sent for sent in texto_procesado.sents if \u0026#39;Gran Hermano\u0026#39; in sent.string][-10:] [‚Äî¬øMorir√° el Gran Hermano?, ael Gran Hermano., Pens√≥ en el Gran Hermano., ¬øCu√°les eran sus verdaderos sentimientos hacia el Gran Hermano?, Dime: ¬øcu√°les son los verdaderos sentimientos que te inspira el Gran Hermano?, Tienes que amar ael Gran Hermano., l Gran Hermano., l Gran Hermano., Amaba ael Gran Hermano. , Hubiera sido posible, por ejemplo, decir el ¬´Gran Hermano inbueno¬ª.] # \u0026lt;!-- collapse=True --\u0026gt; def encontrar_personajes(doc): \u0026#34;\u0026#34;\u0026#34; Devuelve una lista de los personajes de un `doc` con su cantidad de ocurrencias :param doc: NLP documento parseado por Spacy :return: Lista de Tuplas con la forma [(\u0026#39;winston\u0026#39;, 686), (\u0026#34;o\u0026#39;brien\u0026#34;, 135), (\u0026#39;julia\u0026#39;, 85),] \u0026#34;\u0026#34;\u0026#34; personajes = Counter() for ent in doc.ents: if ent.label_ == \u0026#39;PERSON\u0026#39;: personajes[ent.lemma_] += 1 return personajes.most_common() # Extrayendo los personajes principales del texto y contando cuantas veces # son nombrados. print(encontrar_personajes(texto_procesado)[:20]) [('winston', 686), (\u0026quot;o'brien\u0026quot;, 135), ('julia', 85), ('partido', 85), ('parsons', 36), ('syme', 29), ('goldstein', 29), ('pensamiento', 22), ('odio', 13), ('ministerio', 13), ('', 11), ('katharine', 11), ('winston ?', 10), ('rutherford', 9), ('ogilvy', 8), ('aaronson', 8), ('charrington', 8), ('‚Äîla', 7), ('withers', 6), ('ingsoc', 6)] # \u0026lt;!-- collapse=True --\u0026gt; def obtener_adj_pers(doc, personaje): \u0026#34;\u0026#34;\u0026#34; Encontrar todos los adjetivos relacionados a un personaje en un `doc` :param doc: NLP documento parseado por Spacy :param personaje: un objeto String :return: lista de adjetivos relacionados a un `personaje` \u0026#34;\u0026#34;\u0026#34; adjetivos = [] for ent in doc.ents: if ent.lemma_ == personaje: for token in ent.subtree: if token.pos_ == \u0026#39;ADJ\u0026#39;: adjetivos.append(token.lemma_) for ent in doc.ents: if ent.lemma_ == personaje: if ent.root.dep_ == \u0026#39;nsubj\u0026#39;: for child in ent.root.head.children: if child.dep_ == \u0026#39;acomp\u0026#39;: adjetivos.append(child.lemma_) return adjetivos # Encontrar adjetivos que describen a alg√∫n personaje. print(obtener_adj_pers(texto_procesado, \u0026#34;winston\u0026#34;)) ['superior', 'separado', 'fuertes', 'abierto', 'oscura', 'dirigi√©ndose', 'negro', 'saltones', 'tristes', 'burlones', 'solo', 'humano', 'sostenemos', 'dubitativo', 'completa', 'definitiva', 'sobresaltado', 'fascinado', 'extra√±a', 'sobrado', 'propio', 'solos', 'joven', 'sorprendido', 'sorprendido', 'hermosa', 'breve', 'cortante', 'primera', 'junto', 'obediente', 'metaf√≠sico', 'blanca', 'sonriente', 'sentado', 'irresoluto', 'sumergido', 'feliz'] # \u0026lt;!-- collapse=True --\u0026gt; def personaje_verbo(doc, verbo): \u0026#34;\u0026#34;\u0026#34; Encontrar los personajes que utilizan determinado `verbo` en `doc` :param doc: NLP documento parseado por Spacy :param verbo: un objeto String :return: lista de personajes que utilizan `verbo` \u0026#34;\u0026#34;\u0026#34; contar_verbo = Counter() for ent in doc.ents: if ent.label_ == \u0026#39;PERSON\u0026#39; and ent.root.head.lemma_ == verbo: contar_verbo[ent.text] += 1 return contar_verbo.most_common(10) # Encontrar personajes que utilizan determinado verbo personaje_verbo(texto_procesado, \u0026#34;dijo\u0026#34;) [('Winston', 7), ('Julia', 4), ('Syme', 2), ('Julia‚Äî. Espera', 1), ('Parsons', 1), ('‚Äîle', 1)] # Trabajando con las entidades del texto # Una entidad nombrada es cualquier objeto del mundo real como una persona, # ubicaci√≥n, organizaci√≥n o producto con un nombre propio. # tipos de entidades del texto set(ent.label_ for ent in texto_procesado.ents) {'CARDINAL', 'LOC', 'MISC', 'ORDINAL', 'ORG', 'PERSON'} # Entidades nombradas de tipo ORG [ent for ent in texto_procesado.ents if ent.label_ == \u0026#39;ORG\u0026#39;][:10] [INGSOC, Partido:, ES LA, Departamento de Registro, Dos Minutos de Odio, Departamento de Novela, Partido Interior, Partido Interior, Departamento de Registro, Dos Minutos] # Partes de la oraci√≥n (POS) # En las partes de la oraci√≥n se etiquetan las palabras de acuerdo a lo # que significan segun su contexto. Algunas de estas etiquetas pueden # ser: Adjetivos, verbos, adverbios, conjunciones, pronombres, sustantivos. # Etiquetas del texto set(token.pos_ for token in texto_procesado) {'ADJ', 'ADP', 'ADV', 'AUX', 'CONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SPACE', 'SYM', 'VERB'} # Etiquetas de tipo ADJ [token.orth_ for token in texto_procesado if token.pos_ == \u0026#39;ADJ\u0026#39;][1:11] ['fr√≠o', 'clavada', 'molest√≠simo', 'suficiente', 'polvorienta', 'cocidas', 'viejas', 'grande', 'pegado', 'enorme'] # Etiquetas de tipo PROPN [token.orth_ for token in texto_procesado if token.pos_ == \u0026#39;PROPN\u0026#39;][1:11] ['GEORGE', 'ORWELL', 'PARTE', 'CAPITULO', 'Winston', 'Smith', 'Casas', 'Victoria', 'Winston', 'Semana'] Como demuestran estos ejemplos Textacy / Spacy son herramientas muy poderosas que nos pueden ayudar a analizar y obtener informaci√≥n valiosa de un texto en forma r√°pida y sencilla.\nDeep Learning y Procesamiento del Lenguaje Natural# Durante mucho tiempo, las t√©cnicas principales de Procesamiento del Lenguaje Natural fueron dominadas por m√©todos de aprendizaje autom√°tico que utilizaron modelos lineales como las m√°quinas de vectores de soporte o la regresi√≥n log√≠stica, entrenados sobre vectores de caracter√≠sticas de muy alta dimensional pero muy escasos. Recientemente, el campo ha tenido cierto √©xito en el cambio hacia modelos de deep learning sobre entradas m√°s densas.\nLas redes neuronales proporcionan una poderosa maquina de aprendizaje que es muy atractiva para su uso en problemas de lenguaje natural. Un componente importante en las redes neuronales para el lenguaje es el uso de una capa de word embedding, una asignaci√≥n de s√≠mbolos discretos a vectores continuos en un espacio dimensional relativamente bajo. Cuando se utiliza word embedding, se transforman los distintos s√≠mbolos en objetos matem√°ticos sobre los que se pueden realizar operaciones. En particular, la distancia entre vectores puede equipararse a la distancia entre palabras, facilitando la generalizaci√≥n del comportamiento de una palabra sobre otra. Esta representaci√≥n de palabras como vectores es aprendida por la red como parte del proceso de entrenamiento. Subiendo en la jerarqu√≠a, la red tambi√©n aprende a combinar los vectores de palabras de una manera que es √∫til para la predicci√≥n. Esta capacidad alivia en cierta medida los problemas de dispersi√≥n de los datos. Hay dos tipos principales de arquitecturas de redes neuronales que resultan muy √∫tiles en los problemas de Procesamiento del Lenguaje Natural: las Redes neuronales prealimentadas y las Redes neuronales recurrentes.\nPara ejemplificar, veamos como podemos utilizar word embedding utilizando el modelo word2vec de Gensim.\n# \u0026lt;!-- collapse=True --\u0026gt; # importando gensim y TSNE para graficar import gensim from sklearn.manifold import TSNE Using TensorFlow backend. # transformando el texto para pasarlo al modelo de gensim texto = [[str(palabra).lower() for palabra in sent if str(palabra) not in stop ] for sent in sentencias] # generando el diccionario diccionario = gensim.corpora.Dictionary(texto) # creando el modelo modelo = gensim.models.Word2Vec(texto, workers=4, size=100, min_count=5, window=10, sample=1e-3) # representaci√≥n de la palabra hermano como vector. modelo[\u0026#39;hermano\u0026#39;] array([-0.22692642, -0.08890257, -0.12868501, -0.17392403, 0.22627664, 0.10127033, -0.09027202, 0.10692301, -0.30289358, 0.06429829, 0.17862263, 0.20448232, -0.54694331, -0.40681064, 0.61438572, 0.0217872 , 0.080202 , 0.46306548, 0.09076022, -0.02869571, -0.46194851, 0.28670114, 0.38570273, 0.32555154, 0.13098474, -0.03134775, -0.09577781, 0.06859019, -0.15935177, 0.61558241, 0.07509102, -0.24245416, -0.44668666, -0.77279037, 0.84581488, -0.54047441, -0.18756895, -0.12506978, -0.52870399, 0.1898849 , -0.00930689, 0.36932173, 0.22370262, -0.67407966, -0.45509291, -0.00848365, 0.62967575, 0.16172817, 0.09978516, 0.15064637, -0.34957823, 0.20686783, 0.1038606 , -0.09155462, 0.08276461, 0.31154567, -0.3129864 , -0.45181432, -0.12060832, 0.30541465, -0.37994722, 0.13566031, 0.16380484, 0.32732216, 0.15746659, 0.69340295, -0.25527388, 0.37333885, 0.23317885, -0.4710786 , -0.22506852, 0.14103019, -0.30253953, 0.00573605, -0.14745024, -0.50815731, -0.37789851, -0.3400358 , 0.62753612, 0.04747195, -0.07443633, 0.4276363 , -0.28931141, 0.29784235, -0.07251735, -0.07709371, -0.1003265 , -0.29098341, 0.47159177, 0.41372281, -0.10831725, -0.04670507, 0.07489309, 0.00146162, -0.02867368, -0.2771121 , 0.37281424, -0.53325164, 0.19094327, 0.51455575], dtype=float32) # como convertimos las palabras en vectores y los vectores capturan muchas # regularidades ling√º√≠sticas, podemos aplicar operaciones vectoriales para # extraer muchas propiedades interesantes. # palabras similares a persona modelo.most_similar_cosmul(\u0026#39;persona\u0026#39;) [('mano', 0.9999627470970154), ('se', 0.9999592304229736), ('mesa', 0.9999556541442871), ('lo', 0.999954879283905), ('pod√≠a', 0.9999546408653259), ('personas', 0.9999545812606812), ('habr√≠a', 0.9999533891677856), ('sitio', 0.9999532103538513), ('no', 0.999953031539917), ('pero', 0.999951958656311)] # por √∫ltimo podemos graficar el modelo vocab = list(modelo.wv.vocab) X = modelo[vocab] # aplicamos TSNE tsne = TSNE(n_components=2) X_tsne = tsne.fit_transform(X) # transformamos en DataFrame df = pd.concat([pd.DataFrame(X_tsne), pd.Series(vocab)], axis=1) df.columns = [\u0026#39;x\u0026#39;, \u0026#39;y\u0026#39;, \u0026#39;palabra\u0026#39;] # creamos el gr√°fico fig = plt.figure(figsize=(10, 8)) ax = fig.add_subplot(1, 1, 1) # reducimos el dataset a 15 palabras para el ejemplo df_e = df.head(15) ax.scatter(df_e[\u0026#39;x\u0026#39;], df_e[\u0026#39;y\u0026#39;]) for i, txt in enumerate(df_e[\u0026#39;palabra\u0026#39;]): ax.annotate(txt, (df_e[\u0026#39;x\u0026#39;].iloc[i], df_e[\u0026#39;y\u0026#39;].iloc[i])) plt.show() Aqu√≠ termina este art√≠culo, obviamente el Procesamiento del Lenguaje Natural es un campo muy amplio y quedaron muchas cosas sin desarrollar. Espero que esta introducci√≥n les haya sido de utilidad.\nSaludos!\nEste post fue escrito por Ra√∫l e. L√≥pez Briega utilizando Jupyter notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2017-09-23","id":7,"permalink":"/blog/2017/09/23/procesamiento-del-lenguaje-natural-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, Analisis de datos y Python. El contenido esta bajo la licencia BSD.\n\u0026ldquo;El lenguaje sirve no s√≥lo para expresar el pensamiento, sino para hacer posibles pensamientos que no podr√≠an existir sin √©l.\u0026rdquo;\nBertrand Russell\nIntroducci√≥n# El lenguaje es una de las herramientas centrales en nuestra vida social y profesional. Entre otras cosas, act√∫a como un medio para transmitir ideas, informaci√≥n, opiniones y sentimientos; as√≠ como para persuadir, pedir informaci√≥n, o dar ordenes.","tags":["python","programacion","analisis de datos","machine learning","redes neuronales","lenguaje natural"],"title":"Procesamiento del Lenguaje Natural con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega para el sitio de capacitaciones de IAAR. El contenido esta bajo la licencia BSD.\nEste art√≠culo fue publicado originalmente en el sitio de capacitaciones de IAAR.\nIntroducci√≥n # El Deep Learning es sin duda el √°rea de investigaci√≥n m√°s popular dentro del campo de la inteligencia artificial. La mayor√≠a de las nuevas investigaciones que se realizan, trabajan con modelos basados en las t√©cnicas de Deep Learning; ya que las mismas han logrado resultados sorprendes en campos como Procesamiento del lenguaje natural y Visi√≥n por computadora. Pero\u0026hellip; ¬øqu√© es este misterioso concepto que ha ganado tanta popularidad? y\u0026hellip; ¬øc√≥mo se relaciona con el campo de la inteligencia artificial y el Machine Learning?.\nInteligencia artificial, Machine learning y Deep learning # En general se suelen utilizar los t√©rminos de inteligencia artificial, Machine Learning y Deep Learning en forma intercambiada. Sin embargo, √©stos t√©rminos no son los mismo y abarcan distintas cosas.\nInteligencia Artificial# El t√©rmino inteligencia artificial es el m√°s general y engloba a los campos de Machine Learning y Deep Learning junto con otras t√©cnicas como los algoritmos de b√∫squeda, el razonamiento simb√≥lico, el razonamiento l√≥gico y la estad√≠stica. Naci√≥ en los a√±os 1950s, cuando un grupo de pioneros de la computaci√≥n comenzaron a preguntarse si se pod√≠a hacer que las computadoras pensaran. Una definici√≥n concisa de la inteligencia artificial ser√≠a: el esfuerzo para automatizar las tareas intelectuales que normalmente realizan los seres humanos.\nMachine Learning# El Machine Learning o Aprendizaje autom√°tico se refiere a un amplio conjunto de t√©cnicas inform√°ticas que nos permiten dar a las computadoras la capacidad de aprender sin ser expl√≠citamente programadas. Hay muchos tipos diferentes de algoritmos de Aprendizaje autom√°tico, entre los que se encuentran el aprendizaje por refuerzo, los algoritmos gen√©ticos, el aprendizaje basado en reglas de asociaci√≥n, los algoritmos de agrupamiento, los √°rboles de decisi√≥n, las m√°quinas de vectores de soporte y las redes neuronales. Actualmente, los algoritmos m√°s populares dentro de este campo son los de Deep Learning.\nDeep Learning# El Deep Learning o aprendizaje profundo es un subcampo dentro del Machine Learning, el cu√°l utiliza distintas estructuras de redes neuronales para lograr el aprendizaje de sucesivas capas de representaciones cada vez m√°s significativas de los datos. El profundo o deep en Deep Learning hace referencia a la cantidad de capas de representaciones que se utilizan en el modelo; en general se suelen utilizar decenas o incluso cientos de capas de representaci√≥n. las cuales aprenden automaticamente a medida que el modelo es entrenado con los datos.\n¬øQu√© es el Deep Learning? # Antes de poder entender que es el Deep Learning, debemos en primer lugar conocer dos conceptos fundamentales: las redes neuronales artificiales y la Propagaci√≥n hacia atr√°s.\nRedes Neuronales# Las redes neuronales son un modelo computacional basado en un gran conjunto de unidades neuronales simples (neuronas artificiales), de forma aproximadamente an√°loga al comportamiento observado en los axones de las neuronas en los cerebros biol√≥gicos.\nCada una de estas neuronas simples, va a tener una forma similar al siguiente diagrama:\nEn donde sus componentes son:\n\\(x_1, x_2, \\dots, x_n\\): Los datos de entrada en la neurona, los cuales tambi√©n puede ser que sean producto de la salida de otra neurona de la red.\n\\(x_0\\): La unidad de sesgo; un valor constante que se le suma a la entrada de la funci√≥n de activaci√≥n de la neurona. Generalmente tiene el valor 1. Este valor va a permitir cambiar la funci√≥n de activaci√≥n hacia la derecha o izquierda, otorg√°ndole m√°s flexibilidad para aprender a la neurona.\n\\(w_0, w_1, w_2, \\dots, w_n\\): Los pesos relativos de cada entrada. Tener en cuenta que incluso la unidad de sesgo tiene un peso.\na: La salida de la neurona. Que va a ser calculada de la siguiente forma:\n$$a = f\\left(\\sum_{i=0}^n w_i \\cdot x_i \\right)$$ Aqu√≠ \\(f\\) es la funci√≥n de activaci√≥n de la neurona. Esta funci√≥n es la que le otorga tanta flexibilidad a las redes neuronales y le permite estimar complejas relaciones no lineales en los datos. Puede ser tanto una funci√≥n lineal, una funci√≥n log√≠stica, hiperb√≥lica, etc.\nCada unidad neuronal est√° conectada con muchas otras y los enlaces entre ellas pueden incrementar o inhibir el estado de activaci√≥n de las neuronas adyacentes. Estos sistemas aprenden y se forman a s√≠ mismos, en lugar de ser programados de forma expl√≠cita, y sobresalen en √°reas donde la detecci√≥n de soluciones o caracter√≠sticas es dif√≠cil de expresar con la programaci√≥n convencional.\nPropagaci√≥n hacia atr√°s# La propagaci√≥n hacia atr√°s o backpropagation es un algoritmo que funciona mediante la determinaci√≥n de la p√©rdida (o error) en la salida y luego propag√°ndolo de nuevo hacia atr√°s en la red. De esta forma los pesos se van actualizando para minimizar el error resultante de cada neurona. Este algoritmo es lo que les permite a las redes neuronales aprender.\n¬øC√≥mo funciona el Deep Learning? # En general, cualquier t√©cnica de Machine Learning trata de realizar la asignaci√≥n de entradas (por ejemplo, im√°genes) a salidas objetivo (Por ejemplo, la etiqueta \u0026ldquo;gato\u0026rdquo;), mediante la observaci√≥n de un gran n√∫mero de ejemplos de entradas y salidas. El Deep Learning realiza este mapeo de entrada-a-objetivo por medio de una red neuronal artificial que est√° compuesta de un n√∫mero grande de capas dispuestas en forma de jerarqu√≠a. La red aprende algo simple en la capa inicial de la jerarqu√≠a y luego env√≠a esta informaci√≥n a la siguiente capa. La siguiente capa toma esta informaci√≥n simple, lo combina en algo que es un poco m√°s complejo, y lo pasa a la tercer capa. Este proceso contin√∫a de forma tal que cada capa de la jerarqu√≠a construye algo m√°s complejo de la entrada que recibi√≥ de la capa anterior. De esta forma, la red ir√° aprendiendo por medio de la exposici√≥n a los datos de ejemplo.\nLa especificaci√≥n de lo que cada capa hace a la entrada que recibe es almacenada en los pesos de la capa, que en esencia, no son m√°s que n√∫meros. Utilizando terminolog√≠a m√°s t√©cnica podemos decir que la transformaci√≥n de datos que se produce en la capa es parametrizada por sus pesos. Para que la red aprenda debemos encontrar los pesos de todas las capas de forma tal que la red realice un mapeo perfecto entre los ejemplos de entrada con sus respectivas salidas objetivo. Pero el problema reside en que una red de Deep Learning puede tener millones de par√°metros, por lo que encontrar el valor correcto de todos ellos puede ser una tarea realmente muy dif√≠cil, especialmente si la modificaci√≥n del valor de uno de ellos afecta a todos los dem√°s.\nPara poder controlar algo, en primer lugar debemos poder observarlo. En este sentido, para controlar la salida de la red neuronal, deber√≠amos poder medir cuan lejos esta la salida que obtuvimos de la que se esperaba obtener. Este es el trabajo de la funci√≥n de p√©rdida de la red. Esta funci√≥n toma las predicciones que realiza el modelo y los valores objetivos (lo que realmente esperamos que la red produzca), y calcula cu√°n lejos estamos de ese valor, de esta manera, podemos capturar que tan bien esta funcionando el modelo para el ejemplo especificado. El truco fundamental del Deep Learning es utilizar el valor que nos devuelve esta funci√≥n de p√©rdida para retroalimentar la red y ajustar los pesos en la direcci√≥n que vayan reduciendo la p√©rdida del modelo para cada ejemplo. Este ajuste, es el trabajo del optimizador, el cu√°l implementa la propagaci√≥n hacia atr√°s.\nResumiendo, el funcionamiento ser√≠a el siguiente: inicialmente, los pesos de cada capa son asignados en forma aleatoria, por lo que la red simplemente implementa una serie de transformaciones aleatorias. En este primer paso, obviamente la salida del modelo dista bastante del ideal que deseamos obtener, por lo que el valor de la funci√≥n de p√©rdida va a ser bastante alto. Pero a medida que la red va procesando nuevos casos, los pesos se van ajustando de forma tal de ir reduciendo cada vez m√°s el valor de la funci√≥n de p√©rdida. Este proceso es el que se conoce como entrenamiento de la red, el cual repetido una suficiente cantidad de veces, generalmente 10 iteraciones de miles de ejemplos, logra que los pesos se ajusten a los que minimizan la funci√≥n de p√©rdida. Una red que ha minimizado la p√©rdida es la que logra los resultados que mejor se ajustan a las salidas objetivo, es decir, que el modelo se encuentra entrenado.\nArquitecturas de Deep Learning # La estructura de datos fundamental de una red neuronal est√° vagamente inspirada en el cerebro humano. Cada una de nuestras c√©lulas cerebrales (neuronas) est√° conectada a muchas otras neuronas por sinapsis. A medida que experimentamos e interactuamos con el mundo, nuestro cerebro crea nuevas conexiones, refuerza algunas conexiones y debilita a los dem√°s. De esta forma, en nuestro cerebro se desarrollan ciertas regiones que se especializan en el procesamiento de determinadas entradas. As√≠ vamos a tener un √°rea especializada en la visi√≥n, otra que se especializa en la audici√≥n, otra para el lenguaje, etc. De forma similar, dependiendo del tipo de entradas con las que trabajemos, van a existir distintas arquitecturas de redes neuronales que mejor se adaptan para procesar esa informaci√≥n. Algunas de las arquitecturas m√°s populares son:\nRedes neuronales prealimentadas# Las Redes neuronales prealimentadas fueron las primeras que se desarrollaron y son el modelo m√°s sencillo. En estas redes la informaci√≥n se mueve en una sola direcci√≥n: hacia adelante. Los principales exponentes de este tipo de arquitectura son el perceptr√≥n y el perceptr√≥n multicapa. Se suelen utilizar en problemas de clasificaci√≥n simples.\nRedes neuronales convolucionales# Las redes neuronales convolucionales son muy similares a las redes neuronales ordinarias como el perceptron multicapa; se componen de neuronas que tienen pesos y sesgos que pueden aprender. Cada neurona recibe algunas entradas, realiza un producto escalar y luego aplica una funci√≥n de activaci√≥n. Al igual que en el perceptron multicapa tambi√©n vamos a tener una funci√≥n de p√©rdida o costo sobre la √∫ltima capa, la cual estar√° totalmente conectada. Lo que diferencia a las redes neuronales convolucionales es que suponen expl√≠citamente que las entradas son im√°genes, lo que nos permite codificar ciertas propiedades en la arquitectura; permitiendo ganar en eficiencia y reducir la cantidad de par√°metros en la red.\nEn general, las redes neuronales convolucionales van a estar construidas con una estructura que contendr√° 3 tipos distintos de capas:\nUna capa convolucional, que es la que le da le nombre a la red. Una capa de reducci√≥n o de pooling, la cual va a reducir la cantidad de par√°metros al quedarse con las caracter√≠sticas m√°s comunes. Una capa clasificadora totalmente conectada, la cual nos va dar el resultado final de la red. Algunas implementaciones espec√≠ficas que podemos encontrar sobre este tipo de redes son: inception v3, ResNet, VGG16 y xception, entre otras. Todas ellas han logrado excelentes resultados.\nRedes neuronales recurrentes# Los seres humanos no comenzamos nuestro pensamiento desde cero cada segundo, sino que los mismos tienen una persistencia. Las Redes neuronales prealimentadas tradicionales no cuentan con esta persistencia, y esto parece una deficiencia importante. Las Redes neuronales recurrentes abordan este problema. Son redes con bucles de retroalimentaci√≥n, que permiten que la informaci√≥n persista.\nUna Red neural recurrente puede ser pensada como una red con m√∫ltiples copias de ella misma, en las que cada una de ellas pasa un mensaje a su sucesor. Esta naturaleza en forma de cadena revela que las Redes neurales recurrentes est√°n √≠ntimamente relacionadas con las secuencias y listas; por lo que son ideales para trabajar con este tipo de datos. En los √∫ltimos a√±os, ha habido un √©xito incre√≠ble aplicando Redes neurales recurrentes a una variedad de problemas como: reconocimiento de voz, modelado de lenguaje, traducci√≥n, subt√≠tulos de im√°genes y la lista contin√∫a.\nLas redes de memoria de largo plazo a corto plazo - generalmente llamadas LSTMs - son un tipo especial de Redes neurales recurrentes, capaces de aprender dependencias a largo plazo. Ellas tambi√©n tienen una estructura como cadena, pero el m√≥dulo de repetici√≥n tiene una estructura diferente. En lugar de tener una sola capa de red neuronal, tiene cuatro, que interact√∫an de una manera especial permitiendo tener una memoria a m√°s largo plazo.\nPara m√°s informaci√≥n sobre diferentes arquitecturas de redes neuronales pueden visitar el siguiente art√≠culo de wikipedia.\nLogros del Deep Learning # En los √∫ltimos a√±os el Deep Learning ha producido toda una revoluci√≥n en el campo del Machine Learning, con resultados notables en todos los problemas de percepci√≥n, como ver y escuchar, problemas que implican habilidades que parecen muy naturales e intuitivas para los seres humanos, pero que desde hace tiempo se han mostrado dif√≠ciles para las m√°quinas. En particular, el Deep Learning ha logrado los siguientes avances, todos ellos en √°reas hist√≥ricamente dif√≠ciles del Machine Learning.\nUn nivel casi humano para la clasificaci√≥n de im√°genes. Un nivel casi humano para el reconocimiento del lenguaje hablado. Un nivel casi humano en el reconocimiento de escritura. Grandes mejoras en traducciones de lenguas. Grandes mejoras en conversaciones text-to-speech. Asistentes digitales como Google Now o Siri. Un nivel casi humano en autos aut√≥nomos. Mejores resultados de b√∫squeda en la web. Grandes mejoras para responder preguntas en lenguaje natural. Alcanzado Nivel maestro (superior al humano) en varios juegos. En muchos sentidos, el Deep Learning todav√≠a sigue siendo un campo misterioso para explorar, por lo que seguramente veremos nuevos avances en nuevas √°reas utilizando estas t√©cnicas. Tal vez alg√∫n d√≠a el Deep Learning ayuda a los seres humanos a hacer ciencia, desarrollar software y mucho m√°s.\n¬øPor qu√© estos sorprendentes resultados surgen ahora? # Muchos de los conceptos del Deep Learning se desarrollaron en los a√±os 80s y 90s, algunos incluso mucho antes. Sin embargo, los primeros resultados exitosos del Deep Learning surgieron en los √∫ltimos 5 a√±os. ¬øqu√© fue lo que cambio para lograr la popularidad y √©xito de los modelos basados en Deep Learning en estos √∫ltimos a√±os?\nSi bien existen m√∫ltiples factores para explicar esta revoluci√≥n del Deep Learning, los dos principales componentes parecen ser la disponibilidad de masivos vol√∫menes de datos, lo que actualmente se conoce bajo el nombre de Big Data; y el progreso en el poder de computo, especialmente gracias a los GPUs. Entonces, dentro de los factores que explican esta popularidad de los modelos de Deep Learning podemos encontrar:\nLa disponibilidad de conjuntos de datos enormes y de buena calidad. Gracias a la revoluci√≥n digital en que nos encontramos, podemos generar conjuntos de datos enormes con los cuales alimentar a los algoritmos de Deep Learning, los cuales necesitan de muchos datos para poder generalizar.\nComputaci√≥n paralela masiva con GPUs. En l√≠neas generales, los modelos de redes neuronales no son m√°s que complicados c√°lculos num√©ricos que se realizan en paralelo. Gracias al desarrollo de los GPUs estos c√°lculos ahora se pueden realizar en forma mucho m√°s r√°pida, permitiendo que podamos entrenar modelos m√°s profundos y grandes.\nFunciones de activaci√≥n amigables para Backpropagation. La progaci√≥n hacia atr√°s o Backpropagation es el algoritmo fundamental que hace funcionar a las redes neuronales; pero la forma en que trabaja implica c√°lculos realmente complicados. La transici√≥n desde funciones de activaci√≥n como tanh o sigmoid a funciones como ReLU o SELU han simplificado estos problemas.\nNuevas arquitecturas. Arquitecturas como Resnets, inception y GAN mantienen el campo actualizado y contin√∫an aumentando las flexibilidad de los modelos.\nNuevas t√©cnicas de regularizaci√≥n. T√©cnicas como dropout, batch normalization y data-augmentation nos permiten entrenar redes m√°s grandes con menos peligro de sobreajuste.\nOptimizadores m√°s robustos. La optimizaci√≥n es fundamental para el funcionamiento de las redes neuronales. Mejoras sobre el tradicional procedimiento de SGD, como ADAM han ayudado a mejorar el rendimiento de los modelos.\nPlataformas de software. Herramientas como TensorFlow, Theano, Keras, CNTK, PyTorch, Chainer, y mxnet nos permiten crear prototipos en forma m√°s r√°pida y trabajar con GPUs sin tantas complicaciones. Nos permiten enfocarnos en la estructura del modelo sin tener que preocuparnos por los detalles de m√°s bajo nivel.\nOtra raz√≥n por la que el Deep Learning ha tenido tanta repercusi√≥n √∫ltimamente adem√°s de ofrecer un mejor rendimiento en muchos problemas; es que el Deep Learning esta haciendo la resoluci√≥n de problemas mucho m√°s f√°cil, ya que automatiza completamente lo que sol√≠a ser uno de los pasos m√°s dif√≠ciles y cruciales en el flujo de trabajo de Machine Learning: la ingenier√≠a de atributos. Antes del Deep Learning, para poder entrenar un modelo, primero deb√≠amos refinar las entradas para adaptarlas al tipo de transformaci√≥n del modelo; ten√≠amos que cuidadosamente seleccionar los atributos m√°s representativos y desechar los poco informativos. El Deep Learning, en cambio, automatiza este proceso; aprendemos todos los atributos de una sola pasada y el mismo modelo se encarga de adaptarse y quedarse con lo m√°s representativo.\nFrameworks para Deep Learning# En estos momentos, si hay un campo en donde Python sobresale sobre cualquier otro lenguaje, es en su soporte para frameworks de Deep Learning. Existen una gran variedad y de muy buena calidad, entre los que se destacan:\nTensorFlow: TensorFlow es un frameworks desarrollado por Google. Es una librer√≠a de c√≥digo libre para computaci√≥n num√©rica usando grafos de flujo de datos.\nPyTorch: PyTorch es un framework de Deep Learning que utiliza el lenguaje Python y cuenta con el apoyo de Facebook.\nTheano: Theano es una librer√≠a de Python que permite definir, optimizar y evaluar expresiones matem√°ticas que involucran tensores de manera eficiente.\nCNTK: CNTK es un conjunto de herramientas, desarrolladas por Microsoft, f√°ciles de usar, de c√≥digo abierto que entrena algoritmos de Deep Learning para aprender como el cerebro humano.\nKeras: Keras es una librer√≠a de alto nivel, muy f√°cil de utilizar. Est√° escrita y mantenida por Francis Chollet, miembro del equipo de Google Brain. Permite a los usuarios elegir si los modelos que se construyen seran ejecutados en el grafo simb√≥lico de Theano, TensorFlow o CNTK.\nMXNet: MXNet es una librer√≠a flexible y eficiente para armar modelos de Deep Learning con soporte para varios idiomas.\n¬øC√≥mo mantenerse actualizado en el campo de Deep Learning? # El campo del Deep Learning se mueve muy rapidamente, con varios papers que se publican por mes; por tal motivo, mantenerse actualizado con las √∫ltimas tendencias del campo puede ser bastante complicado. Algunos consejos pueden ser:\nEstarse atento a las publicaciones en arxiv, especialmente a la secci√≥n de machine learning. La mayor√≠a de los papers m√°s relevantes, los vamos a poder encontrar en esa plataforma.\nSeguir el blog de keras en el cual podemos encontrar como implementar varios modelos utilizando esta genial librer√≠a.\nSeguir el blog de openai en d√≥nde detallan las investigaciones que van realizando, especialmente trabajando con GANs.\nSeguir el blog de Google research; en d√≥nde se viene haciendo bastante foco en los modelos de Deep Learning.\nUtilizar la secci√≥n de Machine Learning de reddit.\nSuscribirse al podcast Talking machines; en d√≥nde se entrevista a los principales exponentes del campo de la inteligencia artificial.\nPor √∫ltimo, obviamente estar atentos a las publicaciones que se realizan en IAAR.\nAqu√≠ concluye el art√≠culo; espero que les sirva como una introducci√≥n para que puedan ingresar en el fascinante y prometedor mundo del Deep Learning.\nSaludos!\nEste post fue escrito por Ra√∫l e. L√≥pez Briega utilizando Jupyter notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2017-06-13","id":8,"permalink":"/blog/2017/06/13/introduccion-al-deep-learning/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega para el sitio de capacitaciones de IAAR. El contenido esta bajo la licencia BSD.\nEste art√≠culo fue publicado originalmente en el sitio de capacitaciones de IAAR.\nIntroducci√≥n # El Deep Learning es sin duda el √°rea de investigaci√≥n m√°s popular dentro del campo de la inteligencia artificial. La mayor√≠a de las nuevas investigaciones que se realizan, trabajan con modelos basados en las t√©cnicas de Deep Learning; ya que las mismas han logrado resultados sorprendes en campos como Procesamiento del lenguaje natural y Visi√≥n por computadora.","tags":["python","programacion","analisis de datos","machine learning","deep learning"],"title":"Introducci√≥n al Deep Learning"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, Analisis de datos y Python. El contenido esta bajo la licencia BSD.\n\u0026ldquo;La opini√≥n de toda una multitud es siempre m√°s cre√≠ble que la de una minor√≠a.\u0026rdquo;\nMiguel de Unamuno\nIntroducci√≥n# La meta de construir sistemas que puedan adaptarse a sus entornos y aprender de su experiencia ha atra√≠do a investigadores de muchos campos, como la Inform√°tica, Matem√°ticas, F√≠sica, Neurociencia y la Ciencia cognitiva. Intuitivamente, para que un algoritmo de aprendizaje sea efectivo y preciso en sus predicciones, deber√≠a reunir tres condiciones b√°sicas:\nDeber√≠a ser entrenado con suficientes datos de entrenamiento. Sus resultados deber√≠an ajustarse bastante bien a los ejemplos de entrenamiento (lo que significar√≠a tener una tasa de error baja). Deber√≠a ser lo suficientemente \u0026ldquo;simple\u0026rdquo;. Esta √∫ltima condici√≥n, que las reglas m√°s simples suelen ser las mejores, se conoce a menudo con el nombre de \u0026ldquo;La navaja de Occam\u0026rdquo;. Muchos algoritmos se han creado y existen a√∫n muchos por descubrir; pero unos de los que ha ganado mucha atracci√≥n en los √∫ltimos a√±os por su simpleza y su gran √©xito en competencias como kraggle, son los algoritmos de Boosting.\n¬øQu√© es Boosting?# Boosting es un enfoque de Machine Learning basado en la idea de crear una regla de predicci√≥n altamente precisa combinando muchas reglas relativamente d√©biles e imprecisas. Una teor√≠a notablemente rica ha evolucionado en torno al Boosting, con conexiones a una amplia gama de ramas de la ciencia, incluyendo estad√≠sticas, teor√≠a de juegos, optimizaci√≥n convexa y geometr√≠a de la informaci√≥n. Los algoritmos de Boosting han tenido √©xito pr√°ctico con aplicaciones, por ejemplo, en biolog√≠a, visi√≥n y procesamiento del lenguaje natural. En varios momentos de su historia, el Boosting ha sido objeto de controversia por el misterio y la paradoja que parece presentar. El Boosting asume la disponibilidad de un algoritmo de aprendizaje base o d√©bil que, dado ejemplos de entrenamiento etiquetados, produce un clasificador base o d√©bil. El objetivo de Boosting es el de mejorar el rendimiento del algoritmo de aprendizaje al tratarlo como una \u0026ldquo;caja negra\u0026rdquo; que se puede llamar repetidamente, como una subrutina. Si bien el algoritmo de aprendizaje base puede ser rudimentario y moderadamente inexacto, no es del todo trivial ni poco informativo y debe obtener resultados mejores a los que se podr√≠an obtener en forma aleatoria. La idea fundamental detr√°s de Boosting es elegir conjuntos de entrenamiento para el algoritmo de aprendizaje base de tal manera que lo obligue a inferir algo nuevo sobre los datos cada vez que se lo llame. Uno de los primeros algoritmos de Boosting en tener √©xito en problemas de clasificaci√≥n binaria fue AdaBoost.\nAdaBoost# AdaBoost es la abreviatura de adaptive boosting, es un algoritmo que puede ser utilizado junto con otros algoritmos de aprendizaje para mejorar su rendimiento. AdaBoost funciona eligiendo un algoritmo base (por ejemplo √°rboles de decisi√≥n) y mejor√°ndolo iterativamente al tomar en cuenta los casos incorrectamente clasificados en el conjunto de entrenamiento.\nEn AdaBoost asignamos pesos iguales a todos los ejemplos de entrenamiento y elegimos un algoritmo base. En cada paso de iteraci√≥n, aplicamos el algoritmo base al conjunto de entrenamiento y aumentamos los pesos de los ejemplos incorrectamente clasificados. Iteramos n veces, cada vez aplicando el algoritmo base en el conjunto de entrenamiento con pesos actualizados. El modelo final es la suma ponderada de los resultados de los n algoritmos base. AdaBoost en conjunto con √°rboles de decisi√≥n se ha mostrado sumamente efectivo en varios problemas de Machine Learning. Veamos un ejemplo en Python.\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # Importando las librer√≠as que vamos a utilizar import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.model_selection import train_test_split from sklearn.ensemble import GradientBoostingClassifier from sklearn.ensemble import AdaBoostClassifier from sklearn.metrics import accuracy_score from sklearn.tree import DecisionTreeClassifier from sklearn.tree import export_graphviz from sklearn.datasets import load_breast_cancer from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OneHotEncoder import graphviz import xgboost as xgb # graficos incrustados %matplotlib inline # parametros esteticos de seaborn sns.set_palette(\u0026#34;deep\u0026#34;, desat=.6) sns.set_context(rc={\u0026#34;figure.figsize\u0026#34;: (8, 4)}) # Utilzando el dataset breast cancer cancer = load_breast_cancer() # dataset en formato tabular pd.DataFrame(data=cancer.data, columns=cancer.feature_names).head() mean radius mean texture mean perimeter mean area mean smoothness mean compactness mean concavity mean concave points mean symmetry mean fractal dimension 0 17.99 10.38 122.80 1001.0 0.11840 0.27760 0.3001 0.14710 0.2419 0.07871 1 20.57 17.77 132.90 1326.0 0.08474 0.07864 0.0869 0.07017 0.1812 0.05667 2 19.69 21.25 130.00 1203.0 0.10960 0.15990 0.1974 0.12790 0.2069 0.05999 3 11.42 20.38 77.58 386.1 0.14250 0.28390 0.2414 0.10520 0.2597 0.09744 4 20.29 14.34 135.10 1297.0 0.10030 0.13280 0.1980 0.10430 0.1809 0.05883 5 rows √ó 30 columns\n# Separando los datos en sets de entrenamiento y evaluaci√≥n X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=1) # Armando un simple arbol de decisi√≥n tree = DecisionTreeClassifier(max_depth=2, random_state=0) tree.fit(X_train, y_train) print(\u0026#39;Precisi√≥n modelo inicial train/test {0:.3f}/{1:.3f}\u0026#39; .format(tree.score(X_train, y_train), tree.score(X_test, y_test))) Precisi√≥n modelo inicial train/test 0.962/0.888 # Dibujando el modelo export_graphviz(tree, out_file=\u0026#34;tree.dot\u0026#34;, class_names=[\u0026#34;malignant\u0026#34;, \u0026#34;benign\u0026#34;], feature_names=cancer.feature_names, impurity=False, filled=True) with open(\u0026#34;tree.dot\u0026#34;) as f: dot_graph = f.read() graphviz.Source(dot_graph) # Utilizando AdaBoost para aumentar la precisi√≥n ada = AdaBoostClassifier(base_estimator=tree, n_estimators=500, learning_rate=1.5, random_state=1) # Ajustando los datos ada = ada.fit(X_train, y_train) # Imprimir la precisi√≥n. y_train_pred = ada.predict(X_train) y_test_pred = ada.predict(X_test) ada_train = accuracy_score(y_train, y_train_pred) ada_test = accuracy_score(y_test, y_test_pred) print(\u0026#39;Precisi√≥n modelo con AdaBoost train/test {0:.3f}/{1:.3f}\u0026#39; .format(ada_train, ada_test)) Precisi√≥n modelo con AdaBoost train/test 1.000/0.965 Para este ejemplo utilizamos el conjunto de datos breast cancer que ya viene en cargado en scikit-learn; la idea es clasificar casos de c√°ncer de pecho seg√∫n varios atributos de los tumores.\nEn primer lugar, creamos un clasificador simple, un √°rbol de decisi√≥n de hasta dos niveles de profundidad. Este clasificador tuvo un rendimiento bastante bueno, logrando una precisi√≥n del 96% con los datos de entrenamiento y del 89% con los datos de evaluaci√≥n.\nLuego aplicamos AdaBoost sobre el mismo modelo para mejorar la precisi√≥n. Vemos que el modelo con AdaBoost logra una precisi√≥n del 100% en los datos de entrenamiento y del 96% en los datos de evaluaci√≥n. Debemos tener en cuenta que una precisi√≥n del 100% sobre los datos de entrenamiento, puede ser un indicio de que el modelo tal vez este sobreajustado. El sobreajuste es uno de los riesgo que suele traer aparejado la utilizaci√≥n de las t√©cnicas de Boosting.\nA partir del √©xito inicial de AdaBoost, las t√©cnicas de Boosting fueron evolucionando hacia un modelo estad√≠stico m√°s generalizado, tratando el problema como un caso de optimizaci√≥n num√©rica en d√≥nde el objetivo es minimizar la funci√≥n de perdida del modelo mediante la adici√≥n de los algoritmos de aprendizaje d√©biles utilizando un procedimiento de optimizaci√≥n del tipo de gradiente descendiente. Esta generalizaci√≥n permiti√≥ utilizar funciones arbitrarias de p√©rdida diferenciables, ampliando la t√©cnica m√°s all√° de los problemas de clasificaci√≥n binaria hacia problemas de regresi√≥n y de clasificaci√≥n multi-variable. Esta nueva familia de algoritmos de Boosting se conocen bajo el nombre de Gradient boosting.\nGradient Boosting# El Gradient boosting implica tres elementos:\nUna funci√≥n de perdida a optimizar . Un algoritmo de aprendizaje d√©bil para hacer las predicciones. Un modelo aditivo para a√±adir los algoritmos de aprendizaje d√©biles que minimizan la funci√≥n de perdida. Funci√≥n de p√©rdida# La funci√≥n de perdida utilizada va a depender del tipo de problema al que nos enfrentamos. La principal caracter√≠stica que debe poseer, es que sea diferenciable. Existen varias funciones de p√©rdida est√°ndar. Por ejemplo, para problemas de regresi√≥n podemos utilizar un error cuadr√°tico y para problemas de clasificaci√≥n podemos utilizar una p√©rdida logar√≠tmica o una entrop√≠a cruzada.\nAlgoritmo de aprendizaje d√©bil# El algoritmo de aprendizaje d√©bil que se utiliza en el Gradient boosting es el de √°rboles de decisi√≥n. Espec√≠ficamente se usan √°rboles de regresi√≥n que producen valores reales para las divisiones y cuya salida se puede sumar, permitiendo que los resultados de los modelos subsiguientes sean agregados y corrijan los errores promediando las predicciones. Es com√∫n restringir a los √°rboles de decisi√≥n de manera espec√≠fica para asegurarnos que el algoritmo permanezca d√©bil. Se suelen restringir el n√∫mero m√°ximo de capas, nodos, divisiones u hojas.\nModelo aditivo# Los √°rboles de decisi√≥n son agregados de a uno a la vez, y los √°rboles existentes en el modelo no cambian. Para determinar los par√°metros que tendr√°n cada uno de los √°rboles de decisi√≥n que son agregados al modelo se utiliza un procedimiento de gradiente descendiente que minimizar√° la funci√≥n de perdida. De esta forma se van agregando √°rboles con distintos par√°metros de forma tal que la combinaci√≥n de ellos minimiza la p√©rdida del modelo y mejora la predicci√≥n.\n√Årboles de decisi√≥n con Gradient boosting es uno de los modelos m√°s poderosos y m√°s utilizados para problemas de aprendizaje supervisado. Su principal inconveniente es que requieren un ajuste cuidadoso de los par√°metros y puede requerir mucho tiempo de entrenamiento. Al igual que otros modelos basados en √°rboles, el algoritmo funciona y escala bien con una mezcla de caracter√≠sticas binarias y continuas. Asimismo, tambi√©n arrastra el problema de los √°rboles de decisi√≥n en los casos en que los datos est√°n dispersos y tienen una alta dimensionalidad. Veamos un ejemplo con scikit-learn utilizando los mismos datos de breast cancer del ejemplo anterior.\n# Armando el modelo con parametro max_depth gbrt = GradientBoostingClassifier(random_state=0, n_estimators=500, max_depth=1, learning_rate=0.01) # Ajustando el modelo gbrt.fit(X_train, y_train) print(\u0026#39;Precisi√≥n Gradient Boosting train/test {0:.3f}/{1:.3f}\u0026#39; .format(gbrt.score(X_train, y_train), gbrt.score(X_test, y_test))) Precisi√≥n Gradient Boosting train/test 0.991/0.937 # Graficando la importancia de cada atributo n_atributos = cancer.data.shape[1] plt.barh(range(n_atributos), gbrt.feature_importances_, align=\u0026#39;center\u0026#39;) plt.yticks(np.arange(n_atributos), cancer.feature_names) plt.xlabel(\u0026#34;Importancia de atributo\u0026#34;) plt.ylabel(\u0026#34;Atributo\u0026#34;) plt.show(); Los principales par√°metros de los modelos de √°rboles de decisi√≥n con Gradient boosting son el n√∫mero de √°rboles, n_estimators, y la tasa de aprendizaje, learning_rate, que controla el grado en que a cada √°rbol se le permite corregir los errores de los √°rboles anteriores. Estos dos par√°metros est√°n altamente interconectados en el sentido de que si bajamos el valor en la tasa de aprendizaje vamos a necesitar un n√∫mero mayor de √°rboles para construir un modelo de complejidad similar. Como podemos ver en el ejemplo, aplicando Gradient boosting con √°rboles de tan solo un nivel de profundidad, logramos una precisi√≥n del 99 % sobre los datos de entrenamiento y del 93 % sobre los datos de evaluaci√≥n.\nSi deseamos aplicar el algoritmo de Gradient boosting a un problema de gran escala, entonces la librer√≠a que sobresale por su facilidad de utilizaci√≥n y rendimiento es XGBoost.\nXGBoost# XGBoost significa eXtreme Gradient Boosting. Es el algoritmo que ha estado dominando recientemente los problemas Machine learning y las competiciones de Kaggle con datos estructurados o tabulares. XGBoost es una implementaci√≥n de √°rboles de decisi√≥n con Gradient boosting dise√±ada para minimizar la velocidad de ejecuci√≥n y maximizar el rendimiento. Posee una interface para varios lenguajes de programaci√≥n, entre los que se incluyen Python, R, Julia y Scala.\nInternamente, XGBoost representa todos los problemas como un caso de modelado predictivo de regresi√≥n que s√≥lo toma valores num√©ricos como entrada. Si nuestros datos est√°n en un formato diferente, primero vamos a tener que transformarlos para poder hacer uso de todo el poder de esta librer√≠a. El hecho de trabajar s√≥lo con datos num√©ricos es lo que hace que esta librer√≠a sea tan eficiente.\nVeamos como la podemos utilizar en Python con un ejemplo. Para este caso vamos a trabajar con el dataset UCI breast-cancer, el cual contiene todos datos categ√≥ricos que vamos a tener que transformar. Este conjunto de datos describe los detalles t√©cnicos de las biopsias de c√°ncer de mama y la tarea de predicci√≥n es predecir si el paciente tiene o no una recurrencia del c√°ncer, o no.\n# Ejemplo de XGBoost # cargando los datos cancer2 = pd.read_csv(\u0026#39;https://relopezbriega.github.io/downloads/datasets-uci-breast-cancer.csv\u0026#39;) cancer2.head() Unnamed: 0 age menopause tumor-size inv-nodes node-caps deg-malig breast breast-quad irradiat Class 0 0 '40-49' 'premeno' '15-19' '0-2' 'yes' '3' 'right' 'left_up' 'no' 'recurrence-events' 1 1 '50-59' 'ge40' '15-19' '0-2' 'no' '1' 'right' 'central' 'no' 'no-recurrence-events' 2 2 '50-59' 'ge40' '35-39' '0-2' 'no' '2' 'left' 'left_low' 'no' 'recurrence-events' 3 3 '40-49' 'premeno' '35-39' '0-2' 'yes' '3' 'right' 'left_low' 'yes' 'no-recurrence-events' 4 4 '40-49' 'premeno' '30-34' '3-5' 'yes' '2' 'left' 'right_up' 'no' 'recurrence-events' # Divido los datos en data y target. cancer_data = cancer2.values cancer2.data = cancer_data[:,0:9] cancer2.data = cancer2.data.astype(str) cancer2.target = cancer_data[:,9] cancer2.data.shape (286, 9) # Aplico el enconding para transformar los datos de entrada a valores # numericos utilizando OneHotEncoder encoded_data = None for i in range(0, cancer2.data.shape[1]): label_encoder = LabelEncoder() feature = label_encoder.fit_transform(cancer2.data[:,i]) feature = feature.reshape(cancer2.data.shape[0], 1) onehot_encoder = OneHotEncoder(sparse=False) feature = onehot_encoder.fit_transform(feature) if encoded_data is None: encoded_data = feature else: encoded_data = np.concatenate((encoded_data, feature), axis=1) # Aplico LaberEncoder a los valores de la variable target. label_encoder = LabelEncoder() label_encoder = label_encoder.fit(cancer2.target) encoded_y = label_encoder.transform(cancer2.target) # Separando los datos en sets de entrenamiento y evaluaci√≥n X_train, X_test, y_train, y_test = train_test_split(encoded_data, encoded_y, random_state=1) # Construyo el modelo y ajusto los datos. modelo = xgb.XGBClassifier() modelo.fit(X_train, y_train) # Realizo las predicciones y_pred = modelo.predict(X_train) predicciones = [round(value) for value in y_pred] # Eval√∫o las predicciones precision_train = accuracy_score(y_train, predicciones) # Repito el proceso con datos de evaluacion y_pred = modelo.predict(X_test) predicciones = [round(value) for value in y_pred] # Eval√∫o las predicciones precision_test = accuracy_score(y_test, predicciones) print(modelo) print(\u0026#39;Precisi√≥n xgboost train/test {0:.3f}/{1:.3f}\u0026#39; .format(precision_train, precision_test)) XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3, min_child_weight=1, missing=None, n_estimators=100, nthread=-1, objective='binary:logistic', reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=0, silent=True, subsample=1) Precisi√≥n xgboost train/test 0.879/0.694 En este ejemplo logramos una precisi√≥n de 88 % con los datos de entrenamiento y del 69 % con los datos de evaluaci√≥n.\nCon esto termina este art√≠culo. Espero les haya sido de utilidad y puedan explorar todo el poder predictivo de XGBoost. Asimismo, otra implementaci√≥n de Gradient boosting que deber√≠amos tener en cuenta ya que tambi√©n ha obtenido muy buenos resultados en t√©rminos de precisi√≥n y rendimiento es LightGBM, que forma parte del Distributed Machine Learning Toolkit de Microsoft.\nSaludos!\nEste post fue escrito por Ra√∫l e. L√≥pez Briega utilizando Jupyter notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2017-06-10","id":9,"permalink":"/blog/2017/06/10/boosting-en-machine-learning-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, Analisis de datos y Python. El contenido esta bajo la licencia BSD.\n\u0026ldquo;La opini√≥n de toda una multitud es siempre m√°s cre√≠ble que la de una minor√≠a.\u0026rdquo;\nMiguel de Unamuno\nIntroducci√≥n# La meta de construir sistemas que puedan adaptarse a sus entornos y aprender de su experiencia ha atra√≠do a investigadores de muchos campos, como la Inform√°tica, Matem√°ticas, F√≠sica, Neurociencia y la Ciencia cognitiva.","tags":["python","estadistica","probabilidad","Machine Learning","boosting","xgboost","analisis de datos"],"title":"Boosting en Machine Learning con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega para el sitio de capacitaciones de IAAR. El contenido esta bajo la licencia BSD.\nEste art√≠culo fue publicado originalmente en el sitio de capacitaciones de IAAR.\nIntroducci√≥n# El cerebro es el √≥rgano m√°s incre√≠ble del cuerpo humano. Establece la forma en que percibimos las im√°genes, el sonido, los olores, los sabores y el tacto. Nos permite almacenar recuerdos, experimentar emociones e incluso so√±ar. Sin el, ser√≠amos organismos primitivos, incapaces de otra cosa que el m√°s simple de los reflejos. El cerebro es, en definitiva, lo que nos hace inteligentes. Durante d√©cadas hemos so√±ado con construir m√°quinas inteligentes con cerebros como los nuestros; asistentes robotizados para limpiar nuestras casas, coches que se conducen por s√≠ mismos, microscopios que detecten enfermedades autom√°ticamente. Pero construir estas m√°quinas artificialmente inteligentes nos obliga a resolver algunos de los problemas computacionales m√°s complejos que hemos tenido; problemas que nuestros cerebros ya pueden resolver en una fracci√≥n de segundos. La forma de atacar y resolver estos problemas, es el campo de estudio de la Inteligencia Artificial.\n¬øQu√© es la Inteligencia Artificial?# Definir el concepto de Inteligencia Artificial no es nada f√°cil. Una definici√≥n sumamente general ser√≠a que la IA es el estudio de la infrom√°tica centr√°ndose en el desarrollo de software o m√°quinas que exhiben una inteligencia humana.\nObjetivos de la Inteligencia Artificial# Los objetivos principales de la IA incluyen la deducci√≥n y el razonamiento, la representaci√≥n del conocimiento, la planificaci√≥n, el procesamiento del lenguaje natural (NLP), el aprendizaje, la percepci√≥n y la capacidad de manipular y mover objetos. Los objetivos a largo plazo incluyen el logro de la Creatividad, la Inteligencia Social y la Inteligencia General (a nivel Humano).\nCuatro enfoques distintos# Podemos distinguir cuatro enfoques distintos de abordar el problema de la Inteligencia Artificial.\nSistemas que se comportan como humanos: Aqu√≠ la idea es desarrollar m√°quinas capaces de realizar funciones para las cuales se requerir√≠a un humano inteligente. Dentro de este enfoque podemos encontrar la famosa Prueba de Turing. Para poder superar esta prueba, la m√°quina deber√≠a poseer las siguientes capacidades: Procesamiento de lenguaje natural, que le permita comunicarse satisfactoriamente. Representaci√≥n del conocimiento, para almacenar lo que se conoce o se siente. Razonamiento autom√°tico, para utilizar la informaci√≥n almacenada para responder a preguntas y extraer nuevas conclusiones. Aprendizaje autom√°tico, para adaptarse a nuevas circunstancias y para detectar y extrapolar patrones. Visi√≥n computacional, para percibir objetos. Rob√≥tica, para manipular y mover objetos. Sistemas que piensan como humanos: Aqu√≠ la idea es hacer que las m√°quinas piensen como humanos en el sentido m√°s literal; es decir, que tengan capacidades cognitivas de toma de decisiones, resoluci√≥n de problemas, aprendizaje, etc. Dentro de este enfoque podemos encontrar al campo interdisciplinario de la ciencia cognitiva, en el cual convergen modelos computacionales de IA y t√©cnicas experimentales de psicolog√≠a intentando elaborar teor√≠as precisas y verificables sobre el funcionamiento de la mente humana.\nSistemas que piensan racionalmente: Aqu√≠ la idea es descubrir los c√°lculos que hacen posible percibir, razonar y actuar; es decir, encontrar las leyes que rigen el pensamiento racional. Dentro de este enfoque podemos encontrar a la L√≥gica, que intenta expresar las leyes que gobiernan la manera de operar de la mente.\nSistemas que se comportan racionalmente: Aqu√≠ la idea es dise√±ar agentes inteligentes. Dentro de este enfoque un agente racional es aquel que act√∫a con la intenci√≥n de alcanzar el mejor resultado o, cuando hay incertidumbre, el mejor resultado esperado. Un elemento importante a tener en cuenta es que tarde o temprano uno se dar√° cuenta de que obtener una racionalidad perfecta (hacer siempre lo correcto) no es del todo posible en entornos complejos. La demanda computacional que esto implica es demasiado grande, por lo que debemos conformarnos con una racionalidad limitada. Como lo que se busca en este enfoque es realizar inferencias correctas, se necesitan las mismas habilidades que para la Prueba de Turing, es decir, es necesario contar con la capacidad para representar el conocimiento y razonar bas√°ndonos en √©l, porque ello permitir√° alcanzar decisiones correctas en una amplia gama de situaciones. Es necesario ser capaz de generar sentencias comprensibles en lenguaje natural, ya que el enunciado de tales oraciones permite a los agentes desenvolverse en una sociedad compleja. El aprendizaje no se lleva a cabo por erudici√≥n exclusivamente, sino que profundizar en el conocimiento de c√≥mo funciona el mundo facilita la concepci√≥n de estrategias mejores para manejarse en √©l.\nFundamentos de la Inteligencia artificial# Existen varias disciplinas que han contribuido con ideas, puntos de vista y t√©cnicas al desarrollo del campo de la Inteligencia Artificial. Ellas son:\nFilosof√≠a# Muchas han sido las contribuciones de la Filosof√≠a a las ciencias. En el campo de la Inteligencia Artificial a contribuido con varios aportes entre los que se destacan los conceptos de IA d√©bil y IA fuerte.\nLa IA d√©bil se define como la inteligencia artificial racional que se centra t√≠picamente en una tarea estrecha. La inteligencia de la IA d√©bil es limitada, no hay autoconciencia o inteligencia genuina. Siri es un buen ejemplo de una IA d√©bil que combina varias t√©cnicas de IA d√©bil para funcionar. Siri puede hacer un mont√≥n de cosas por nosotros, pero a medida que intentamos tener conversaciones con el asistente virtual, nos damos cuenta de cuan limitada es.\nLa IA fuerte es aquella inteligencia artificial que iguala o excede la inteligencia humana promedio. Este tipo de AI ser√° capaz de realizar todas las tareas que un ser humano podr√≠a hacer. Hay mucha investigaci√≥n en este campo, pero todav√≠a no han habido grandes avances.\nMuchos son los debates filos√≥ficos alrededor de la inteligencia artificial, para aquellos interesados en los aspectos filos√≥ficos les recomiendo inscribirse en nuestro grupo de debate de IAAR\nMatem√°ticas# Si de ciencias aplicadas se trata, no puede faltar el aporte de las Matem√°ticas. Para entender y desarrollar los principales algoritmos que se utilizan en el campo de la Inteligencia Artificial, deber√≠amos tener nociones de:\n√Ålgebra lineal# El √°lgebra lineal es una rama de las matem√°ticas que estudia conceptos tales como vectores, matrices, tensores, sistemas de ecuaciones lineales y en su enfoque de manera m√°s formal, espacios vectoriales y sus transformaciones lineales. Una buena comprensi√≥n del √°lgebra lineal es esencial para entender y trabajar con muchos algoritmos de Machine Learning, y especialmente para los algoritmos de Deep Learning.\nC√°lculo# El C√°lculo es el campo de la matem√°tica que incluye el estudio de los l√≠mites, derivadas, integrales y series infinitas, y m√°s concretamente se puede decir que es el estudio del cambio. Particularmente para el campo de la Inteligencia Artificial algunos conceptos que se deber√≠an conocer incluyen: C√°lculo Diferencial e Integral, Derivadas Parciales, Funciones de Valores Vectoriales, y Gradientes.\nOptimizaci√≥n matem√°tica# La Optimizaci√≥n matem√°tica es la herramienta matem√°tica que nos permite optimizar decisiones, es decir, seleccionar la mejor alternativa de un conjunto de criterios disponibles. Su comprensi√≥n es fundamental para poder entender la eficiencia computacional y la escalabilidad de los principales algoritmos de Machine Learning y Deep Learning, los cuales suelen trabajar con matrices dispersas de gran tama√±o.\nProbabilidad y estad√≠stica# La Probabilidad y estad√≠stica es la rama de la matem√°tica que trata con la incertidumbre, la aleatoriedad y la inferencia. Sus conceptos son fundamentales para cualquier algoritmo de Machine Learning o Deep Learning.\nUna buena introducci√≥n a cada uno de estos campos de las matem√°ticas que son fundamentales para la Inteligencia Artificial, la pueden encontrar en este mismo blog.\nLing√º√≠stica# La Ling√º√≠stica moderna y la Inteligencia Artificial nacieron al mismo tiempo y maduraron juntas, solap√°ndose en un campo h√≠brido llamado ling√º√≠stica computacional o procesamiento de lenguaje natural. El entendimiento del lenguaje requiere la comprensi√≥n de la materia bajo estudio y de su contexto, y no solamente el entendimiento de la estructura de las sentencias; lo que lo convierte en un problema bastante complejo de abordar.\nNeurociencias# La Neurociencia es el estudio del sistema neurol√≥gico, y en especial del cerebro. La forma exacta en la que en un cerebro se genera el pensamiento es uno de los grandes misterios de la ciencia. El hecho de que una colecci√≥n de simples c√©lulas puede llegar a generar razonamiento, acci√≥n, y conciencia es un enigma a resolver. Cerebros y computadores realizan tareas bastante diferentes y tienen propiedades muy distintas. Seg√∫n los c√°lculos de los expertos se estima que para el 2020 las computadoras igualaran la capacidad de procesamiento de los cerebros. Muchos modelos de IA fueron inspirados en la estructura y el funcionamiento de nuestro cerebro.\nPsicolog√≠a# La Psicolog√≠a trata sobre el estudio y an√°lisis de la conducta y los procesos mentales de los individuos y grupos humanos. La rama que m√°s influencia ha tenido para la Inteligencia Artificial es la de la psicolog√≠a cognitiva que se encarga del estudio de la cognici√≥n; es decir, de los procesos mentales implicados en el conocimiento. Tiene como objeto de estudio los mecanismos b√°sicos y profundos por los que se elabora el conocimiento, desde la percepci√≥n, la memoria y el aprendizaje, hasta la formaci√≥n de conceptos y razonamiento l√≥gico. Las teor√≠a descritas por esta rama han sido utilizados para desarrollar varios modelos de Inteligencia Artificial y Machine Learning\nRamas de la Inteligencia artificial# Dentro de la Inteligencia Artificial podemos encontrar distintas ramas, entre las que se destacan:\nMachine Learning# El Machine Learning es el dise√±o y estudio de las herramientas inform√°ticas que utilizan la experiencia pasada para tomar decisiones futuras; es el estudio de programas que pueden aprenden de los datos. El objetivo fundamental del Machine Learning es generalizar, o inducir una regla desconocida a partir de ejemplos donde esa regla es aplicada. El ejemplo m√°s t√≠pico donde podemos ver el uso del Machine Learning es en el filtrado de los correo basura o spam. Mediante la observaci√≥n de miles de correos electr√≥nicos que han sido marcados previamente como basura, los filtros de spam aprenden a clasificar los mensajes nuevos.\nEl Machine Learning tiene una amplia gama de aplicaciones, incluyendo motores de b√∫squeda, diagn√≥sticos m√©dicos, detecci√≥n de fraude en el uso de tarjetas de cr√©dito, an√°lisis del mercado de valores, clasificaci√≥n de secuencias de ADN, reconocimiento del habla y del lenguaje escrito, juegos y rob√≥tica. Pero para poder abordar cada uno de estos temas es crucial en primer lugar distingir los distintos tipos de problemas de Machine Learning con los que nos podemos encontrar.\nAprendizaje supervisado# En los problemas de aprendizaje supervisado se ense√±a o entrena al algoritmo a partir de datos que ya vienen etiquetados con la respuesta correcta. Cuanto mayor es el conjunto de datos, el algoritmo podr√° generalizar en una forma m√°s precisa. Una vez concluido el entrenamiento, se le brindan nuevos datos, ya sin las etiquetas de las respuestas correctas, y el algoritmo de aprendizaje utiliza la experiencia pasada que adquiri√≥ durante la etapa de entrenamiento para predecir un resultado.\nAprendizaje no supervisado# En los problemas de aprendizaje no supervisado, el algoritmo es entrenado usando un conjunto de datos que no tiene ninguna etiqueta; en este caso, nunca se le dice al algoritmo lo que representan los datos. La idea es que el algoritmo pueda encontrar por si solo patrones que ayuden a entender el conjunto de datos.\nAprendizaje por refuerzo# En los problemas de aprendizaje por refuerzo, el algoritmo aprende observando el mundo que le rodea. Su informaci√≥n de entrada es el feedback o retroalimentaci√≥n que obtiene del mundo exterior como respuesta a sus acciones. Por lo tanto, el sistema aprende a base de ensayo-error. Un buen ejemplo de este tipo de aprendizaje lo podemos encontrar en los juegos, donde vamos probando nuevas estrategias y vamos seleccionando y perfeccionando aquellas que nos ayudan a ganar el juego. A medida que vamos adquiriendo m√°s practica, el efecto acumulativo del refuerzo a nuestras acciones victoriosas terminar√° creando una estrategia ganadora.\nDeep Learning# El Deep Learning constituye un conjunto particular de algoritmos de Machine Learning que utilizan estructuras profundas de redes neuronales para encontrar patrones en los datos. Estos tipos de algoritmos cuentan actualmente con un gran inter√©s, ya que han demostrado ser sumamente exitosos para resolver determinados tipos de problemas; como por ejemplo, el reconocimiento de im√°genes. Muchos consideran que este tipo de modelos son los que en el futuro nos llevaran a resolver definitivamente el problema de la Inteligencia Artificial.\nRazonamiento probabil√≠stico# El razonamiento probabil√≠stico se encarga de lidiar con la incertidumbre inherente de todo proceso de aprendizaje. El problema para crear una Inteligencia Artificial entonces se convierte en encontrar la forma de trabajar con informaci√≥n ruidosa, incompleta e incluso muchas veces contradictoria. Estos algoritmos est√°n sumamente ligados a la estad√≠stica bayesiana; y la principal herramienta en la que se apoyan es en el teorema de Bayes.\nAlgortimos gen√©ticos# Los algoritmos gen√©ticos se basan en la idea de que la madre de todo aprendizaje es la selecci√≥n natural. Si la Naturaleza pudo crearnos, puede crear cualquier cosa; por tal motivo lo √∫nico que deber√≠amos hacer para alcanzar una Inteligencia Artificial es simular sus mecanismos en una computadora. La idea de estos algoritmos es imitar a la Evoluci√≥n; funcionan seleccionando individuos de una poblaci√≥n de soluciones candidatas, y luego intentando producir nuevas generaciones de soluciones mejores que las anteriores una y otra vez hasta aproximarse a una soluci√≥n perfecta.\nAplicaciones de la Inteligencia artificial# Las t√©cnicas de la Inteligencia Artificial pueden ser aplicadas en una gran variedad de industrias y situaciones, como ser:\nMedicina# Apoy√°ndose en las herramientas que proporciona la Inteligencia Artificial, los doctores podr√≠an realizar diagn√≥sticos m√°s certeros y oportunos, lo que llevar√≠a a mejores tratamientos y m√°s vidas salvadas.\nAutos aut√≥nomos# Utilizando Inteligencia Artificial podr√≠amos crear autos aut√≥nomos que aprendan de los datos y experiencias de millones de otros autos, mejorando el tr√°fico y haciendo mucho m√°s segura la conducci√≥n.\nBancos# Utilizando t√©cnicas de Machine Learning los bancos pueden detectar fraudes antes de que ocurran por medio de analizar los patrones de comportamiento de gastos e identificando r√°pidamente actividades sospechosas.\nAgricultura# En Agricultura se podr√≠a optimizar el rendimiento de los cultivos por medio de la utilizaci√≥n de las t√©cnicas de Inteligencia Artificial para analizar los datos del suelo y del clima en tiempo real, logrando producir m√°s alimentos incluso con climas perjudiciales.\nEducaci√≥n# En la Educaci√≥n se podr√≠an utilizar las t√©cnicas de la Inteligencia Artificial para dise√±ar programas de estudios personalizados basados en datos que mejoren el rendimiento y el ritmo de aprendizaje de los alumnos.\nLa √©tica y los riesgos de desarrollar una Inteligencia Artificial# Actualmente tambi√©n ha surgido un debate √©tico alrededor de la Inteligencia Artificial. Algunos de los pensadores m√°s importantes del planeta han establecido su preocupaci√≥n sobre el progreso de la IA. Entre los problemas que puede traer aparejado el desarrollo de la Inteligencia Artificial, podemos encontrar los siguientes:\nLas personas podr√≠an perder sus trabajos por la automatizaci√≥n. Las personas podr√≠an tener demasiado (o muy poco) tiempo de ocio. Las personas podr√≠an perder el sentido de ser √∫nicos. Las personas podr√≠an perder algunos de sus derechos privados. La utilizaci√≥n de los sistemas de IA podr√≠a llevar a la p√©rdida de responsabilidad. El √©xito de la IA podr√≠a significar el fin de la raza humana. El debate sobre los beneficios y riesgos del desarrollo de la Inteligencia Artificial est√° todav√≠a abierto.\n¬øC√≥mo iniciarse en el campo de la Inteligencia artificial?# Si luego de leer esta introducci√≥n, te has quedado fascinado por el campo de la Inteligencia Artificial y quieres incursionar en el mismo, aqu√≠ te dejo algunas recomendaciones para iniciarse.\nIAAR# IAAR es la comunidad argentina de inteligencia artificial. Agrupa a ingenieros, desarrolladores, emprendedores, investigadores, entidades gubernamentales y empresas en pos del desarrollo √©tico y humanitario de las tecnolog√≠as cognitivas. Para comenzar a formar parte de la comunidad pueden inscribirse en los grupos de facebook: IAAR, Debates, Proyectos, Capacitaci√≥n; y/o en el meetup.\nProgramaci√≥n# Para poder trabajar en problemas relacionados al campo de la Inteligencia Artificial es necesario saber programar. Los principales lenguajes que se utilizan son Python y R. En los repositorios de Academia de IAAR van a poder encontrar material sobre estos lenguajes.\nFrameworks# Existen varios frameworks open source que nos facilitan el trabajar con modelos de Deep Learning, entre los que se destacan:\nTensorFlow: TensorFlow es un frameworks desarrollado por Google. Es una librer√≠a de c√≥digo libre para computaci√≥n num√©rica usando grafos de flujo de datos que utiliza el lenguaje Python. PyTorch: PyTorch es un framework de Deep Learning que utiliza el lenguaje Python y cuenta con el apoyo de Facebook. Caffe: Caffe es un framework de Deep Learning hecho con expresi√≥n, velocidad y modularidad en mente, el cual es desarrollado por la universidad de Berkeley. CNTK: CNTK es un conjunto de herramientas, desarrolladas por Microsoft, f√°ciles de usar, de c√≥digo abierto que entrena algoritmos de Deep Learning para aprender como el cerebro humano. Theano: Theano es una librer√≠a de Python que permite definir, optimizar y evaluar expresiones matem√°ticas que involucran tensores de manera eficiente. DeepLearning4j: DeepLearning4j Es una librer√≠a open source para trabajar con modelos de Deep Learning distribuidos utilizando el lenguaje Java. Bots# Una de las ramas con mayor crecimiento y que m√°s se ha beneficiado con el boom de la Inteligencia Artificial es la de los Bots. Generar peque√±os Bots que puedan tener conversaciones b√°sicas con los usuarios es bastante simple. Pueden encontrar una gu√≠a con una gran n√∫mero de herramientas en el blog de capacitaci√≥n de IAAR.\nAqu√≠ concluye esta introducci√≥n por el fascinante campo de la Inteligencia Artificial. Recuerden anotarse en el grupo de facebook de IAAR y al meetup.\nSaludos!\nEste post fue escrito por Ra√∫l e. L√≥pez Briega utilizando Jupyter notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2017-06-05","id":10,"permalink":"/blog/2017/06/05/introduccion-a-la-inteligencia-artificial/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega para el sitio de capacitaciones de IAAR. El contenido esta bajo la licencia BSD.\nEste art√≠culo fue publicado originalmente en el sitio de capacitaciones de IAAR.\nIntroducci√≥n# El cerebro es el √≥rgano m√°s incre√≠ble del cuerpo humano. Establece la forma en que percibimos las im√°genes, el sonido, los olores, los sabores y el tacto. Nos permite almacenar recuerdos, experimentar emociones e incluso so√±ar.","tags":["python","programacion","analisis de datos","estadistica","probabilidad","inteligencia artificial","tensorflow","redes neuronales"],"title":"Introducci√≥n a la Inteligencia Artificial"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\n\u0026ldquo;Las preguntas m√°s importantes de la vida son, en su mayor parte, nada m√°s que problemas de probabilidad.\u0026rdquo;\nPierre-Simon Lapace\nIntroducci√≥n: La incertidumbre y el problema de la inducci√≥n# ¬øC√≥mo se deben evaluar las hip√≥tesis?¬øCu√°l es el papel de la evidencia en este proceso?¬øCu√°les son los experimentos que debemos realizar para obtener la mayor informaci√≥n posible?. √âstas son preguntas que est√°n en los cimientos de toda b√∫squeda cient√≠fica. Las hip√≥tesis cient√≠ficas tienen un car√°cter general en relaci√≥n con las observaciones emp√≠ricas que se supone deben explicar, teniendo implicaciones sobre fen√≥menos y acontecimientos que no podemos encontrar en ninguna evidencia real. Existe, por lo tanto, un vac√≠o l√≥gico entre la informaci√≥n derivada de la observaci√≥n emp√≠rica y el contenido de nuestras teor√≠as cient√≠ficas. ¬øC√≥mo, entonces, esta informaci√≥n nos da una confianza razonable en esas teor√≠as? √âste, es el tradicional problema de la inducci√≥n.\nDavid Hume, demostr√≥ correctamente, que partiendo solamente de las evidencias del pasado y del presente, no podemos inferir con seguridad nada sobre el futuro. Que hayamos visto solamente miles de cisnes blancos, no quiere decir que todos los cisnes sean blancos; siempre existir√° la posibilidad de que podamos encontrar un cisne negro que falsifique toda nuestra teor√≠a. Este problema, hace que en esencia todas las teor√≠as tengan un car√°cter probabil√≠stico. Desde hace ya tiempo se considera que las teor√≠as cient√≠ficas se extienden m√°s all√° de cualquier dato experimental y por lo tanto no pueden ser verificadas (es decir, logicamente implicadas) por ellos. Si bien existe un consenso en que la certeza absoluta no puede ser nunca alcanzable; la mayor√≠a de los cient√≠ficos coinciden en que las teor√≠as pueden alcanzar un estado intermedio entre la certeza absoluta y la falsificaci√≥n; el cual depender√° de la calidad de las observaciones y de c√≥mo la teor√≠a se ve afectada por nuevas evidencias. Desde esta perspectiva, entonces la pregunta crucial pasa a ser ¬øc√≥mo adaptamos nuestras creencias a medida que vamos incorporando nuevas evidencias sobre nuestras teor√≠as? la respuesta a esta pregunta, la podemos encontrar en el teorema de Bayes.\nEl Teorema de Bayes# Thomas Bayes fue un ministro presbiteriano y matem√°tico ingl√©s que estudi√≥ la relaci√≥n √≠ntima que existe entre la probabilidad, la predicci√≥n y el progreso cient√≠fico. Su trabajo se centr√≥ principalmente en c√≥mo formulamos nuestras creencias probabil√≠sticas sobre el mundo que nos rodea cuando nos encontramos con nuevos datos o evidencias. El argumento de Bayes no es que el mundo es intr√≠nsecamente probabil√≠stico o incierto, ya que √©l era un creyente en la divina perfecci√≥n; sino que aprendemos sobre el mundo a trav√©s de la aproximaci√≥n, acerc√°ndonos cada vez m√°s a la verdad a medida que recogemos m√°s evidencias. Este argumento lo expres√≥ matem√°ticamente a trav√©s de su famoso teorema:\n$$P(H|D) = \\frac{P(D|H)P(H)}{P(D)} $$ En donde:\n\\(P(H)\\) es el a priori, la forma de introducir conocimiento previo sobre los valores que puede tomar la hip√≥tesis. A veces cuando no sabemos demasiado se suelen usar a prioris que asignan igual probabilidad a todos los valores de la hip√≥tesis; otras veces se puede elegir a prioris que restrinjan los valores a rangos razonables, por ejemplo solo valores positivos; y otras veces contamos con informaci√≥n mucho m√°s precisa, como experimentos previos o l√≠mites impuesto por alguna teor√≠a.\n\\(P(D|H)\\) es el likelihood, la forma de incluir nuestros datos en el an√°lisis. Es una expresi√≥n matem√°tica que especifica la plausibilidad de los datos. A medida que la cantidad de datos aumenta, el likelihood tiene cada vez m√°s peso en los resultados. Debemos tener en cuenta que si bien el likelihood se asemeja a una probabilidad, en realidad no lo es; el likelihood de una hip√≥tesis \\(H\\), dados los datos \\(D\\) va a ser proporcional a la probabilidad de obtener \\(D\\) dado que \\(H\\) es verdadera. Como el likelihood no es una probabilidad tampoco tiene que respetar las leyes de las probabilidades y por lo tanto no necesariamente tiene que sumar 1.\n\\(P(H|D)\\) es el a posteriori, la distribuci√≥n de probabilidad final para la hip√≥tesis. Es la consecuencia l√≥gica de haber usado un conjunto de datos, un likelihood y un a priori. Se lo suele pensar como la versi√≥n actualizada del a priori luego de que hemos agregado los datos adicionales.\n\\(P(D)\\) es el likelihood marginal o evidencia, la probabilidad de observar los datos \\(D\\) promediado sobre todas las posibles hip√≥tesis \\(H\\). En general, la evidencia puede ser vista como una simple constante de normalizaci√≥n que en la mayor√≠a de los problemas pr√°cticos puede omitirse sin demasiada perdida de generalidad.\nSi los fundamentos filos√≥ficos del teorema de Bayes son sorprendentemente ricos, sus matem√°ticas son incre√≠blemente simples. En su forma m√°s b√°sica, no es m√°s que una expresi√≥n algebraica con tres variables conocidas y una inc√≥gnita; y que trabaja con probabilidades condicionales; nos dice la probabilidad de que una hip√≥tesis \\(H\\) sea verdadera si alg√∫n evento \\(D\\) ha sucedido. El teorema de Bayes es √∫til porque lo que normalmente sabemos es la probabilidad de los efectos dados las causas, pero lo que queremos saber es la probabilidad de las causas dadas los efectos. Por ejemplo, podemos saber cual es el porcentaje de pacientes con gripe que tiene fiebre, pero lo que realmente queremos saber es la probabilidad de que un paciente con fiebre tenga gripe. El teorema de Bayes nos permite ir de uno a otro con suma facilidad.\nLa inferencia Bayesiana# Toda forma de inferencia que realicemos sobre el mundo que nos rodea, debe indefectiblemente lidiar con la incertidumbre. Existen por lo menos, tres tipos de incertidumbre con la que nos debemos enfrentar:\nIgnorancia, los l√≠mites de nuestro conocimiento nos llevan a ser ignorantes sobre muchas cosas. Aleatoriedad, es imposible negar la influencia del azar en casi todo lo que nos rodea; incluso aunque podamos saber todo sobre una moneda y la forma de lanzarla, es imposible predecir con anterioridad si va a caer cara o seca. Vaguedad, muchos de los conceptos que utilizamos en nuestro pensamiento tienen cierto grado de subjetividad en su definici√≥n. ¬øc√≥mo calificar√≠amos si una persona es valiente o no?. Cada uno de nosotros puede tener una apreciaci√≥n diferente del concepto de valent√≠a. La inferencia bayesiana es la filosof√≠a que afirma que para entender la opini√≥n humana como debe ser, limitada por la ignorancia y la incertidumbre; debemos utilizar al c√°lculo de probabilidad como la herramienta m√°s importante para representar la fortaleza de nuestras creencias.\nEn esencia, la inferencia bayesiana combina nuestra experiencia previa, en la forma de la probabilidad a priori; con los datos observados, en la forma del likelihood; para interpretarlos y arribar a una probabilidad a posteriori. La inferencia bayesiana no nos va a garantizar que podamos alcanzar la respuesta correcta. En su lugar, nos va a proporcionar la probabilidad de que cada una de un n√∫mero de respuestas alternativas, sea verdadera. Y luego podemos utilizar esta informaci√≥n para encontrar la respuesta que m√°s probablemente sea la correcta. En otras palabras, nos proporciona un mecanismo para hacer una especie de adivinaci√≥n basada en informaci√≥n.\nBayes en el diagnostico m√©dico# Para que quede m√°s claro, ilustremos la aplicaci√≥n de la inferencia bayesiana con un simple ejemplo del diagnostico m√©dico, uno de los campos d√≥nde m√°s √©xito ha tenido. Supongamos que nos hicimos un estudio y nos ha dado positivo para una rara enfermedad que solo el 0.3 % de la poblaci√≥n tiene. La tasa de efectividad de este estudio es del 99 %, es decir, que solo da falsos positivos en el 1 % de los casos. ¬øCu√°n probable es que realmente tengamos la enfermedad?.\nEn un principio, nos ver√≠amos tentados a responder que hay un 99 % de probabilidad de que tengamos la enfermedad; pero en este caso nos estar√≠amos olvidando del concepto importante del a priori. Sabemos con anterioridad que la enfermedad es extremadamente rara (solo el 0.3 % la tiene); si incluimos esta informaci√≥n previa en nuestro c√°lculo de probabilidad y aplicamos el teorema de Bayes podemos llegar a una conclusi√≥n totalmente distinta.\n$$ P(\\text{ enfermedad | pos}) = \\frac{P(\\text{ pos | enfermedad})P( \\text{enfermedad})}{P(\\text{pos})}$$ # Ejemplo simple teorema de Bayes aplicado a estimaci√≥n de un s√≥lo par√°metro. a_priori = 0.003 likelihood = 0.99 evidencia = 0.01 a_posteriori = likelihood * a_priori / evidencia a_posteriori 0.297 Como vemos, luego de aplicar el teorema de Bayes llegamos a la conclusi√≥n de que en realidad nuestra probabilidad de estar realmente enfermo es de s√≥lo 30 % y no de 99 %, ya que podemos ser uno de los falsos positivos del estudio y la enfermedad es realmente muy rara. Como este ejemplo demuestra, la inclusi√≥n del a priori es sumamente importante para la inferencia bayesiana, por lo cual tambi√©n debemos ser sumamente cuidadosos a la hora de elegirla. Cuando nuestra a priori es fuerte, puede ser sorprendentemente resistente frente a nuevas evidencias.\nRedes Bayesianas# El teorema de Bayes nos permite actualizar las probabilidades de variables cuyo estado no hemos observado dada una serie de nuevas observaciones. Las redes bayesianas automatizan este proceso, permitiendo que el razonamiento avance en cualquier direcci√≥n a trav√©s de la red de variables. Las redes bayesianas est√°n constituidas por una estructura en forma de grafo, en la que cada nodo representa variables aleatorias (discretas o continuas) y cada arista representa las conexiones directas entre ellas. Estas conexiones suelen representar relaciones de causalidad. Adicionalmente, las redes bayesianas tambi√©n modelan el peso cuantitativo de las conexiones entre las variables, permitiendo que las creencias probabil√≠sticas sobre ellas se actualicen autom√°ticamente a medida que se disponga de nueva informaci√≥n. Al construir una red bayesiana, los principales problemas de modelizaci√≥n que surgen son:\n¬øCu√°les son las variables? ¬øCu√°les son sus valores / estados? ¬øCu√°l es la estructura del grafo? ¬øCu√°les son los par√°metros (probabilidades)? Profundicemos un poco en cada uno de estos puntos.\nNodos y variables# Lo primero que debemos hacer es identificar las variables de inter√©s. Sus valores deben ser mutuamente excluyentes y exhaustivos. Los tipos de nodos discretos m√°s comunes son:\nNodos booleanos, que representan proposiciones tomando los valores binarios Verdadero (V) y Falso (F). En el dominio del diagn√≥stico m√©dico, por ejemplo, un nodo llamado \u0026ldquo;C√°ncer\u0026rdquo; podr√≠a representar la proposici√≥n del que paciente tenga c√°ncer. Valores ordenados Por ejemplo, un nodo \u0026ldquo;Contaminaci√≥n\u0026rdquo; podr√≠a representar la exposici√≥n de un paciente a la contaminaci√≥n del ambiente y tomar los valores {alta, baja}. Valores enteros. Por ejemplo, un nodo llamado \u0026ldquo;Edad\u0026rdquo; puede representar la edad de un paciente y tener valores posibles de 1 a 120. Lo importante es elegir valores que representen el dominio de manera eficiente, pero con suficiente detalle para realizar el razonamiento requerido.\nEstructura# La estructura o topolog√≠a de la red debe captar las relaciones cualitativas entre las variables. En particular, dos nodos deben conectarse directamente si uno afecta o causa al otro, con la arista indicando la direcci√≥n del efecto. Por lo tanto, en nuestro ejemplo de diagn√≥stico m√©dico, podr√≠amos preguntarnos qu√© factores afectan la probabilidad de tener c√°ncer. Si la respuesta es \u0026ldquo;Contaminaci√≥n y Fumar\u0026rdquo;, entonces deber√≠amos agregar aristas desde \u0026ldquo;Contaminaci√≥n\u0026rdquo; y desde \u0026ldquo;Fumador\u0026rdquo; hacia el nodo \u0026ldquo;C√°ncer\u0026rdquo;. Del mismo modo, tener c√°ncer afectar√° la respiraci√≥n del paciente y las posibilidades de tener un resultado positivo de rayos X. Por lo tanto, tambi√©n podemos agregar aristas de \u0026ldquo;C√°ncer\u0026rdquo; a \u0026ldquo;Disnea\u0026rdquo; y \u0026ldquo;RayosX\u0026rdquo;.\nEs deseable construir redes bayesianas lo m√°s compactas posibles por tres razones. Primero, mientras m√°s compacto es el modelo, es m√°s f√°cil de manejar. Segundo, cuando las redes se vuelven demasiado densas, fallan en representar la independencia en forma expl√≠cita. Y Tercero, las redes excesivamente densas no suelen representar las dependencias causales del dominio.\nProbabilidades condicionales# Una vez que tenemos definida la estructura de la red bayesiana, el siguiente paso es cuantificar las relaciones entre los nodos interconectados; esto se hace especificando una probabilidad condicional para cada nodo. Primero, para cada nodo necesitamos mirar todas las posibles combinaciones de valores de los nodos padres. Por ejemplo, continuando con el ejemplo del diagnostico del c√°ncer, si tomamos el nodo \u0026ldquo;C√°ncer\u0026rdquo; con sus dos nodos padres \u0026ldquo;Contaminaci√≥n\u0026rdquo; y \u0026ldquo;Fumador\u0026rdquo; podemos calcular los posibles valores conjuntos { (A, V), (A, F), (B, V), (B, F)}. La tabla de probabilidad condicional especifica para cada uno de estos casos podr√≠a ser la siguiente: {0,05, 0,02, 0,03, 0,001}. Con estos datos, ya estamos en condiciones de representar el grafo de la red bayesiana de nuestro ejemplo.\nRazonando con redes Bayesianas# La tarea b√°sica de cualquier sistema de inferencia probabil√≠stica es la de obtener la distribuci√≥n a posteriori para cada conjunto de nodos. Esta tarea se llama actualizaci√≥n de creencia o inferencia probabil√≠stica. En el caso de las redes bayesianas, el proceso de inferencia es muy flexible, nueva evidencia puede ser introducida en cualquiera de los nodos mientras que las creencias son actualizadas en cualquiera de los otros nodos. En la pr√°ctica, la velocidad del proceso de inferencia va a depender de la estructura y complejidad de la red.\nProgramaci√≥n probabil√≠stica y PyMC3# A pesar de que las redes bayesianas y dem√°s modelos de inferencia bayesiana son conceptualmente simples; a menudo los c√°lculos de sus probabilidades conducen a expresiones que no se pueden resolver en forma anal√≠tica. Durante muchos a√±os, este fue un gran problema y fue probablemente una de las principales razones que obstaculizaron la adopci√≥n de los m√©todos bayesianos. La llegada de las computadoras y el desarrollo de m√©todos num√©ricos que se pueden aplicar para calcular la distribuci√≥n a posteriori de casi cualquier modelo, junto con el avance en las t√©cnicas de muestreo de los m√©todos de Monte-Carlo; han transformado completamente la pr√°ctica del an√°lisis de datos Bayesiano.\nLa posibilidad de automatizar la inferencia probabil√≠stica ha conducido al desarrollo de la Programaci√≥n probabil√≠stica, la cu√°l utiliza las ventajas de los lenguajes de programaci√≥n modernos y nos permite realizar una clara separaci√≥n entre la creaci√≥n del modelo y el proceso de inferencia. En Programaci√≥n probabil√≠stica, especificamos un modelo probabil√≠stico completo escribiendo unos cuantos l√≠neas de c√≥digo y luego la inferencia se realiza en forma autom√°tica.\nPyMC3# PyMC3 es un paquete para Programaci√≥n probabil√≠stica que utiliza el lenguaje de programaci√≥n Python. PyMC3 es lo suficientemente maduro para resolver muchos de los principales problemas estad√≠sticos. Permite crear modelos probabil√≠sticos usando una sintaxis intuitiva y f√°cil de leer que es muy similar a la sintaxis usada para describir modelos probabil√≠sticos.\nVeamos algunos ejemplos:\nEl problema de la moneda# Los problemas de monedas son cl√°sicos cuando hablamos de probabilidad y estad√≠stica, nos permiten ejemplificar conceptos abstractos de forma simple. Asimismo, pueden ser muchas veces conceptualmente similares a situaciones reales, de hecho cualquier problema en donde obtengamos resultados binarios, 0/1, enfermo/sano, spam/no-spam, puede ser pensado como si estuvi√©ramos hablando de monedas. En este caso, la idea es utilizar un modelo bayesiano para inferir si la moneda se encuentra sesgada o no.\nPara este ejemplo, vamos a utilizar una distribuci√≥n binomial como likelihood y una distribuci√≥n beta como a priori. Veamos como lo podemos modelar con PyMC3.\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # importando modulos necesarios import matplotlib.pyplot as plt import numpy as np import pandas as pd import scipy.stats as stats import seaborn as sns import pymc3 as pm import theano.tensor as tt from sklearn import datasets from sklearn.naive_bayes import GaussianNB from sklearn.metrics import confusion_matrix from sklearn.model_selection import train_test_split np.random.seed(1984) #replicar random %matplotlib inline # El problema de la moneda # de 100 lanzamientos 80 caras n = 100 caras = 80 # Creaci√≥n del modelo niter = 2000 with pm.Model() as modelo_moneda: # a priori p = pm.Beta(\u0026#39;p\u0026#39;, alpha=2, beta=2) # likelihood y = pm.Binomial(\u0026#39;y\u0026#39;, n=n, p=p, observed=caras) # Realizando el muestreo para la inferencia with modelo_moneda: trace = pm.sample(niter, njobs=4) Auto-assigning NUTS sampler... Initializing NUTS using advi... Average ELBO = -4.656: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200000/200000 [00:19\u0026lt;00:00, 10433.82it/s] Finished [100%]: Average ELBO = -4.639 WARNING (theano.tensor.blas): We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library. 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:08\u0026lt;00:00, 226.40it/s] # Analizando los resultados pm.traceplot(trace, varnames=[\u0026#39;p\u0026#39;], lines={\u0026#39;p\u0026#39;:.8}) pass # Informaci√≥n resumen. #Vemos que hay un 95% de probabilidades de que el valor de sesgo este entre # .706 y .864 pm.summary(trace) p: Mean SD MC Error 95% HPD interval ------------------------------------------------------------------- 0.789 0.040 0.001 [0.706, 0.864] Posterior quantiles: 2.5 25 50 75 97.5 |--------------|==============|==============|--------------| 0.701 0.764 0.791 0.816 0.861 Como vemos el modelo nos indica que la moneda parece tener un claro sesgo hacia cara.\nEl problema de la hierba mojada# Supongamos que hay dos eventos los cuales pueden causar que la hierba est√© h√∫meda: que el rociador est√© activado o que est√© lloviendo. Tambi√©n supongamos que la lluvia tiene un efecto directo sobre el uso del rociador (usualmente cuando llueve el rociador se encuentra apagado). Entonces la situaci√≥n puede ser modelada con la siguiente red bayesiana.\n# Problema de la hierba mojada # https://es.wikipedia.org/wiki/Red_bayesiana#Ejemplo niter = 10000 # 10000 tune = 5000 # 5000 modelo = pm.Model() with modelo: tv = [1] lluvia = pm.Bernoulli(\u0026#39;lluvia\u0026#39;, 0.2, shape=1, testval=tv) rociador_p = pm.Deterministic(\u0026#39;rociador_p\u0026#39;, pm.math.switch(lluvia, 0.01, 0.40)) rociador = pm.Bernoulli(\u0026#39;rociador\u0026#39;, rociador_p, shape=1, testval=tv) hierba_mojada_p = pm.Deterministic(\u0026#39;hierba_mojada_p\u0026#39;, pm.math.switch(lluvia, pm.math.switch(rociador, 0.99, 0.80), pm.math.switch(rociador, 0.90, 0.0))) hierba_mojada = pm.Bernoulli(\u0026#39;hierba_mojada\u0026#39;, hierba_mojada_p, observed=np.array([1]), shape=1) trace = pm.sample(20000, step=[pm.BinaryGibbsMetropolis([lluvia, rociador])], tune=tune, random_seed=124) # pm.traceplot(trace) dictionary = { \u0026#39;lluvia\u0026#39;: [1 if ii[0] else 0 for ii in trace[\u0026#39;lluvia\u0026#39;].tolist() ], \u0026#39;rociador\u0026#39;: [1 if ii[0] else 0 for ii in trace[\u0026#39;rociador\u0026#39;].tolist() ], \u0026#39;rociador_p\u0026#39;: [ii[0] for ii in trace[\u0026#39;rociador_p\u0026#39;].tolist()], \u0026#39;hierba_mojada_p\u0026#39;: [ii[0] for ii in trace[\u0026#39;hierba_mojada_p\u0026#39;].tolist()], } df = pd.DataFrame(dictionary) p_lluvia = df[(df[\u0026#39;lluvia\u0026#39;] == 1)].shape[0] / df.shape[0] print(\u0026#34;\\nProbabilidad de que la hierba este mojada por la lluvia: {0}\u0026#34; .format(p_lluvia)) p_rociador = df[(df[\u0026#39;rociador\u0026#39;] == 1)].shape[0] / df.shape[0] print(\u0026#34;Probabilidad de que la hierba este mojada por el rociador: {0}\u0026#34; .format(p_rociador)) [-----------------100%-----------------] 20000 of 20000 complete in 8.8 sec Probabilidad de que la hierba este mojada por la lluvia: 0.38355 Probabilidad de que la hierba este mojada por el rociador: 0.62105 De acuerdo a los resultados de la red bayesiana, si vemos que la hierba esta mojada, la probabilidad de que este lloviendo es alrededor del 38%.\nClasificador Bayes ingenuo# Uno de los clasificadores m√°s utilizados en Machine Learning por su simplicidad y rapidez, es el Clasificador Bayes ingenuo. El cual es una t√©cnica de clasificaci√≥n supervisada basada en el teorema de Bayes que asume que existe una independencia entre los atributos. En t√©rminos simples, un Clasificador Bayes ingenuo asume que la presencia de una caracter√≠stica particular en una clase no est√° relacionada con la presencia de cualquier otra caracter√≠stica. Por ejemplo, una fruta puede considerarse como una manzana si es roja, redonda y de aproximadamente 9 cm de di√°metro. Incluso si estas caracter√≠sticas dependen unas de otras o de la existencia de otras caracter√≠sticas, todas estas propiedades contribuyen independientemente a la probabilidad de que esta fruta sea una manzana. Se lo llama ingenuo ya que asumir independencia absoluta entre todos los atributos, no es algo que se suela dar en la realidad. El modelo Bayes ingenuo es f√°cil de construir y particularmente √∫til para conjuntos de datos muy grandes. A pesar de su simplicidad y de su irealista postulado de independencia, este clasificador se ha mostrado muy efectivo y se suele utilizar como el est√°ndar para evaluar el rendimiento de otros modelos de Machine Learning.\nEl Clasificador Bayes ingenuo se utiliza en m√∫ltiples escenarios de la vida real, tales como:\nClasificaci√≥n de texto: Es uno de los algoritmos conocidos m√°s exitosos cuando se trata de la clasificaci√≥n de documentos de texto, es decir, si un documento de texto pertenece a una o m√°s categor√≠as (clases). Detecci√≥n de spam: Es un ejemplo de clasificaci√≥n de texto. Se ha convertido en un mecanismo popular para distinguir el correo electr√≥nico spam del correo electr√≥nico leg√≠timo. An√°lisis de sentimientos: Puede ser utilizado para analizar el tono de tweets, comentarios y revisiones, ya sean negativos, positivos o neutrales. Sistema de Recomendaciones: El algoritmo Bayes ingenuo en combinaci√≥n con el filtrado colaborativo se utiliza para construir sistemas de recomendaci√≥n h√≠bridos que ayudan a predecir si un usuario desea un recurso determinado o no. Veamos un ejemplo con la ayuda de Scikit-Learn:\n# Ejemplo Naive Bayes usuando iris dataset iris = datasets.load_iris() X = iris.data y = iris.target # Dividir los datos en entrenamiento y evaluaci√≥n X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=0) # inicializar el clasificador Naive Bayes bayes_ingenuo = GaussianNB() # predicci√≥n y_pred = bayes_ingenuo.fit(X_train, y_train).predict(X_test) # Matriz de confusi√≥n cnf_matrix = confusion_matrix(y_test, y_pred) print(\u0026#34;Cantidad de errores de clasificaci√≥n sobre un total de {0} casos: {1}\u0026#34; .format(y_test.shape[0],(y_test != y_pred).sum())) print(\u0026#34;Efectividad del algoritmo: {0: .2f}\u0026#34; .format(1 - (y_test != y_pred).sum()/y_test.shape[0])) # Graficando la matriz de confusi√≥n sns.heatmap(cnf_matrix.T, square=True, annot=True, fmt=\u0026#39;d\u0026#39;, cbar=False) plt.xlabel(\u0026#39;Clase verdadera\u0026#39;) plt.ylabel(\u0026#39;Clase predecida\u0026#39;) plt.title(\u0026#39;Matriz de Confusi√≥n\u0026#39;) plt.show() Cantidad de errores de clasificaci√≥n sobre un total de 105 casos: 7 Efectividad del algoritmo: 0.93 En este sencillo ejemplo, podemos ver como el Clasificador Bayes ingenuo ha clasificado correctamente la mayor√≠a de los casos del dataset iris, obteniendo un efectividad del 93 %.\nDebido a que los clasificadores bayesianos ingenuos hacen suposiciones tan estrictas acerca de los datos, generalmente no funcionar√°n tan bien con modelos m√°s complicados. Dicho esto, tienen varias ventajas:\nSon extremadamente r√°pidos tanto para entrenamiento como para predicci√≥n Proporcionan una predicci√≥n probabil√≠stica directa A menudo son muy f√°cilmente interpretables Tienen muy pocos par√°metros que necesiten optimizarse. Estas ventajas significan que un clasificador bayesiano ingenuo es a menudo una buena opci√≥n como un modelo de clasificaci√≥n inicial. Si obtenemos resultados satisfactorios, entonces tenemos un clasificador muy r√°pido, y muy f√°cil de interpretar. Si no funciona bien, entonces podemos comenzar a explorar modelos m√°s sofisticados.\nAqu√≠ concluye esta introducci√≥n a la inferencia bayesiana; como vemos es una teor√≠a sumamente fascinante con serias implicancias filos√≥ficas. La teor√≠a Bayesiana es mucho m√°s que un simple teorema de probabilidad, es una l√≥gica para razonar sobre el amplio espectro de la vida que se encuentra en las √°reas grises entre la verdad absoluta y la incertidumbre total. A menudo tenemos informaci√≥n sobre s√≥lo una peque√±a parte de lo que nos preguntamos. Sin embargo, todos queremos predecir algo basado en nuestras experiencias pasadas; y adaptamos nuestras creencias a medida que adquirimos nueva informaci√≥n. La inferencia bayesiana nos proporciona una forma de pensar racionalmente sobre el mundo que nos rodea.\nSaludos!\nEste post fue escrito utilizando Jupyter notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2017-05-21","id":11,"permalink":"/blog/2017/05/21/introduccion-a-la-inferencia-bayesiana-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\n\u0026ldquo;Las preguntas m√°s importantes de la vida son, en su mayor parte, nada m√°s que problemas de probabilidad.\u0026rdquo;\nPierre-Simon Lapace\nIntroducci√≥n: La incertidumbre y el problema de la inducci√≥n# ¬øC√≥mo se deben evaluar las hip√≥tesis?¬øCu√°l es el papel de la evidencia en este proceso?","tags":["python","estadistica","analisis de datos","probabilidad","distribuciones","inferencia","Monte-Carlo","MCMC","Bayes"],"title":"Introducci√≥n a la inferencia Bayesiana con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# La optimizaci√≥n es fundamental para cualquier problema relacionado con la toma de decisiones, ya sea en ingenier√≠a o en ciencias econ√≥micas. La tarea de tomar decisiones implica elegir entre varias alternativas. Esta opci√≥n va a estar gobernada por nuestro deseo de tomar la \u0026ldquo;mejor\u0026rdquo; decisi√≥n posible. Que tan buena va a ser cada una de las alternativas va a estar descripta por una funci√≥n objetivo o √≠ndice de rendimiento. La teor√≠a y los m√©todos de optimizaci√≥n nos van a ayudar a seleccionar la mejor alternativa de acuerdo a esta funci√≥n objetivo dada. El √°rea de optimizaci√≥n ha recibido gran atenci√≥n en los √∫ltimos a√±os, principalmente por el r√°pido desarrollo de las ciencias de computaci√≥n, incluido el desarrollo y la disponibilidad de herramientas de software sumamente amigables, procesadores paralelos de alta velocidad, y redes neuronales artificiales. El poder de los m√©todos de optimizaci√≥n reside en la posibilidad de determinar la soluci√≥n √≥ptima sin realmente tener que probar todos los casos posibles. Para logar esto, se utiliza un nivel modesto de Matem√°ticas y se realizan c√°lculos num√©ricos iterativos utilizando procedimientos l√≥gicos claramente definidos o algoritmos implementados en computadoras.\n¬øQu√© es un problema de optimizaci√≥n?# Un problema de optimizaci√≥n comienza con un conjunto de variables independientes o par√°metros, y a menudo incluye condiciones o restricciones que definen los valores aceptables de estas variables. Tales restricciones se denominan las limitaciones del problema. El otro componente esencial de un problema de optimizaci√≥n es una medida √∫nica de \u0026ldquo;bondad\u0026rdquo;, denominada funci√≥n objetivo, la cual va a depender tambi√©n de las variables independientes. La soluci√≥n al problema de optimizaci√≥n va a estar dada por un conjunto de valores permitidos para las variables independientes, de acuerdo con las limitaciones del problema, en los que la funci√≥n objetivo asume un valor √≥ptimo. En t√©rminos matem√°ticos, la optimizaci√≥n implica normalmente maximizar o minimizar la funci√≥n objetivo.\nRequisitos para la aplicaci√≥n de los m√©todos de optimizaci√≥n# Para aplicar los resultados matem√°ticos y t√©cnicas num√©ricas de la teor√≠a de optimizaci√≥n a problemas concretos, es necesario delinear claramente los l√≠mites del sistema a optimizar, definir los par√°metros cuantitativos que se utilizaran como criterio en base al cual ser√°n clasificadas las alternativas para determinar la \u0026ldquo;mejor\u0026rdquo; opci√≥n, seleccionar las variables que se utilizar√°n para caracterizar o identificar alternativas; y definir el modelo que exprese la forma en que las variables estar√°n relacionadas. Una buena formulaci√≥n del problema es la clave para el √©xito de un problema de optimizaci√≥n y es en alto grado un arte. Se aprende a trav√©s de la pr√°ctica y el estudio de aplicaciones exitosas y se basa en el conocimiento de las fortalezas, debilidades y particularidades de las t√©cnicas proporcionadas por los distintos m√©todos de optimizaci√≥n.\nClasificaci√≥n de los problemas de optimizaci√≥n# Un paso importante en cualquier problema de optimizaci√≥n es clasificar nuestro modelo, ya que los algoritmos para resolver problemas de optimizaci√≥n generalmente est√°n dise√±ados para atacar un tipo de problema en particular. Algunas de las principales clasificaciones que vamos a poder encontrar son las siguientes:\nOptimizaci√≥n continua versus optimizaci√≥n discreta: Algunos modelos s√≥lo tienen sentido si las variables toman valores de un conjunto discreto, a menudo un subconjunto de enteros, mientras que otros modelos contienen variables que pueden asumir cualquier valor real o continuo. Los modelos con variables discretas son problemas de optimizaci√≥n discretos; mientras que los modelos con variables continuas son problemas de optimizaci√≥n continua. Los problemas de optimizaci√≥n continua tienden a ser m√°s f√°ciles de resolver que los problemas de optimizaci√≥n discreta; sin embargo, las mejoras en los algoritmos junto con los avances en la tecnolog√≠a inform√°tica han aumentado dram√°ticamente el tama√±o y la complejidad de los problemas de optimizaci√≥n discreta que se pueden resolver eficientemente.\nOptimizaci√≥n sin restricciones versus optimizaci√≥n con restricciones: Otra distinci√≥n importante que podemos encontrar es entre los problemas en los que no hay restricciones sobre las variables y los problemas en los que hay restricciones sobre las variables. Los problemas de optimizaci√≥n sin restricciones surgen directamente en muchas aplicaciones pr√°cticas; tambi√©n surgen en la reformulaci√≥n de problemas de optimizaci√≥n restringida en los que las restricciones son reemplazadas por un t√©rmino de penalizaci√≥n en la funci√≥n objetivo. Los problemas de optimizaci√≥n restringida o con restrincciones surgen de las aplicaciones en las que hay restricciones expl√≠citas sobre las variables. Las restricciones sobre las variables pueden variar ampliamente de simples l√≠mites a sistemas de igualdades y desigualdades que modelan relaciones complejas entre las variables. Los problemas de optimizaci√≥n con restricciones se pueden clasificar asimismo seg√∫n la naturaleza de las restricciones que poseen (por ejemplo, lineales, no lineales, convexos) y la suavidad de las funciones (por ejemplo, diferenciables o no diferenciables).\nNinguna, una o varias funciones objetivo: La mayor√≠a de los problemas de optimizaci√≥n tienen una √∫nica funci√≥n objetivo, sin embargo, hay casos interesantes en los cuales los problemas de optimizaci√≥n no tienen una funci√≥n objetivo o tienen m√∫ltiples de ellas. Los problemas de factibilidad, son problemas en los que el objetivo es encontrar valores para las variables que satisfacen las limitaciones de un modelo sin un objetivo particular de optimizaci√≥n. Los problemas de complementariedad surgen a menudo en ingenier√≠a y econom√≠a; el objetivo es encontrar una soluci√≥n que satisfaga las condiciones de complementariedad. Los problemas de optimizaci√≥n multiobjetivo surgen en muchos campos, como la ingenier√≠a, la econom√≠a y la log√≠stica, cuando es necesario tomar decisiones √≥ptimas en presencia de compromisos entre dos o m√°s objetivos conflictivos. En la pr√°ctica, los problemas con objetivos m√∫ltiples a menudo se reformulan como problemas con objetivos √∫nicos, ya sea formando una combinaci√≥n ponderada de los diferentes objetivos o sustituyendo algunos de los objetivos por restricciones.\nOptimizaci√≥n determinista versus optimizaci√≥n estoc√°stica: En la optimizaci√≥n determinista, se supone que los datos para el problema dado se conocen con exactitud. Sin embargo, para muchos problemas reales, los datos no pueden ser conocidos con precisi√≥n por una variedad de razones. La primera raz√≥n se debe a un simple error de medici√≥n. La segunda raz√≥n m√°s fundamental es que algunos datos representan informaci√≥n sobre el futuro (por ejemplo, la demanda o el precio del producto para un per√≠odo de tiempo futuro) y simplemente no se puede saber con certeza. En la optimizaci√≥n bajo incertidumbre, o optimizaci√≥n estoc√°stica, la incertidumbre se incorpora en el modelo. El objetivo es encontrar una soluci√≥n que sea factible para todos los datos y √≥ptima en alg√∫n sentido. Los modelos de programaci√≥n estoc√°stica aprovechan el hecho de que las distribuciones de probabilidad que gobiernan los datos son conocidas o pueden ser estimadas; el objetivo es encontrar alguna soluci√≥n que sea factible para todas (o casi todas) las posibles instancias de los datos y optimice el rendimiento esperado del modelo.\nProgramaci√≥n lineal# Matem√°ticamente, un problema de optimizaci√≥n con restricciones asume la siguiente forma:\n$$ \\begin{array}{ll} \\min \\hspace{1cm} f_0(x)\\\\ \\mbox{sujeto a } \\ f_i (x) \\leq b_i, \\hspace{1cm} i=1, \\dots, m. \\end{array} $$ En donde el vector \\(x = (x_1, \\dots, x_n)\\) es la variable de optimizaci√≥n del problema, la funci√≥n \\(f_0: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) es la funci√≥n objetivo, las funciones \\(f_i: \\mathbb{R}^n \\rightarrow \\mathbb{R}, i=1, \\dots, m\\); son las funciones de restricciones de desigualdad; y las constantes \\(b_1, \\dots, m\\) son los l√≠mites de las restricciones. Dentro de este marco, un caso importante es el de la programaci√≥n lineal, en el cual la funci√≥n objetivo y las restricciones son lineales. El objetivo de la programaci√≥n lineal es determinar los valores de las variables de decisi√≥n que maximizan o minimizan una funci√≥n objetivo lineal, y en donde las variables de decisi√≥n est√°n sujetas a restricciones lineales. En general, el objetivo es encontrar un punto que minimice la funci√≥n objetivo al mismo tiempo que satisface las restricciones.\nPara resolver un problema de programaci√≥n lineal, debemos seguir los siguientes pasos:\nElegir las inc√≥gnitas o variables de decisi√≥n.\nEscribir la funci√≥n objetivo en funci√≥n de los datos del problema.\nEscribir las restricciones en forma de sistema de inecuaciones.\nAveriguar el conjunto de soluciones factibles representando gr√°ficamente las restricciones.\nCalcular las coordenadas de los v√©rtices del recinto de soluciones factibles (si son pocos).\nCalcular el valor de la funci√≥n objetivo en cada uno de los v√©rtices para ver en cu√°l de ellos presenta el valor m√°ximo o m√≠nimo seg√∫n nos pida el problema (hay que tener en cuenta aqu√≠ la posible no existencia de soluci√≥n).\nUno de los algoritmos m√°s eficientes para resolver problemas de programaci√≥n lineal, es el m√©todo simplex.\nOptimizaci√≥n convexa# Otro caso importante de optimizaci√≥n que debemos destacar es el de la optimizaci√≥n convexa, en el cual la funci√≥n objetivo y las restricciones son convexas. En realidad, la programaci√≥n lineal que vimos anteriormente, no es m√°s que un caso especial de optimizaci√≥n convexa.\n¬øqu√© es una funci√≥n convexa?# Podemos decir que un un conjunto es convexo si se puede ir de cualquier punto a cualquier otro en l√≠nea recta, sin salir del mismo. El concepto de convexidad es el opuesto de concavidad. LLevado a las funciones, podemos decir que una funci√≥n es convexa en un intervalo (a,c), si para todo punto b del intervalo la recta tangente en ese punto queda por debajo de la funci√≥n.\nLos conjuntos y funciones convexas tienen algunas propiedades que los hacen especiales para problemas de optimizaci√≥n, como ser:\nUna funci√≥n convexa no tiene m√≠nimos locales que no sean globales.\nUn conjunto convexo tiene un interior relativo no vac√≠o.\nUn conjunto convexo est√° conectado y tiene direcciones factibles en cualquier punto.\nUna funci√≥n no convexa puede ser convexificada manteniendo al mismo tiempo lo √≥ptima de sus m√≠nimos globales.\nUna funci√≥n convexa es continua dentro del interior de su dominio, y tiene buenas propiedades de diferenciaci√≥n.\nentre otras\nLibrer√≠as de Python para problemas de optimizaci√≥n# Como es de esperar, en el ecosistema cient√≠fico de Python podemos encontrar varias librer√≠as que nos van a ayudar a enfrentar los problemas de optimizaci√≥n, entre las que podemos destacar:\nscipy.optimize: Este es el m√≥dulo de optimizaci√≥n de SciPy, en el cual vamos a poder encontrar varias rutinas num√©ricas para resolver problemas no lineales de optimizaci√≥n.\nCVXopt: Esta es una librer√≠a con una interface amigable para resolver problemas de optimizaci√≥n convexa.\nPuLP: Esta librer√≠a nos proporciona un lenguaje para modelar y resolver problemas de optimizaci√≥n utilizando programaci√≥n lineal.\nPyomo: Esta librer√≠a tambi√©n nos va a proporcionar un lenguaje para modelar problemas de optimizaci√≥n en Python. Tiene una notaci√≥n similar a la que utilizar√≠amos en la definici√≥n matem√°tica de los problemas.\nDebemos destacar que tanto PuLP como Pyomo requieren la instalaci√≥n adicional de diferentes solvers para poder resolver los problemas de optimizaci√≥n. Algunos de los solvers que soportan son: GLPK, COIN CLP/CBC, CPLEX y GURUBI, entre otros.\nEjemplos con Python# Bien, ahora lleg√≥ el momento de ver como podemos resolver algunos problemas de optimizaci√≥n con la ayuda de las librer√≠as antes mencionadas.\nM√≠nimos cuadrados no lineales# En general, podemos resolver problemas de M√≠nimos cuadrados utilizando un poco de √°lgebra lineal, pero los M√≠nimos cuadrados tambi√©n pueden ser vistos como un problema de optimizaci√≥n; ya que como su nombre lo indica, debemos minimizar la suma de los cuadrados de las diferencias entre los puntos generados por la funci√≥n elegida y los correspondientes valores en los datos. scipy.optimize nos ofrece algunos m√©todos para resolver este tipo de problemas utilizando t√©cnicas de optimizaci√≥n, como por ejemplo el algoritmo Gauss-Newton. Estos m√©todos pueden sernos de mucha utilizar sobre todo si la funci√≥n tiene componentes no lineales. Veamos un ejemplo\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # importando modulos necesarios import matplotlib.pyplot as plt import numpy as np from scipy import optimize import cvxopt import pulp from pyomo.environ import * from pyomo.opt import SolverFactory import pyomo.environ np.random.seed(1984) #replicar random %matplotlib inline # Ejemplo m√≠nimos cuadrados no lineales utilizando scipy.optimize beta = (0.25, 0.75, 0.5) # funcion modelo def f(x, b0, b1, b2): return b0 + b1 * np.exp(-b2 * x**2) # datos aleatorios para simular las observaciones xdata = np.linspace(0, 5, 50) y = f(xdata, *beta) ydata = y + 0.05 * np.random.randn(len(xdata)) # funci√≥n residual def g(beta): return ydata - f(xdata, *beta) # comenzamos la optimizaci√≥n beta_start = (1, 1, 1) beta_opt, beta_cov = optimize.leastsq(g, beta_start) beta_opt array([ 0.24022514, 0.76030423, 0.48425909]) # graficamos fig, ax = plt.subplots(figsize=(10,8)) ax.scatter(xdata, ydata) ax.plot(xdata, y, \u0026#39;r\u0026#39;, lw=2) ax.plot(xdata, f(xdata, *beta_opt), \u0026#39;b\u0026#39;, lw=2) ax.set_xlim(0, 5) ax.set_xlabel(r\u0026#34;$x$\u0026#34;, fontsize=18) ax.set_ylabel(r\u0026#34;$f(x, \\beta)$\u0026#34;, fontsize=18) ax.set_title(\u0026#39;M√≠nimos cuadrados no lineales\u0026#39;) plt.show() Optimizaci√≥n con restricciones# Las restricciones a√±aden otro nivel de complejidad a los problemas de optimizaci√≥n. En su forma m√°s simple, simplemente consiste en poner algunos l√≠mites sobre los valores que las variables pueden alcanzar. Por ejemplo, podr√≠as intentar resolver el siguiente problema:\n$$\\min \\ f(x)= (x_1 -1)^2 - (x_2 -1)^2 \\hspace{1cm} \\mbox{sujeto a } \\ 2 \\leq x_1 \\leq 3 \\mbox{ y } 0 \\leq x_2 \\leq 2 $$ Este tipo de problema se puede resolver utilizando el m√©todo L-BFGS-B que nos ofrece scipy.optimize.\n# Ejemplo de optimizaci√≥n con restricciones scipy # defino una funcion de ayuda para subregion en el gr√°fico def func_X_Y_to_XY(f, X, Y): \u0026#34;\u0026#34;\u0026#34; Wrapper for f(X, Y) -\u0026gt; f([X, Y]) \u0026#34;\u0026#34;\u0026#34; s = np.shape(X) return f(np.vstack([X.ravel(), Y.ravel()])).reshape(*s) # funci√≥n a minimizar def f(X): x, y = X return (x - 1)**2 + (y - 1)**2 # minimizo la funci√≥n si restricciones x_opt = optimize.minimize(f, (1, 1), method=\u0026#39;BFGS\u0026#39;).x # el m√≠nimo para las restricciones bnd_x1, bnd_x2 = (2, 3), (0, 2) x_cons_opt = optimize.minimize(f, np.array([1, 1]), method=\u0026#39;L-BFGS-B\u0026#39;, bounds=[bnd_x1, bnd_x2]).x # graficando la soluci√≥n fig, ax = plt.subplots(figsize=(10, 8)) x_ = y_ = np.linspace(-1, 3, 100) X, Y = np.meshgrid(x_, y_) c = ax.contour(X, Y, func_X_Y_to_XY(f, X, Y), 50) ax.plot(x_opt[0], x_opt[1], \u0026#39;b*\u0026#39;, markersize=15) ax.plot(x_cons_opt[0], x_cons_opt[1], \u0026#39;r*\u0026#39;, markersize=15) bound_rect = plt.Rectangle((bnd_x1[0], bnd_x2[0]), bnd_x1[1] - bnd_x1[0], bnd_x2[1] - bnd_x2[0], facecolor=\u0026#34;grey\u0026#34;) ax.add_patch(bound_rect) ax.set_xlabel(r\u0026#34;$x_1$\u0026#34;, fontsize=18) ax.set_ylabel(r\u0026#34;$x_2$\u0026#34;, fontsize=18) plt.colorbar(c, ax=ax) ax.set_title(\u0026#39;Optimizaci√≥n con restricciones\u0026#39;) plt.show() Las restricciones que se definen por igualdades o desigualdades que incluyen m√°s de una variable son m√°s complicadas de tratar. Sin embargo, existen t√©cnicas generales que tambi√©n podemos utilizar para este tipo de problemas. Volviendo al ejemplo anterior, cambiemos la restricci√≥n por una m√°s compleja, como ser: \\(g(x) = x_1 - 1.75 - ( x_0 - 0.75 )^4 \\geq 0\\). Para resolver este problema scipy.optimize nos ofrece un m√©todo llamado programaci√≥n secuencial por m√≠nimos cuadrados, o SLSQP por su abreviatura en ingl√©s.\n# Ejemplo scipy SLSQP # funcion de restriccion def g(X): return X[1] - 1.75 - (X[0] - 0.75)**4 # definimos el diccionario con la restricci√≥n restriccion = dict(type=\u0026#39;ineq\u0026#39;, fun=g) # resolvemos x_opt = optimize.minimize(f, (0, 0), method=\u0026#39;BFGS\u0026#39;).x x_cons_opt = optimize.minimize(f, (0, 0), method=\u0026#39;SLSQP\u0026#39;, constraints=restriccion).x # graficamos ig, ax = plt.subplots(figsize=(10, 8)) x_ = y_ = np.linspace(-1, 3, 100) X, Y = np.meshgrid(x_, y_) c = ax.contour(X, Y, func_X_Y_to_XY(f, X, Y), 50) ax.plot(x_opt[0], x_opt[1], \u0026#39;b*\u0026#39;, markersize=15) ax.plot(x_, 1.75 + (x_-0.75)**4, \u0026#39;k-\u0026#39;, markersize=15) ax.fill_between(x_, 1.75 + (x_-0.75)**4, 3, color=\u0026#39;grey\u0026#39;) ax.plot(x_cons_opt[0], x_cons_opt[1], \u0026#39;r*\u0026#39;, markersize=15) ax.set_ylim(-1, 3) ax.set_xlabel(r\u0026#34;$x_0$\u0026#34;, fontsize=18) ax.set_ylabel(r\u0026#34;$x_1$\u0026#34;, fontsize=18) plt.colorbar(c, ax=ax) plt.show() Programaci√≥n lineal con CVXopt# Ahora veamos un ejemplo de programaci√≥n lineal utilizando los m√©todos de optimizaci√≥n convexa que nos ofrece la librer√≠a CVXopt. Recordemos que la programaci√≥n lineal es un caso especial de la optimizaci√≥n convexa y el principal algoritmo que se utiliza es el m√©todo simplex. Para este peque√±o ejemplo vamos a maximizar la siguiente funci√≥n objetivo: $$f(x_1,x_2) = 50x_1 + 40x_2$$ con las siguientes restricciones:\n$$x_{1} + 1.5x_{2} \\leq 750 \\\\ 2x_1 + x_2 \\leq 1000 \\\\ x_1 \\geq 0 \\\\ x_2 \\geq 0 $$ # Ejemplo programaci√≥n lineal con CVXopt # Resolviendo el problema con cvxopt A = cvxopt.matrix([[-1., -2., 1., 0.], # columna de x1 [-1.5, -1., 0., 1.]]) # columna de x2 b = cvxopt.matrix([750., 1000., 0., 0.]) # resultados c = cvxopt.matrix([50., 40.]) # funcion objetivo # resolviendo el problema sol=cvxopt.solvers.lp(c,A,b) pcost dcost gap pres dres k/t 0: -2.5472e+04 -3.6797e+04 5e+03 0e+00 3e-01 1e+00 1: -2.8720e+04 -2.9111e+04 1e+02 2e-16 9e-03 2e+01 2: -2.8750e+04 -2.8754e+04 1e+00 8e-17 9e-05 2e-01 3: -2.8750e+04 -2.8750e+04 1e-02 4e-16 9e-07 2e-03 4: -2.8750e+04 -2.8750e+04 1e-04 9e-17 9e-09 2e-05 Optimal solution found. # imprimiendo la solucion. print(\u0026#39;{0:.2f}, {1:.2f}\u0026#39;.format(sol[\u0026#39;x\u0026#39;][0]*-1, sol[\u0026#39;x\u0026#39;][1]*-1)) 375.00, 250.00 # Resolviendo la optimizacion graficamente. x_vals = np.linspace(0, 800, 10) # 10 valores entre 0 y 800 y1 = ((750 - x_vals)/1.5) # x1 + 1.5x2 = 750 y2 = (1000 - 2*x_vals) # 2x1 + x2 = 1000 plt.figure(figsize=(10,8)) plt.plot(x_vals, y1, label=r\u0026#39;$x_1 + 1.5x_2 \\leq 750$\u0026#39;) plt.plot(x_vals, y2, label=r\u0026#39;$2x_1 + x_2 \\leq 1000$\u0026#39;) # plt.plot(375, 250, \u0026#39;b*\u0026#39;, markersize=15) # Regi√≥n factible y3 = np.minimum(y1, y2) plt.fill_between(x_vals, 0, y3, alpha=0.15, color=\u0026#39;b\u0026#39;) plt.axis(ymin = 0) plt.title(\u0026#39;Optimizaci√≥n lineal\u0026#39;) plt.legend() plt.show() Como podemos ver, tanto la soluci√≥n utilizando CVXopt, como la soluci√≥n gr√°fica; nos devuelven el mismo resultado \\(x_1=375\\) y \\(x_2=250\\).\nEl problema de transporte# El problema de transporte es un problema cl√°sico de programaci√≥n lineal en el cual se debe minimizar el costo del abastecimiento a una serie de puntos de demanda a partir de un grupo de puntos de oferta, teniendo en cuenta los distintos precios de env√≠o de cada punto de oferta a cada punto de demanda. Por ejemplo, supongamos que tenemos que enviar cajas de cervezas de 2 cervecer√≠as a 5 bares de acuerdo al siguiente gr√°fico:\nAsimismo, supongamos que nuestro gerente financiero nos informa que el costo de transporte por caja de cada ruta se conforma de acuerdo a la siguiente tabla:\nBar 1 Bar 2 Bar 3 Bar 4 Bar 5 Cervecer√≠a A 2 4 5 2 1 Cervecer√≠a B 3 1 3 2 3 Y por √∫ltimo, las restricciones del problema, van a estar dadas por las capacidades de oferta y demanda de cada cervecer√≠a y cada bar, las cuales se detallan en el gr√°fico de m√°s arriba.\nVeamos como podemos modelar este ejemplo con la ayuda de PuLP y de Pyomo.\n# Ejemplo del problema de transporte de las cervezas utilizando PuLP # Creamos la variable prob que contiene los datos del problema prob = pulp.LpProblem(\u0026#34;Problema de distribuci√≥n de cerveza\u0026#34;, pulp.LpMinimize) # Creamos lista de cervecer√≠as o nodos de oferta cervecerias = [\u0026#34;Cervecer√≠a A\u0026#34;, \u0026#34;Cervercer√≠a B\u0026#34;] # diccionario con la capacidad de oferta de cada cerveceria oferta = {\u0026#34;Cervecer√≠a A\u0026#34;: 1000, \u0026#34;Cervercer√≠a B\u0026#34;: 4000} # Creamos la lista de los bares o nodos de demanda bares = [\u0026#34;Bar 1\u0026#34;, \u0026#34;Bar 2\u0026#34;, \u0026#34;Bar 3\u0026#34;, \u0026#34;Bar 4\u0026#34;, \u0026#34;Bar 5\u0026#34;] # diccionario con la capacidad de demanda de cada bar demanda = {\u0026#34;Bar 1\u0026#34;:500, \u0026#34;Bar 2\u0026#34;:900, \u0026#34;Bar 3\u0026#34;:1800, \u0026#34;Bar 4\u0026#34;:200, \u0026#34;Bar 5\u0026#34;:700,} # Lista con los costos de transporte de cada nodo costos = [ #Bares #1 2 3 4 5 [2,4,5,2,1],#A Cervecer√≠as [3,1,3,2,3] #B ] # Convertimos los costos en un diccionario de PuLP costos = pulp.makeDict([cervecerias, bares], costos,0) # Creamos una lista de tuplas que contiene todas las posibles rutas de tranporte. rutas = [(c,b) for c in cervecerias for b in bares] # creamos diccionario x que contendr√° la candidad enviada en las rutas x = pulp.LpVariable.dicts(\u0026#34;ruta\u0026#34;, (cervecerias, bares), lowBound = 0, cat = pulp.LpInteger) # Agregamos la funci√≥n objetivo al problema prob += sum([x[c][b]*costos[c][b] for (c,b) in rutas]), \\ \u0026#34;Suma_de_costos_de_transporte\u0026#34; # Agregamos la restricci√≥n de m√°xima oferta de cada cervecer√≠a al problema. for c in cervecerias: prob += sum([x[c][b] for b in bares]) \u0026lt;= oferta[c], \\ \u0026#34;Suma_de_Productos_que_salen_de_cervecerias_%s\u0026#34;%c # Agregamos la restricci√≥n de demanda m√≠nima de cada bar for b in bares: prob += sum([x[c][b] for c in cervecerias]) \u0026gt;= demanda[b], \\ \u0026#34;Sum_of_Products_into_Bar%s\u0026#34;%b # Los datos del problema son exportado a archivo .lp prob.writeLP(\u0026#34;problemaDeTransporte.lp\u0026#34;) # Resolviendo el problema. prob.solve() # Imprimimos el estado del problema. print(\u0026#34;Status: {}\u0026#34;.format(pulp.LpStatus[prob.status])) # Imprimimos cada variable con su soluci√≥n √≥ptima. for v in prob.variables(): print(\u0026#34;{0:} = {1:}\u0026#34;.format(v.name, v.varValue)) # Imprimimos el valor √≥ptimo de la funci√≥n objetivo print(\u0026#34;Costo total de transporte = {}\u0026#34;.format(prob.objective.value())) Status: Optimal ruta_Cervecer√≠a_A_Bar_1 = 300.0 ruta_Cervecer√≠a_A_Bar_2 = 0.0 ruta_Cervecer√≠a_A_Bar_3 = 0.0 ruta_Cervecer√≠a_A_Bar_4 = 0.0 ruta_Cervecer√≠a_A_Bar_5 = 700.0 ruta_Cervercer√≠a_B_Bar_1 = 200.0 ruta_Cervercer√≠a_B_Bar_2 = 900.0 ruta_Cervercer√≠a_B_Bar_3 = 1800.0 ruta_Cervercer√≠a_B_Bar_4 = 200.0 ruta_Cervercer√≠a_B_Bar_5 = 0.0 Costo total de transporte = 8600.0 Como vemos, la soluci√≥n √≥ptima que encontramos con la ayuda de PuLP, nos dice que deber√≠amos enviar desde la Cervecer√≠a A, 300 cajas al Bar 1 y 700 cajas al Bar 5; y que desde la Cervecer√≠a B deber√≠amos enviar 200 cajas al Bar 1, 900 cajas al Bar 2, 1800 cajas al Bar 3 y 200 cajas al Bar 4. De esta forma podemos minimizar el costo de transporte a un total de 8600.\nVeamos ahora si podemos formular este mismo problema con Pyomo, as√≠ podemos darnos una idea de las diferencias entre las herramientas.\n# Ejemplo del problema de transporte de las cervezas utilizando Pyomo # Creamos el modelo modelo = ConcreteModel() # Creamos los nodos de oferta y demanda modelo.i = Set(initialize=[\u0026#39;Cervecer√≠a A\u0026#39;,\u0026#39;Cervecer√≠a B\u0026#39;], doc=\u0026#39;Cervecer√≠as\u0026#39;) modelo.j = Set(initialize=[\u0026#39;Bar 1\u0026#39;, \u0026#39;Bar 2\u0026#39;, \u0026#39;Bar 3\u0026#39;, \u0026#39;Bar 4\u0026#39;, \u0026#39;Bar 5\u0026#39;], doc=\u0026#39;Bares\u0026#39;) # Definimos las capacidades de oferta y demanda modelo.a = Param(modelo.i, initialize={\u0026#39;Cervecer√≠a A\u0026#39;:1000,\u0026#39;Cervecer√≠a B\u0026#39;:4000}, doc=\u0026#39;Capacidad de oferta de las cervecer√≠as\u0026#39;) modelo.b = Param(modelo.j, initialize={\u0026#39;Bar 1\u0026#39;:500,\u0026#39;Bar 2\u0026#39;:900,\u0026#39;Bar 3\u0026#39;:1800, \u0026#39;Bar 4\u0026#39;:200, \u0026#39;Bar 5\u0026#39;:700 }, doc=\u0026#39;Demanda de cada bar\u0026#39;) # Costo de transporte costos = { (\u0026#39;Cervecer√≠a A\u0026#39;, \u0026#39;Bar 1\u0026#39;): 2, (\u0026#39;Cervecer√≠a A\u0026#39;, \u0026#39;Bar 2\u0026#39;): 4, (\u0026#39;Cervecer√≠a A\u0026#39;, \u0026#39;Bar 3\u0026#39;): 5, (\u0026#39;Cervecer√≠a A\u0026#39;, \u0026#39;Bar 4\u0026#39;): 2, (\u0026#39;Cervecer√≠a A\u0026#39;, \u0026#39;Bar 5\u0026#39;): 1, (\u0026#39;Cervecer√≠a B\u0026#39;, \u0026#39;Bar 1\u0026#39;): 3, (\u0026#39;Cervecer√≠a B\u0026#39;, \u0026#39;Bar 2\u0026#39;): 1, (\u0026#39;Cervecer√≠a B\u0026#39;, \u0026#39;Bar 3\u0026#39;): 3, (\u0026#39;Cervecer√≠a B\u0026#39;, \u0026#39;Bar 4\u0026#39;): 2, (\u0026#39;Cervecer√≠a B\u0026#39;, \u0026#39;Bar 5\u0026#39;): 3 } modelo.d = Param(modelo.i, modelo.j, initialize=costos, doc=\u0026#39;Costo de transporte\u0026#39;) # definimos el costo de tranporte def f_costo(modelo, i, j): return modelo.d[i,j] modelo.c = Param(modelo.i, modelo.j, initialize=f_costo, doc=\u0026#39;Costo de transporte\u0026#39;) # definimos variable x con las cantidades de cajas enviadas modelo.x = Var(modelo.i, modelo.j, bounds=(0.0,None), doc=\u0026#39;Cantidad de cajas\u0026#39;) ## Definimos las restricciones ## # L√≠mite de oferta def f_oferta(modelo, i): return sum(modelo.x[i,j] for j in modelo.j) \u0026lt;= modelo.a[i] modelo.oferta = Constraint(modelo.i, rule=f_oferta, doc=\u0026#39;L√≠mites oferta de cada Cervecer√≠a\u0026#39;) # L√≠mite de demanda def f_demanda(modelo, j): return sum(modelo.x[i,j] for i in modelo.i) \u0026gt;= modelo.b[j] modelo.demanda = Constraint(modelo.j, rule=f_demanda, doc=\u0026#39;L√≠mites demanda de cada bar\u0026#39;) ## Definimos la funci√≥n objetivo y resolvemos el problema ## # Funci√≥n objetivo def f_objetivo(modelo): return sum(modelo.c[i,j]*modelo.x[i,j] for i in modelo.i for j in modelo.j) modelo.objetivo = Objective(rule=f_objetivo, sense=minimize, doc=\u0026#39;Funci√≥n Objetivo\u0026#39;) # resolvemos el problema e imprimimos resultados def pyomo_postprocess(options=None, instance=None, results=None): modelo.x.display() # utilizamos solver glpk opt = SolverFactory(\u0026#34;glpk\u0026#34;) resultados = opt.solve(modelo) # imprimimos resultados print(\u0026#34;\\nSoluci√≥n √≥ptima encontrada\\n\u0026#34; + \u0026#39;-\u0026#39;*80) pyomo_postprocess(None, None, resultados) Soluci√≥n √≥ptima encontrada -------------------------------------------------------------------------------- x : Cantidad de cajas Size=10, Index=x_index Key : Lower : Value : Upper : Fixed : Stale : Domain ('Cervecer√≠a A', 'Bar 1') : 0.0 : 300.0 : None : False : False : Reals ('Cervecer√≠a A', 'Bar 2') : 0.0 : 0.0 : None : False : False : Reals ('Cervecer√≠a A', 'Bar 3') : 0.0 : 0.0 : None : False : False : Reals ('Cervecer√≠a A', 'Bar 4') : 0.0 : 0.0 : None : False : False : Reals ('Cervecer√≠a A', 'Bar 5') : 0.0 : 700.0 : None : False : False : Reals ('Cervecer√≠a B', 'Bar 1') : 0.0 : 200.0 : None : False : False : Reals ('Cervecer√≠a B', 'Bar 2') : 0.0 : 900.0 : None : False : False : Reals ('Cervecer√≠a B', 'Bar 3') : 0.0 : 1800.0 : None : False : False : Reals ('Cervecer√≠a B', 'Bar 4') : 0.0 : 200.0 : None : False : False : Reals ('Cervecer√≠a B', 'Bar 5') : 0.0 : 0.0 : None : False : False : Reals Como podemos ver, arribamos a la mismo soluci√≥n que utilizando PuLP. Ambas herramientas siguen un lenguaje de modelado totalmente distinto, particularmente me gusta m√°s la sintaxis que ofrece PuLP sobre la de Pyomo.\nAqu√≠ concluye este art√≠culo, como vemos la optimizaci√≥n se puede aplicar a un amplio rango de problemas, por lo que es sumamente importe conocer sus principales m√©todos y como podemos aplicarlos con √©xito.\nSaludos!\nEste post fue escrito utilizando IPython notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2017-01-18","id":12,"permalink":"/blog/2017/01/18/problemas-de-optimizacion-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# La optimizaci√≥n es fundamental para cualquier problema relacionado con la toma de decisiones, ya sea en ingenier√≠a o en ciencias econ√≥micas. La tarea de tomar decisiones implica elegir entre varias alternativas. Esta opci√≥n va a estar gobernada por nuestro deseo de tomar la \u0026ldquo;mejor\u0026rdquo; decisi√≥n posible.","tags":["python","analisis de datos","programacion","machine learning","redes neuronales","matematica","optimizacion"],"title":"Problemas de Optimizaci√≥n con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# En el cierre de mi art√≠culo anterior, comentaba sobre la sorprendente influencia que han tenido los n√∫meros aleatorios, que junto con el poder de c√°lculo que nos proporcionan las computadoras modernas; nos han ayudado a resolver muchos de los problemas num√©ricos m√°s complejos en ciencia, ingenier√≠a, finanzas y estad√≠sticas. En esencia, detr√°s de cada una de esas soluciones encontramos una familia de m√©todos que se conocen bajo el nombre de M√©todos de Monte-Carlo.\n¬øQu√© son los m√©todos de Monte-Carlo?# Los M√©todos de Monte-Carlo son t√©cnicas para analizar fen√≥menos por medio de algoritmos computacionales, que utilizan y dependen fundamentalmente de la generaci√≥n de n√∫meros aleatorios. El t√©rmino Monte-Carlo, hace referencia al casino de Montecarlo, una de las capitales de los juegos de azar; y se utiliz√≥ como denominaci√≥n para est√°s t√©cnicas por la aleatoriedad inherente que poseen. El estudio de los M√©todos de Monte-Carlo requiere un conocimiento detallado en una amplia gama de campos; por ejemplo, la probabilidad para describir los experimentos y procesos aleatorios, la estad√≠stica para analizar los datos, las ciencias de la computaci√≥n para implementar eficientemente los algoritmos y la programaci√≥n matem√°tica para formular y resolver problemas de optimizaci√≥n.\nComo los M√©todos de Monte-Carlo dependen en gran medida de la posibilidad de producir, con una computadora, un flujo infinito de variables aleatorias para todo tipo de distribuciones; no podemos hablar de los M√©todos de Monte-Carlo, sin antes explicar los n√∫meros aleatorios y como podemos generarlos con la ayuda de una computadora.\nN√∫meros aleatorios y Monte-Carlo# En el coraz√≥n de los M√©todos de Monte-Carlo encontramos un generador de n√∫meros aleatorios, es decir un procedimiento que produce un flujo infinito de variables aleatorias, que generalmente se encuentran en el intervalo (0, 1); los cuales son independientes y est√°n uniformemente distribuidos de acuerdo a una distribuci√≥n de probabilidad. La mayor√≠a de los lenguajes de programaci√≥n hoy en d√≠a contienen un generador de n√∫meros aleatorios por defecto al cual simplemente debemos ingresarle un valor inicial, generalmente llamado seed o semilla, y que luego en cada invocaci√≥n nos va a devolver un secuencia uniforme de variables aleatorias independientes en el intervalo (0, 1).\nN√∫meros pseudoaleatorios# El concepto de una secuencia infinita de variables aleatorias es una abstracci√≥n matem√°tica que puede ser imposible de implementar en una computadora. En la pr√°ctica, lo mejor que se puede hacer es producir una secuencia de n√∫meros pseudoaleatorios con propiedades estad√≠sticas que son indistinguibles de las de una verdadera secuencia de variables aleatorias. Aunque actualmente m√©todos de generaci√≥n f√≠sica basados en la radiaci√≥n de fondo o la mec√°nica cu√°ntica parecen ofrecer una fuente estable de n√∫meros verdaderamente aleatorios , la gran mayor√≠a de los generadores de n√∫meros aleatorios que se utilizan hoy en d√≠a est√°n basados en algoritmos simples que pueden ser f√°cilmente implementados en una computadora; por lo que en realidad son generadores de n√∫meros pseudoaleatorios.\nN√∫meros aleatorios en Python# En Python el m√≥dulo random nos proporciona un r√°pido generador de n√∫meros pseudoaleatorios basado en el algoritmo Mersenne Twister; el cual genera n√∫meros con una distribuci√≥n casi uniforme y un per√≠odo grande, haci√©ndolo adecuado para una amplia gama de aplicaciones. Veamos un peque√±o ejemplo.\n# Utilizando random para genera n√∫meros aleatorios. import random random.seed(1984) # semilla para replicar la aleatoriedad random.random() # primer llamado a random 0.36352835585530807 random.random() # segundo llamado a random 0.49420568181919666 for i in range(5): print(random.random()) # 5 n√∫meros aleatorios 0.33961008717180197 0.21648780903913534 0.8626522767441037 0.8493329421213219 0.38578540884489343 # volviendo a llamar a seed para replicar el mismo resultado aleatorio. random.seed(1984) for i in range(7): print(random.random()) # Mismos resultados que arriba. 0.36352835585530807 0.49420568181919666 0.33961008717180197 0.21648780903913534 0.8626522767441037 0.8493329421213219 0.38578540884489343 En este ejemplo podemos ver como la funci√≥n random genera n√∫meros aleatorios entre 0 y 1, tambi√©n podemos ver como con el uso de seed podemos replicar el comportamiento aleatorio.\nElegir un buen generador de n√∫meros aleatorios# Existe una gran variedad de generadores de n√∫meros aleatorios que podemos elegir; pero elegir un buen generador de n√∫meros aleatorios es como elegir un coche nuevo: para algunas personas o aplicaciones la velocidad es primordial, mientras que para otros la robustez y la fiabilidad son m√°s importantes. Para la simulaci√≥n de Monte-Carlo las propiedades distributivas de los generadores aleatorios es primordial, mientras que en la criptograf√≠a la imprevisibilidad es crucial. Por tal motivo, el generador que vayamos a elegir depender√° de la aplicaci√≥n que le vayamos a dar.\nMonte-Carlo en acci√≥n# Los M√©todos de Monte-Carlo se basan en la analog√≠a entre probabilidad y volumen. Las matem√°ticas de las medidas formalizan la noci√≥n intuitiva de probabilidad, asociando un evento con un conjunto de resultados y definiendo que la probabilidad del evento ser√° el volumen o medida relativa del universo de posibles resultados. Monte-Carlo usa esta identidad a la inversa, calculando el volumen de un conjunto interpretando el volumen como una probabilidad. En el caso m√°s simple, esto significa muestrear aleatoriamente un universo de resultados posibles y tomar la fracci√≥n de muestras aleatorias que caen en un conjunto dado como una estimaci√≥n del volumen del conjunto. La ley de grandes n√∫meros asegura que esta estimaci√≥n converja al valor correcto a medida que aumenta el n√∫mero de muestras. El teorema del l√≠mite central proporciona informaci√≥n sobre la magnitud del probable error en la estimaci√≥n despu√©s de un n√∫mero finito de muestras. En esencia podemos decir que el M√©todo de Monte-Carlo consiste en calcular o aproximar ciertas expresiones a trav√©s de adivinarlas con la ayuda de dibujar una cantidad normalmente grande de n√∫meros aleatorios. Veamos como funciona con un ejemplo, calculemos el √°rea de un c√≠rculo de radio 1; lo que es lo mismo a decir que aproximemos el valor de \\(\\pi\\).\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # importando modulos necesarios import matplotlib.pyplot as plt import numpy as np # importando numpy import pandas as pd # importando pandas from scipy import stats np.random.seed(1984) # para poder replicar el random %matplotlib inline # Ejemplo: Aproximando el valor de pi - √°rea de un c√≠rculo de # radio = 1. def mc_pi_aprox(N=10000): plt.figure(figsize=(8,8)) # tama√±o de la figura x, y = np.random.uniform(-1, 1, size=(2, N)) interior = (x**2 + y**2) \u0026lt;= 1 pi = interior.sum() * 4 / N error = abs((pi - np.pi) / pi) * 100 exterior = np.invert(interior) plt.plot(x[interior], y[interior], \u0026#39;b.\u0026#39;) plt.plot(x[exterior], y[exterior], \u0026#39;r.\u0026#39;) plt.plot(0, 0, label=\u0026#39;$\\hat \\pi$ = {:4.4f}\\nerror = {:4.4f}%\u0026#39; .format(pi,error), alpha=0) plt.axis(\u0026#39;square\u0026#39;) plt.legend(frameon=True, framealpha=0.9, fontsize=16) mc_pi_aprox() # con 1000000 experimentos mc_pi_aprox(N=100000) Como vemos en este ejemplo, para calcular el √°rea del c√≠rculo realizamos un gran n√∫mero de experimentos aleatorios, en el primer ejemplo utilizamos 10,000 experimentos; y luego calculamos el √°rea obteniendo una media aritm√©tica de los valores que caen dentro de la superficie del c√≠rculo. Debemos hacer notar que incluso utilizando un gran n√∫mero de experimentos a√∫n as√≠ en el primer ejemplo no logramos obtener los primeros dos decimales correctos; reci√©n en el segundo ejemplo, cuando utilizamos 100,000 experimentos logramos obtener los primeros dos d√≠gitos correctos; esto demuestra que el M√©todo de Monte-Carlo en su versi√≥n m√°s cruda tarda bastante en converger.\nT√©cnicas de reducci√≥n de varianza# Existen varias t√©cnicas generales para la reducci√≥n de la varianza, estos m√©todos mejoran la precisi√≥n y la tasa de convergencia de la integraci√≥n por medio del M√©todo de Monte-Carlo sin aumentar el n√∫mero de experimentos. Algunas de estas t√©cnicas son:\nMuestreo de importancia: La idea principal detr√°s del Muestreo de importancia es simplemente encontrar una distribuci√≥n para la variable aleatoria subyacente que asigne una alta probabilidad a aquellos valores que son importantes para calcular la cantidad que nos interesa determinar. Muestreo estratificado: El principal principio subyacente al Muestreo estratificado es natural: tomar una muestra de una peque√±a subpoblaci√≥n que refleje las propiedades del total de la poblaci√≥n tanto como sea posible. Variantes de control: El m√©todo de las Variantes de control explota la informaci√≥n sobre los errores en las estimaciones de las cantidades conocidas para reducir el error de una estimaci√≥n de una cantidad desconocida. Variaciones antit√©ticas: El m√©todo de las Variaciones antit√©ticas es el m√©todo de reducci√≥n de la varianza, m√°s f√°cil. Se basa en la idea de combinar una selecci√≥n aleatoria de puntos con una opci√≥n sistem√°tica. Su principal principio es la reducci√≥n de la varianza, mediante la introducci√≥n de la simetr√≠a. Excede al alcance de este art√≠culo el profundizar en cada una de estas t√©cnicas, pueden encontrar un an√°lisis m√°s detallado de las mismas en el siguiente enlace (en ingl√©s).\nM√©todos de Monte-Carlo via cadenas de Markov# El desarrollo de los m√©todos de Monte-Carlo via cadenas de Markov, o MCMC por sus siglas en ingl√©s, es sin duda uno de los mayores avances en el enfoque computacional de la estad√≠stica. Muchos de los problemas que son intratables utilizando un enfoque anal√≠tico a menudo pueden ser resueltos utilizando alguna forma de MCMC, incluso aunque se trate de problemas en varias dimensiones. Las t√©cnicas MCMC se aplican para resolver problemas de integraci√≥n y optimizaci√≥n en grandes espacios dimensionales. Estos dos tipos de problemas desempe√±an un papel fundamental en machine learning, f√≠sica, estad√≠stica, econometr√≠a y el an√°lisis de decisiones.\n¬øQu√© es una cadena de Markov?# Una cadena de Markov es un objeto matem√°tico que consiste en una secuencia de estados y un conjunto de probabilidades que describen las transiciones entre esos estados. La caracter√≠stica principal que tiene esta cadena es que la probabilidad de moverse a otros estados depende solamente del estado actual. Dada una cadena, se puede realizar una caminata aleatoria eligiendo un punto de partida y movi√©ndose a otros estados siguiendo las probabilidades de transici√≥n. Si de alguna manera encontramos una cadena de Markov con transiciones proporcionales a la distribuci√≥n que queremos probar, el muestreo se convierte simplemente en una cuesti√≥n de moverse entre los estados de esta cadena.\nCaminata aleatoria en un grafo# Una cadena de Markov puede ser ilustrada con el siguiente grafo, el cual representa una cadena de Markov de cuatro estados.\nLos estados de la cadena se representan como los nodos del grafo, una arista de direcci√≥n se extiende de un nodo \\(x\\) hacia otro nodo \\(y\\) si la transici√≥n de \\(x\\) hacia \\(y\\) es posible en una iteraci√≥n. Cada una de estas aristas de direcci√≥n tienen una probabilidad asociada \\(P_{xy}\\); esta es la probabilidad de que el nodo sea elegido en la siguiente iteraci√≥n cuando la cadena se encuentra en el estado \\(x\\). La cadena comienza en alg√∫n estado, digamos \\(X_0\\), que puede ser elegido en forma aleatoria de acuerdo con una distribuci√≥n inicial o simplemente asignado en forma arbitraria. Desde all√≠, la cadena se mueve de un estado a otro en cada iteraci√≥n seg√∫n las probabilidades de transici√≥n del nodo vecino.\nRepresentaci√≥n matricial# Como alternativa a la representaci√≥n en forma de grafo, la cadena antes descripta tambi√©n puede ser representada por una matriz \\(P = (p_{xy})\\) de las probabilidades \\(p_{xy}\\) de transici√≥n de un estado $x$ a un estado \\(y\\) en una iteraci√≥n de la cadena. Esta matriz es llamada matriz de transici√≥n y tiene la caracter√≠stica que todas sus filas deben sumar 1. Por ejemplo, la matriz de transici√≥n de nuestro ejemplo ser√≠a la siguiente:\n$$ P = \\begin{bmatrix} p_{11} \u0026 p_{12} \u0026 0 \u0026 p_{14} \\\\ p_{21} \u0026 0 \u0026 p_{23} \u0026 0 \\\\ p_{31} \u0026 0 \u0026 0 \u0026 p_{34} \\\\ 0 \u0026 0 \u0026 p_{43} \u0026 p_{44} \\end{bmatrix} $$ Esta formulaci√≥n de matriz es mucho m√°s que una descripci√≥n tabular de la cadena; tambi√©n es una herramienta de c√°lculo. Ya que si por ejemplo definimos a \\(p_t\\) como el vector de probabilidad de una variable aleatoria \\(X_t\\); entonces podemos calcular \\(p_{t + 1}\\) como una multiplicaci√≥n de matrices\n$$ p_{t + 1} = p_t \\cdot P, \\hspace{1cm} t= 1, 2, \\dots $$ La distribuci√≥n invariante# Una de las caracter√≠sticas generales de las cadenas de Markov es que pueden poseer una distribuci√≥n invariante, tomemos por ejemplo la siguiente representaci√≥n matricial de una cadena de Markov:\n$$ P = \\begin{bmatrix} 0.3 \u0026 0.2 \u0026 0.5 \\\\ 0.4 \u0026 0.3 \u0026 0.3 \\\\ 0.3 \u0026 0.4 \u0026 0.3 \\end{bmatrix} $$ Si comenzamos en el primer estado de la cadena, podemos obtener \\(p_1\\) del siguiente modo:\n$$ p_1 = \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.3 \u0026 0.2 \u0026 0.5 \\\\ 0.4 \u0026 0.3 \u0026 0.3 \\\\ 0.3 \u0026 0.4 \u0026 0.3 \\end{bmatrix} = \\begin{bmatrix} 0.3 \u0026 0.2 \u0026 0.5 \\end{bmatrix} $$ Ahora que ya obtuvimos \\(p_1\\), podemos continuar y obtener \\(p_2\\):\n$$ p_2 = p_1 P = \\begin{bmatrix} 0.3 \u0026 0.2 \u0026 0.5 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.3 \u0026 0.2 \u0026 0.5 \\\\ 0.4 \u0026 0.3 \u0026 0.3 \\\\ 0.3 \u0026 0.4 \u0026 0.3 \\end{bmatrix} = \\begin{bmatrix} 0.32 \u0026 0.22 \u0026 0.36 \\end{bmatrix} $$ Si continuamos con este proceso en forma recursiva, veremos que la distribuci√≥n tiende hac√≠a un l√≠mite, este l√≠mite es su distribuci√≥n invariante.\n$$ p_3 = p_2 P = \\begin{bmatrix} 0.332 \u0026 0.304 \u0026 0.364 \\end{bmatrix}, \\\\ p_4 = p_3 P = \\begin{bmatrix} 0.3304 \u0026 0.3032 \u0026 0.3664 \\end{bmatrix}, \\\\ p_5 = p_4 P = \\begin{bmatrix} 0.33032 \u0026 0.3036 \u0026 0.36608 \\end{bmatrix}, \\\\ \\dots \\hspace{1cm} \\dots \\\\ p_{10} = p_9 P = \\begin{bmatrix} 0.330357 \u0026 0.303571 \u0026 0.366072 \\end{bmatrix} $$ Veamos el ejemplo con la ayuda de Python para que quede m√°s claro.\n# Ejemplo distribuci√≥n invariante P = np.array( [[0.3, 0.2, 0.5], [0.4, 0.3, 0.3 ], [0.3, 0.4, 0.3]] ) P array([[ 0.3, 0.2, 0.5], [ 0.4, 0.3, 0.3], [ 0.3, 0.4, 0.3]]) p1 = np.array( [1, 0, 0] ) for i in range(1, 12): p_i = p1 @ P print(\u0026#39;p_{0:} = {1:}\u0026#39;.format(i, p_i)) p1 = p_i p_1 = [ 0.3 0.2 0.5] p_2 = [ 0.32 0.32 0.36] p_3 = [ 0.332 0.304 0.364] p_4 = [ 0.3304 0.3032 0.3664] p_5 = [ 0.33032 0.3036 0.36608] p_6 = [ 0.33036 0.303576 0.366064] p_7 = [ 0.3303576 0.3035704 0.366072 ] p_8 = [ 0.33035704 0.30357144 0.36607152] p_9 = [ 0.33035714 0.30357145 0.36607141] p_10 = [ 0.33035714 0.30357143 0.36607143] p_11 = [ 0.33035714 0.30357143 0.36607143] Como vemos, luego de 12 iteraciones la distribuci√≥n alcanza su l√≠mite y ya no cambian los resultados. Hemos alcanzado la distribuci√≥n invariante!\nEl algoritmo Metropolis-Hastings# Uno de los m√©todos MCMC m√°s populares es el algoritmo Metropolis-Hastings; de hecho la mayor√≠a de los algoritmos de MCMC pueden ser interpretados como casos especiales de este algoritmo. El algoritmo Metropolis-Hastings esta catalogado como uno de los 10 algoritmos m√°s importantes y m√°s utilizados en ciencia e ingenier√≠a en los √∫ltimos veinte a√±os.Se encuentra en el coraz√≥n de la mayor√≠a de los m√©todos de muestreo MCMC. El problema b√°sico que intenta resolver el algoritmo Metropolis-Hastings es proporcionar un m√©todo para generar muestras de alguna distribuci√≥n gen√©rica, \\(P(x)\\). La idea es que en muchos casos, podemos saber c√≥mo escribir la ecuaci√≥n para la distribuci√≥n de probabilidad \\(P(x)\\), pero no sabemos c√≥mo generar muestras aleatorias de la misma. Entonces la idea b√°sica detr√°s de este algoritmo es la de construir una cadena de Markov cuya distribuci√≥n invariante sea la distribuci√≥n de muestreo que deseamos, es decir \\(P(x)\\). En principio, esto puede parecer bastante complicado, pero la flexibilidad inherente en la elecci√≥n de las probabilidades de transici√≥n lo hacen m√°s simple de lo que parece.\n¬øC√≥mo funciona el algoritmo?# El algoritmo funciona del siguiente modo. Supongamos que el estado actual de la cadena de Markov es \\(x_n\\), y queremos generar \\(x_{n + 1}\\). De acuerdo con el algoritmo Metropolis-Hastings, la generaci√≥n de \\(x_{n + 1}\\) es un proceso en dos etapas. La primera etapa consiste en generar un candidato, que denominaremos \\(x^\\). El valor de \\(x^\\) se genera a partir de la distribuci√≥n propuesta, denotada \\(Q (x^* | x_n)\\), la cual depende del estado actual de la cadena de Markov , \\(x_n\\). Existen algunas limitaciones t√©cnicas menores sobre la distribuci√≥n propuesta que podemos utilizar, pero en su mayor parte puede ser cualquier cosa que deseemos. Una forma t√≠pica de hacerlo es utilizar una distribuci√≥n normal centrada en el estado actual \\(x_n\\). Es decir,\n$$ x^*|x_n \\sim Normal(x_n, \\sigma^2)$$ La segunda etapa es la de aceptaci√≥n-rechazo. Lo primero que debemos hacer en este paso es calcular la probabilidad de aceptaci√≥n \\(A(x_n \\rightarrow x^*)\\), la cual estar√° dada por:\n$$A(x_n \\rightarrow x^*) = \\min \\left(1, \\frac{P(x^*)}{P(x_n)} \\cdot \\frac{Q(x_n | x^*)}{Q(x^* | x_n)} \\right) $$ Muy bien. Ahora que tenemos el candidato \\(x^\\) y hemos calculado la probabilidad de aceptaci√≥n \\(A(x_n \\rightarrow x^)\\), es tiempo de decidir aceptar al candidato (en cuyo caso se establecemos \\(x_{n + 1} = x^*\\)); o rechazar al candidato (en cuyo caso estableceremos \\(x_{n + 1} = x_n\\)). Para tomar esta decisi√≥n, generamos un n√∫mero aleatorio (uniformemente distribuido) entre 0 y 1, que denominaremos \\(u\\). Entonces:\n$$x_{n + 1} = \\left\\{ \\begin{array}{ll} x^* \u0026 \\mbox{si } u \\leq A(x_n \\rightarrow x^*)\\\\ x_n \u0026 \\mbox{si } u \u003e A(x_n \\rightarrow x^*) \\end{array} \\right. $$ Y esto es en esencia como funciona el algoritmo Metropolis-Hastings!\nVeamos un peque√±o ejemplo en Python:\n# Ejemplo algoritmo metropolis def metropolis(func, steps=10000): \u0026#34;\u0026#34;\u0026#34;A very simple Metropolis implementation\u0026#34;\u0026#34;\u0026#34; muestras = np.zeros(steps) old_x = func.mean() old_prob = func.pdf(old_x) for i in range(steps): new_x = old_x + np.random.normal(0, 0.5) new_prob = func.pdf(new_x) aceptacion = new_prob / old_prob if aceptacion \u0026gt;= np.random.random(): muestras[i] = new_x old_x = new_x old_prob = new_prob else: muestras[i] = old_x return muestras # distribuci√≥n beta func = stats.beta(0.4, 2) samples = metropolis(func=func, steps=100000) x = np.linspace(0.01, .99, 100) y = func.pdf(x) plt.figure(figsize=(8,8)) plt.xlim(0, 1) plt.plot(x, y, \u0026#39;r-\u0026#39;, lw=3, label=\u0026#39;Distribuci√≥n verdadera\u0026#39;) plt.hist(samples, bins=30, normed=True, label=\u0026#39;Distribuci√≥n estimada con MCMC\u0026#39;) plt.xlabel(\u0026#39;$x$\u0026#39;, fontsize=14) plt.ylabel(\u0026#39;$pdf(x)$\u0026#39;, fontsize=14) plt.legend(fontsize=14) plt.show() # distribuci√≥n normal func = stats.norm(0.4, 2) samples = metropolis(func=func) x = np.linspace(-6, 10, 100) y = func.pdf(x) plt.figure(figsize=(8,8)) plt.xlim(-6, 6) plt.plot(x, y, \u0026#39;r-\u0026#39;, lw=3, label=\u0026#39;Distribuci√≥n verdadera\u0026#39;) plt.hist(samples, bins=30, normed=True, label=\u0026#39;Distribuci√≥n estimada con MCMC\u0026#39;) plt.xlabel(\u0026#39;$x$\u0026#39;, fontsize=14) plt.ylabel(\u0026#39;$pdf(x)$\u0026#39;, fontsize=14) plt.legend(fontsize=14) plt.show() como vemos, las distribuciones estimadas utilizando MCMC se acercan bastante a las distribuciones reales.\nOtros m√©todos MCMC# Adem√°s del algoritmo Metropolis-Hastings existen otros algoritmos de muestreo que utilizan los m√©todos MCMC. Algunos de ellos son:\nMuestreo de Gibbs, el cual es un caso especial del algoritmo Metropolis-Hastings.\nMonte-Carlo Hamiltoniano o h√≠brido, el cual reduce la correlaci√≥n entre los sucesivos estados de muestreo usando una evoluci√≥n Hamiltoniana.\nMuestreo de rebanada o Slice sampler, este m√©todo se basa en la observaci√≥n de que para muestrear una variable aleatoria se pueden tomar muestras en forma uniforme de la regi√≥n debajo del gr√°fico de su funci√≥n de densidad.\nNUTS o No U turn sampler, el cual es una extensi√≥n del algoritmo h√≠brido de Monte-Carlo que logra incluso mayor eficiencia.\nCon esto concluye este paseo por los M√©todos de Monte-Carlo y la estad√≠stica computacional, espero que les haya parecido interesante y les sea de utilidad en sus proyectos.\nSaludos!\nEste post fue escrito utilizando IPython notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2017-01-10","id":13,"permalink":"/blog/2017/01/10/introduccion-a-los-metodos-de-monte-carlo-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# En el cierre de mi art√≠culo anterior, comentaba sobre la sorprendente influencia que han tenido los n√∫meros aleatorios, que junto con el poder de c√°lculo que nos proporcionan las computadoras modernas; nos han ayudado a resolver muchos de los problemas num√©ricos m√°s complejos en ciencia, ingenier√≠a, finanzas y estad√≠sticas.","tags":["python","estadistica","programacion","analisis de datos","probabilidad","distribuciones","matematica","monte carlo","metropolis"],"title":"Introducci√≥n a los m√©todos de Monte-Carlo con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\n\u0026ldquo;En el fondo, la teor√≠a de probabilidades es s√≥lo sentido com√∫n expresado con n√∫meros\u0026rdquo;\nPierre Simon de Laplace\nIntroducci√≥n: probabilidad y sentido com√∫n# La incertidumbre constituye una pieza fundamental del mundo en que vivimos, en parte hace la vida mucho m√°s interesante, ya que ser√≠a muy aburrido si todo fuera perfectamente predecible. Aun as√≠, una parte de nosotros quisiera predecir el futuro y que las cosas sean mucho m√°s predecibles. Para poder lidiar con la incertidumbre que nos rodea, solemos aplicar lo que llamamos nuestro \u0026ldquo;sentido com√∫n\u0026rdquo;. Por ejemplo, si al levantarnos por la ma√±ana vemos que el d√≠a se encuentra nublado, este hecho no nos da la certeza de que comenzar√° a llover m√°s tarde; sin embargo, nuestro sentido com√∫n puede inducirnos a cambiar nuestros planes y a actuar como si crey√©ramos que fuera a llover si las nubes son los suficientemente oscuras o si escuchamos truenos, ya que nuestra experiencia nos dice que estos signos indicar√≠an una mayor posibilidad de que el hecho de que fuera a llover m√°s tarde realmente ocurra. Nuestro sentido com√∫n es algo tan arraigado en nuestro pensamiento, que lo utilizamos autom√°ticamente sin siquiera ponernos a pensar en ello; pero muchas veces, el sentido com√∫n tambi√©n nos puede jugar una mala pasada y hacernos elegir una respuesta incorrecta.\nTomemos por ejemplo alguna de las siguiente situaciones\u0026hellip;\nSituaci√≥n 1 - La coincidencia de cumplea√±os: Vamos a una fiesta a la que concurren un total de 50 personas. All√≠ un amigo nos desaf√≠a afirmando que en la fiesta debe haber por lo menos 2 personas que cumplen a√±os el mismo d√≠a y nos apuesta 100 pesos a que est√° en lo correcto. Es decir, que si √©l acierta deber√≠amos pagarle los 100 pesos; o en caso contrario, el nos pagar√° los 100 pesos. ¬øDeber√≠amos aceptar la apuesta?\nSituaci√≥n 2 - ¬øQue puerta elegir?: Estamos participando en un concurso en el cual se nos ofrece la posibilidad de elegir una entre tres puertas. Tras una de ellas se encuentra una ferrari ultimo modelo, y detr√°s de las otras dos hay una cabra; luego de elegir una puerta, el presentador del concurso abre una de las puertas restantes y muestra que hay una cabra (el presentador sabe que hay detr√°s de cada puerta). Luego de hacer esto, el presentador nos ofrece la posibilidad de cambiar nuestra elecci√≥n inicial y quedarnos con la otra puerta que no hab√≠amos elegido inicialmente. ¬øDeber√≠amos cambiar o confiar en nuestra elecci√≥n inicial?\n¬øQu√© les dir√≠a su sentido com√∫n que deber√≠an hacer en cada una de estas situaciones?\nPara poder responder √©stas y otras preguntas de una manera m√°s rigurosa, primero deber√≠amos de alguna forma modelar matem√°ticamente nuestro sentido com√∫n, es aqu√≠, como lo expresa la frase del comienzo del art√≠culo, como surge la teor√≠a de probabilidad.\n¬øQu√© es la teor√≠a de probabilidad?# La teor√≠a de probabilidad es la rama de las matem√°ticas que se ocupa de los fen√≥menos aleatorios y de la incertidumbre. Existen muchos eventos que no se pueden predecir con certeza; ya que su observaci√≥n repetida bajo un mismo conjunto espec√≠fico de condiciones puede arrojar resultados distintos, mostrando un comportamiento err√°tico e impredecible. En estas situaciones, la teor√≠a de probabilidad proporciona los m√©todos para cuantificar las posibilidades, o probabilidades, asociadas con los diversos resultados. Su estudio ha atra√≠do a un gran n√∫mero de gente, ya sea por su inter√©s intr√≠nseco como por su aplicaci√≥n con √©xito en las ciencias f√≠sicas, biol√≥gicas y sociales, as√≠ como tambi√©n en √°reas de la ingenier√≠a y en el mundo de los negocios.\nCuantificando la incertidumbre# Ahora bien, en la definici√≥n de arriba dijimos que la teor√≠a de probabilidad, nos proporciona las herramientas para poder cuantificar la incertidumbre, pero ¬øc√≥mo podemos realmente cuantificar estos eventos aleatorios y hacer inferencias sobre ellos? La respuesta a esta pregunta es, a su vez, intuitiva y simple; la podemos encontrar en el concepto del espacio de muestreo.\nEl Espacio de muestreo# El espacio de muestreo hace referencia a la idea de que los posibles resultados de un proceso aleatorio pueden ser pensados como puntos en el espacio. En los casos m√°s simples, este espacio puede consistir en s√≥lo algunos puntos, pero en casos m√°s complejos puede estar representado por un continuo, como el espacio en que vivimos. El espacio de muestreo , en general se expresa con la letra \\(S\\), y consiste en el conjunto de todos los resultados posibles de un experimento. Si el experimento consiste en el lanzamiento de una moneda, entonces el espacio de muestreo ser√° \\(S = {cara, seca }\\), ya que estas dos alternativas representan a todos los resultados posibles del experimento. En definitiva el espacio de muestreo no es m√°s que una simple enumeraci√≥n de todos los resultados posibles, aunque las cosas nunca suelen ser tan simples como aparentan. Si en lugar de considerar el lanzamiento de una moneda, lanzamos dos monedas; uno podr√≠a pensar que el espacio de muestreo para este caso ser√° \\(S = {\\text{ 2 caras}, \\text{2 secas}, \\text{cara y seca} }\\); es decir que de acuerdo con este espacio de muestreo la probabilidad de que obtengamos dos caras es 1 en 3; pero la verdadera probabilidad de obtener dos caras, confirmada por la experimentaci√≥n, es 1 en 4; la cual se hace evidente si definimos correctamente el espacio de muestreo, que ser√° el siguiente: \\(S = {\\text{ 2 caras}, \\text{2 secas}, \\text{cara y seca}, \\text{seca y cara} }\\). Como este simple ejemplo nos ense√±a, debemos ser muy cuidadosos al definir el espacio de muestreo, ya que una mala definici√≥n del mismo, puede inducir a c√°lculos totalmente errados de la probabilidad.\nIndependencia, la ley de grandes n√∫meros y el teorema del l√≠mite central# Una de las cosas m√°s fascinantes sobre el estudio de la teor√≠a de probabilidad es que si bien el comportamiento de un evento individual es totalmente impredecible, el comportamiento de una cantidad suficientemente grande de eventos se puede predecir con un alto grado de certeza!. Si tomamos el caso cl√°sico del lanzamiento de una moneda, no podemos predecir con exactitud cuantas caras podemos obtener luego de 10 tiradas, tal vez el azar haga que obtengamos 7, 10, o 3 caras, dependiendo de con cuanta suerte nos encontremos; pero si repetimos el lanzamiento un mill√≥n de veces, casi con seguridad que la cantidad de caras se aproximar√° a la verdadera probabilidad subyacente del experimento, es decir, al 50% de los lanzamientos. Este comportamiento es lo que en la teor√≠a de probabilidad se conoce con el nombre de ley de grandes n√∫meros; pero antes de poder definir esta ley, primero debemos describir otro concepto tambi√©n muy importante, la independencia de los eventos .\nEl concepto de independencia# En teor√≠a de probabilidad, podemos decir que dos eventos son independientes cuando la probabilidad de cada uno de ellos no se ve afecta porque el otro evento ocurra, es decir que no existe ninguna relaci√≥n entre los eventos. En el lanzamiento de la moneda; la moneda no sabe, ni le interesa saber si el resultado del lanzamiento anterior fue cara; cada lanzamiento es un suceso totalmente aislado el uno del otro y la probabilidad del resultado va a ser siempre 50% en cada lanzamiento.\nDefiniendo la ley de grandes n√∫meros# Ahora que ya conocemos el concepto de independencia, estamos en condiciones de dar una definici√≥n m√°s formal de la ley de grandes n√∫meros, que junto con el Teorema del l√≠mite central, constituyen los cimientos de la teor√≠a de probabilidad. Podemos formular esta ley de la siguiente manera: si se repite un experimento aleatorio, bajo las mismas condiciones, un n√∫mero ilimitado de veces; y si estas repeticiones son independientes la una de la otra, entonces la frecuencia de veces que un evento \\(A\\) ocurra, converger√° con probabilidad 1 a un n√∫mero que es igual a la probabilidad de que \\(A\\) ocurra en una sola repetici√≥n del experimento. Lo que esta ley nos ense√±a, es que la probabilidad subyacente de cualquier suceso aleatorio puede ser aprendido por medio de la experimentaci√≥n, simplemente tendr√≠amos que repetirlo una cantidad suficientemente grande de veces!. Un error que la gente suele cometer y asociar a esta ley, es la idea de que un evento tiene m√°s posibilidades de ocurrir porque ha o no ha ocurrido recientemente. Esta idea de que las chances de un evento con una probabilidad fija, aumentan o disminuyen dependiendo de las ocurrencias recientes del evento, es un error que se conoce bajo el nombre de la falacia del apostador.\nPara entender mejor la ley de grandes n√∫meros, experimentemos con algunos ejemplos en Python. Utilicemos nuevamente el ejemplo del lanzamiento de la moneda, en el primer ejemplo, la moneda va a tener la misma posibilidad de caer en cara o seca; mientras que en el segundo ejemplo, vamos a modificar la probabilidad de la moneda para que caiga cara solo en 1 de 6 veces.\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # importando modulos necesarios import matplotlib.pyplot as plt import numpy as np # importando numpy import pandas as pd # importando pandas np.random.seed(2131982) # para poder replicar el random %matplotlib inline # Ejemplo ley de grandes n√∫meros # moneda p=1/2 cara=1 seca=0 resultados = [] for lanzamientos in range(1,10000): lanzamientos = np.random.choice([0,1], lanzamientos) caras = lanzamientos.mean() resultados.append(caras) # graficamente df = pd.DataFrame({ \u0026#39;lanzamientos\u0026#39; : resultados}) df.plot(title=\u0026#39;Ley de grandes n√∫meros\u0026#39;,color=\u0026#39;r\u0026#39;,figsize=(8, 6)) plt.axhline(0.5) plt.xlabel(\u0026#34;N√∫mero de lanzamientos\u0026#34;) plt.ylabel(\u0026#34;frecuencia caras\u0026#34;) plt.show() # moneda p=1/6 cara=1 seca=0 resultados = [] for lanzamientos in range(1,10000): lanzamientos = np.random.choice([0,1], lanzamientos, p=[5/6, 1/6]) caras = lanzamientos.mean() resultados.append(caras) # graficamente df = pd.DataFrame({ \u0026#39;lanzamientos\u0026#39; : resultados}) df.plot(title=\u0026#39;Ley de grandes n√∫meros\u0026#39;,color=\u0026#39;r\u0026#39;,figsize=(8, 6)) plt.axhline(1/6) plt.xlabel(\u0026#34;N√∫mero de lanzamientos\u0026#34;) plt.ylabel(\u0026#34;frecuencia caras\u0026#34;) plt.show() Como estos ejemplos nos muestran, al comienzo, la frecuencia en que vamos obteniendo caras va variando considerablemente, pero a medida que aumentamos el n√∫mero de repeticiones, la frecuencia de caras se va estabilizando en la probabilidad subyacente el evento, 1 en 2 para el primer caso y 1 en 6 para el segundo ejemplo. En los gr√°ficos podemos ver claramente el comportamiento de la ley.\nEl Teorema del l√≠mite central# El otro gran teorema de la teor√≠a de probabilidad es el Teorema del l√≠mite central. Este teorema establece que la suma o el promedio de casi cualquier conjunto de variables independientes generadas al azar se aproximan a la Distribuci√≥n Normal. El Teorema del l√≠mite central explica por qu√© la Distribuci√≥n Normal surge tan com√∫nmente y por qu√© es generalmente una aproximaci√≥n excelente para la media de casi cualquier colecci√≥n de datos. Este notable hallazgo se mantiene verdadero sin importar la forma que adopte la distribuci√≥n de datos que tomemos. Para ilustrar tambi√©n este teorema, recurramos a un poco m√°s de Python.\n# Ejemplo teorema del l√≠mite central muestra_binomial = [] muestra_exp = [] muestra_possion = [] muestra_geometric = [] mu = .9 lam = 1.0 size=1000 for i in range(1,20000): muestra = np.random.binomial(1, mu, size=size) muestra_binomial.append(muestra.mean()) muestra = np.random.exponential(scale=2.0,size=size) muestra_exp.append(muestra.mean()) muestra = np.random.geometric(p=.5, size=size) muestra_geometric.append(muestra.mean()) muestra = np.random.poisson (lam=lam, size=size) muestra_possion.append(muestra.mean()) df = pd.DataFrame({ \u0026#39;binomial\u0026#39; : muestra_binomial, \u0026#39;poission\u0026#39; : muestra_possion, \u0026#39;geometrica\u0026#39; : muestra_geometric, \u0026#39;exponencial\u0026#39; : muestra_exp}) fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10,10)) df.binomial.hist(ax=axes[0,0], alpha=0.9, bins=1000) df.exponencial.hist(ax=axes[0,1],bins=1000) df.poission.hist(ax=axes[1,0],bins=1000) df.geometrica.hist(ax=axes[1,1],bins=1000) axes[0,0].set_title(\u0026#39;Binomial\u0026#39;) axes[0,1].set_title(\u0026#39;Poisson\u0026#39;) axes[1,0].set_title(\u0026#39;Geom√©trica\u0026#39;) axes[1,1].set_title(\u0026#39;Exponencial\u0026#39;) plt.show() Como nos muestra este ejemplo, al graficar la distribuci√≥n de las medias de las distribuciones Binomial, Poisson, Geom√©trica y Exponencial; vemos que todas ellas responden a la famosa forma de campana de la Distribuci√≥n Normal!. Algo realmente sorprendente!\nCalculando probabilidades# Saber calcular la probabilidad de que un evento o varios eventos ocurran puede ser una habilidad valiosa al tomar decisiones, ya sea en la vida real o jugando juegos de azar. C√≥mo calcular la probabilidad, sin embargo, cambia dependiendo del tipo de evento que se est√° observando. Por ejemplo, no calcular√≠amos nuestras posibilidades de ganar la loter√≠a de la misma manera que calcular√≠amos nuestras posibilidades de obtener una generala servida en un juego de dados. Sin embargo, una vez que determinamos si los eventos son independientes, condicionales o mutuamente excluyentes, calcular su probabilidad es relativamente simple.\nPropiedades b√°sicas de la probabilidad# Antes de poder calcular las probabilidades, primero debemos conocer sus 3 propiedades fundamentales, ellas son:\nLa probabilidad se expresa como un ratio que ser√° un valor positivo menor o igual a 1. \\( 0 \\le p(A) \\le 1\\)\nLa probabilidad de un evento del que tenemos total certeza es 1. \\() p(S) = 1 \\)\nSi el evento \\(A\\) y el evento \\(B\\) son mutuamente excluyentes, entonces: \\( p(A \\cup B ) = p(A) + p(B) \\)\nA partir de estas propiedades b√°sicas, se pueden derivar muchas otras propiedades.\nTeor√≠a de conjuntos y probabilidades# En mi art√≠culo sobre conjuntos comentaba que la teor√≠a de conjuntos se ha convertido en un pilar fundamental de las matem√°ticas, casi cualquier rama de las matem√°ticas puede ser definida utilizando conjuntos; y la teor√≠a de probabilidad no es la excepci√≥n. Antes de poder calcular probabilidades, primero debemos discutir como se relacionan los eventos en t√©rminos de la teor√≠a de conjuntos. Las relaciones que podemos encontrar son:\nUni√≥n: La uni√≥n de varios eventos simples crea un evento compuesto que ocurre si uno o m√°s de los eventos ocurren. La uni√≥n de \\(E\\) y \\(F\\) se escribe \\(E \\cup F\\) y significa \u0026ldquo;Ya sea \\(E\\) o \\(F\\), o ambos \\(E\\) y \\(F\\).\u0026rdquo;\nIntersecci√≥n: La intersecci√≥n de dos o m√°s eventos simples crea un evento compuesto que ocurre s√≥lo si ocurren todos los eventos simples. La intersecci√≥n de \\(E\\) y \\(F\\) se escribe \\(E \\cap F\\) y significa \u0026ldquo;\\(E\\) y \\(F\\).\u0026rdquo;\nComplemento: El complemento de un evento significa todo en el espacio de muestreo que no es ese evento. El complemento del evento \\(E\\) se escribe varias veces como \\(\\sim{E}\\), \\(E^c\\), o \\(\\overline{E}\\), y se lee como \u0026ldquo;no \\(E\\)\u0026rdquo; o \u0026ldquo;complemento \\(E\\)\u0026rdquo;.\nExclusi√≥n mutua: Si los eventos no pueden ocurrir juntos, son mutuamente excluyentes. Siguiendo la misma l√≠nea de razonamiento, si dos conjuntos no tienen ning√∫n evento en com√∫n, son mutuamente excluyentes.\nCalculando la probabilidad de m√∫ltiples eventos# Ahora s√≠, ya podemos calcular las probabilidades de los eventos. Recordemos que la probabilidad de un solo evento se expresa como un ratio entre el n√∫mero de resultados favorables sobre el n√∫mero de los posibles resultados. Pero ¬øqu√© pasa cuando tenemos m√∫ltiples eventos?\nUni√≥n de eventos mutuamente excluyentes# Si los eventos son mutuamente excluyentes entonces para calcular la probabilidad de su uni√≥n, simplemente sumamos sus probabilidades individuales.\n$$p(E \\cup F) = p(E) + p(F)$$ Uni√≥n de eventos que no son mutuamente excluyentes# Si los eventos no son mutuamente excluyentes entonces debemos corregir la f√≥rmula anterior para incluir el efecto de la superposici√≥n de los eventos. Esta superposici√≥n se da en el lugar de la intersecci√≥n de los eventos; por lo tanto la formula para calcular la probabilidad de estos eventos es:\n$$p(E \\cup F) = p(E) + p(F) - p(E \\cap F)$$ Intersecci√≥n de eventos independientes# Para calcular la probabilidad de que ocurran varios eventos (la intersecci√≥n de varios eventos), se multiplican sus probabilidades individuales. La f√≥rmula espec√≠fica utilizada depender√° de si los eventos son independientes o no. Si son independientes, la probabilidad de \\(E\\) y \\(F\\) se calcula como:\n$$p(E \\cap F) = p(E) \\times p(F)$$ Intersecci√≥n de eventos no independientes# Si dos eventos no son independientes, debemos conocer su probabilidad condicional para poder calcular la probabilidad de que ambos se produzcan. La f√≥rmula en este caso es:\n$$p(E \\cap F) = p(E) \\times p(F|E)$$ La probabilidad condicional# Con frecuencia queremos conocer la probabilidad de alg√∫n evento, dado que otro evento ha ocurrido. Esto se expresa simb√≥licamente como \\(p(E | F)\\) y se lee como \u0026ldquo;la probabilidad de \\(E\\) dado \\(F\\)\u0026rdquo;. El segundo evento se conoce como la condici√≥n y el proceso se refiere a veces como \u0026ldquo;condicionamiento en F\u0026rdquo;. La probabilidad condicional es un concepto importante de estad√≠stica, porque a menudo estamos tratando de establecer que un factor tiene una relaci√≥n con un resultado, como por ejemplo, que las personas que fuman cigarrillos tienen m√°s probabilidades de desarrollar c√°ncer de pulm√≥n. La probabilidad condicional tambi√©n se puede usar para definir la independencia. Dos variables se dice que son independientes si la siguiente relaci√≥n se cumple:\n$$p(E | F) = p(E)$$ Calculando la probabilidad condicional# Para calcular la probabilidad del evento \\(E\\) dada la informaci√≥n de que el evento \\(F\\) ha ocurrido utilizamos la siguiente formula:\n$$p(E | F) = \\frac{p(E \\cap F)}{p(F)}$$ Jugando con Probabilidades y Python# Bien, ahora que ya sabemos como calcular probabilidades, lleg√≥ finalmente el momento de ponerse a resolver las situaciones planteadas en el comienzo, para eso vamos a utilizar nuevamente un poco de Python.\nResolviendo la situaci√≥n 1 - La coincidencia de cumplea√±os# La paradoja del cumplea√±os es un problema muy conocido en el campo de la teor√≠a de probabilidad. Plantea las siguientes interesantes preguntas: ¬øCu√°l es la probabilidad de que, en un grupo de personas elegidas al azar, al menos dos de ellas habr√°n nacido el mismo d√≠a del a√±o? ¬øCu√°ntas personas son necesarias para asegurar una probabilidad mayor al 50%?. Excluyendo el 29 de febrero de nuestros c√°lculos y asumiendo que los restantes 365 d√≠as de posibles cumplea√±os son igualmente probables, nos sorprender√≠a darnos cuenta de que, en un grupo de s√≥lo 23 personas, la probabilidad de que dos personas compartan la misma fecha de cumplea√±os es mayor al 50%!.\nEsto ya nos dice algo respecto a nuestras chances en la apuesta con nuestro amigo, pero de todas formas calculemos la probabilidad en un grupo de 50 personas. Calcular la probabilidad de un cumplea√±os duplicado puede parecer una tarea desalentadora. Pero ¬øqu√© pasa con calcular la probabilidad de que no haya un cumplea√±os duplicado? Esto es realmente una tarea m√°s f√°cil. Especialmente si simplificamos el problema a un grupo muy peque√±o. Supongamos que el grupo s√≥lo tiene una persona, en ese caso, hay una probabilidad del 100% que esta persona no comparte un cumplea√±os puesto que no hay nadie m√°s quien compartir. Pero ahora podemos a√±adir una segunda persona al grupo. ¬øCu√°les son las posibilidades de que tenga un cumplea√±os diferente de esa persona? De hecho esto es bastante f√°cil, hay 364 otros d√≠as en el a√±o, as√≠ que las posibilidades son 364/365. ¬øQu√© tal si agregamos una tercera persona al grupo? Ahora hay 363/365 d√≠as. Para obtener la probabilidad general de que no hay cumplea√±os compartidos simplemente multiplicamos las probabilidades individuales. Si utilizamos este procedimiento, con la ayuda de Python podemos calcular f√°cilmente las probabilidades de un cumplea√±os compartido en un grupo de 50 personas.\n# Ejemplo situaci√≥n 2 La coincidencia de cumplea√±os prob = 1.0 asistentes = 50 for i in range(asistentes): prob = prob * (365-i)/365 print(\u0026#34;Probabilidad de que compartan una misma fecha de cumplea√±os es {0:.2f}\u0026#34; .format(1 - prob)) Probabilidad de que compartan una misma fecha de cumplea√±os es 0.97 Como vemos, la apuesta de nuestro amigo es casi una apuesta segura para √©l. Se ve que conoce bastante bien la teor√≠a de probabilidad y quiere disfrutar de la fiesta a consta nuestra!\nResolviendo la situaci√≥n 2 - ¬øQue puerta elegir?# Este problema, m√°s conocido con el nombre de Monty Hall, tambi√©n es un problema muy popular dentro de la teor√≠a de probabilidad; y se destaca por su soluci√≥n que a simple vista parece totalmente anti-intuitiva. Intuitivamente, es bastante sencillo que nuestra elecci√≥n original (cuando hay tres puertas para elegir) tiene una probabilidad de 1/3 de ganar el concurso. Las cosas sin embargo se complican, cuando se descarta una puerta. Muchos dir√≠an que ahora tenemos una probabilidad de 1/2 de ganar, seleccionando cualquiera de las dos puertas; pero este no es el caso. Un aspecto cr√≠tico del problema es darse cuenta de que la elecci√≥n de la puerta a descartar por el presentador, no es una decisi√≥n al azar. El presentador puede descartar una puerta porque √©l sabe (a) qu√© puerta hemos seleccionado y (b) qu√© puerta tiene la ferrari. De hecho, en muchos casos, el presentador debe quitar una puerta espec√≠fica. Por ejemplo, si seleccionamos la puerta 1 y el premio est√° detr√°s de la puerta 3, el presentador no tiene otra opci√≥n que retirar la puerta 2. Es decir, que la elecci√≥n de la puerta a descartar est√° condicionada tanto por la puerta con el premio como por la puerta que seleccionamos inicialmente. Este hecho, cambia totalmente la naturaleza del juego, y hace que las probabilidades de ganar sean 2/3 si cambiamos de puerta!.\nSi aun no est√°n convencidos, simulemos los resultados del concurso con la ayuda de Python.\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # Ejemplo situaci√≥n 2 ¬øQue puerta elegir? (el problema Monty Hall) def elegir_puerta(): \u0026#34;\u0026#34;\u0026#34; Funci√≥n para elegir una puerta. Devuelve 1, 2, o 3 en forma aleatoria. \u0026#34;\u0026#34;\u0026#34; return np.random.randint(1,4) class MontyHall: \u0026#34;\u0026#34;\u0026#34; Clase para modelar el problema de Monty Hall. \u0026#34;\u0026#34;\u0026#34; def __init__(self): \u0026#34;\u0026#34;\u0026#34; Crea la instancia del problema. \u0026#34;\u0026#34;\u0026#34; # Elige una puerta en forma aleatoria. self.puerta_ganadora = elegir_puerta() # variables para la puerta elegida y la puerta descartada self.puerta_elegida = None self.puerta_descartada = None def selecciona_puerta(self): \u0026#34;\u0026#34;\u0026#34; Selecciona la puerta del concursante en forma aleatoria. \u0026#34;\u0026#34;\u0026#34; self.puerta_elegida = elegir_puerta() def descarta_puerta(self): \u0026#34;\u0026#34;\u0026#34; Con este m√©todo el presentador descarta una de la puertas. \u0026#34;\u0026#34;\u0026#34; # elegir puerta en forma aleatoria . d = elegir_puerta() # Si es al puerta ganadora o la del concursante, volver a elegir. while d == self.puerta_ganadora or d == self.puerta_elegida: d = elegir_puerta() # Asignar el valor a puerta_descartada. self.puerta_descartada = d def cambiar_puerta(self): \u0026#34;\u0026#34;\u0026#34; Cambia la puerta del concursante una vez que se elimino una puerta. \u0026#34;\u0026#34;\u0026#34; # 1+2+3=6. Solo existe una puerta para elegir. self.puerta_elegida = 6 - self.puerta_elegida - self.puerta_descartada def gana_concursante(self): \u0026#34;\u0026#34;\u0026#34; Determina si el concursante gana. Devuelve True si gana, False si pierde. \u0026#34;\u0026#34;\u0026#34; return self.puerta_elegida == self.puerta_ganadora def jugar(self, cambiar=True): \u0026#34;\u0026#34;\u0026#34; Una vez que la clase se inicio, jugar el concurso. \u0026#39;cambiar\u0026#39; determina si el concursante cambia su elecci√≥n. \u0026#34;\u0026#34;\u0026#34; # El concursante elige una puerta. self.selecciona_puerta() # El presentador elimina una puerta. self.descarta_puerta() # El concursante cambia su elecci√≥n. if cambiar: self.cambiar_puerta() # Determinar si el concursante ha ganado. return self.gana_concursante() # Ahora, jugamos el concurso. primero nos vamos a quedar con nuestra elecci√≥n # inicial. Vamos a ejecutar el experimiento 10.000 veces. gana, pierde = 0, 0 for i in range(10000): # Crear la instancia del problema. s2 = MontyHall() # ejecutar el concurso sin cambiar de puerta.. if s2.jugar(cambiar=False): # si devuelve True significa que gana. gana += 1 else: # si devuelve False significa que pierde. pierde += 1 # veamos la fecuencia de victorias del concursante. porc_gana = 100.0 * gana / (gana + pierde) print(\u0026#34;\\n10.000 concursos sin cambiar de puerta:\u0026#34;) print(\u0026#34; gana: {0:} concursos\u0026#34;.format(gana)) print(\u0026#34; pierde: {0:} concursos\u0026#34;.format(pierde)) print(\u0026#34; probabilidad: {0:.2f} procentaje de victorias\u0026#34;.format(porc_gana)) 10.000 concursos sin cambiar de puerta: gana: 3311 concursos pierde: 6689 concursos probabilidad: 33.11 procentaje de victorias # Ahora, jugamos el concurso siempre cambiando la elecci√≥n inicial # Vamos a ejecutar el experimiento 10.000 veces. gana, pierde = 0, 0 for i in range(10000): # Crear la instancia del problema. s2 = MontyHall() # ejecutar el concurso sin cambiar de puerta.. if s2.jugar(cambiar=True): # si devuelve True significa que gana. gana += 1 else: # si devuelve False significa que pierde. pierde += 1 # veamos la fecuencia de victorias del concursante. porc_gana = 100.0 * gana / (gana + pierde) print(\u0026#34;\\n10.000 concursos cambiando de puerta:\u0026#34;) print(\u0026#34; gana: {0:} concursos\u0026#34;.format(gana)) print(\u0026#34; pierde: {0:} concursos\u0026#34;.format(pierde)) print(\u0026#34; probabilidad: {0:.2f} procentaje de victorias\u0026#34;.format(porc_gana)) 10.000 concursos cambiando de puerta: gana: 6591 concursos pierde: 3409 concursos probabilidad: 65.91 procentaje de victorias Como esta simulaci√≥n lo demuestra, si utilizamos la estrategia de siempre cambiar de puerta, podemos ganar el concurso un 66% de las veces; mientras que si nos quedamos con nuestra elecci√≥n inicial, solo ganar√≠amos el 33% de las veces.\nDistintas interpretaciones de la probabilidad# Las probabilidades pueden ser interpretadas generalmente de dos maneras distintas. La interpretaci√≥n frecuentista u objetivista de la probabilidad es una perspectiva en la que las probabilidades se consideran frecuencias relativas constantes a largo plazo. Este es el enfoque cl√°sico de la teor√≠a de probabilidad. La interpretaci√≥n Bayesiana o subjetivista de la probabilidad es una perspectiva en la que las probabilidades son consideradas como medidas de creencia que pueden cambiar con el tiempo para reflejar nueva informaci√≥n. El enfoque cl√°sico sostiene que los m√©todos bayesianos sufren de falta de objetividad, ya que diferentes individuos son libres de asignar diferentes probabilidades al mismo evento seg√∫n sus propias opiniones personales. Los bayesianos se oponen a los cl√°sicos sosteniendo que la interpretaci√≥n frecuentista de la probabilidad tiene ya de por s√≠ una subjetividad incorporada (por ejemplo, mediante la elecci√≥n y el dise√±o del procedimiento de muestreo utilizado) y que la ventaja del enfoque bayesiano es que ya hace expl√≠cita esta subjetividad. En la actualidad, la mayor√≠a de los problemas son abordados siguiendo un enfoque mixto entre ambas interpretaciones de la probabilidad.\nEl poder de los n√∫meros aleatorios# Uno podr√≠a pensar que un comportamiento aleatorio es ca√≥tico y totalmente opuesto a la raz√≥n, que ser√≠a una forma de renunciar a un problema, un √∫ltimo recurso. Pero lejos de esto, el sorprendente y cada vez m√°s importante rol que viene desempe√±ando lo aleatorio en las ciencias de la computaci√≥n nos ense√±a que el hacer un uso deliberado de lo aleatorio puede ser una forma muy efectiva de abordar los problemas m√°s dif√≠ciles; incluso en algunos casos, puede ser el √∫nico camino viable. Los Algoritmos probabil√≠sticos como el m√©todo Miller-Rabin para encontrar n√∫meros primos y el m√©todo de Monte Carlo, nos demuestran lo poderoso que puede ser utilizar la aleatoriedad para resolver problemas. Muchas veces, la mejor soluci√≥n a un problema, puede ser simplemente dejarlo al azar en lugar de tratar de razonar totalmente su soluci√≥n!\nAqu√≠ concluye este art√≠culo. Espero les haya resultado √∫til y encuentren tan fascinante como yo a la teor√≠a de probabilidad; despu√©s de todo, la incertidumbre esta en todo lo que nos rodea y la casualidad es un concepto m√°s fundamental que la causalidad!\nSaludos!\nEste post fue escrito utilizando IPython notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2016-11-26","id":14,"permalink":"/blog/2016/11/26/introduccion-a-la-teoria-de-probabilidad-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\n\u0026ldquo;En el fondo, la teor√≠a de probabilidades es s√≥lo sentido com√∫n expresado con n√∫meros\u0026rdquo;\nPierre Simon de Laplace\nIntroducci√≥n: probabilidad y sentido com√∫n# La incertidumbre constituye una pieza fundamental del mundo en que vivimos, en parte hace la vida mucho m√°s interesante, ya que ser√≠a muy aburrido si todo fuera perfectamente predecible.","tags":["python","estadistica","probabilidad","distribuciones","analisis de datos"],"title":"Introducci√≥n a la teor√≠a de probabilidad con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Los datos obtenidos a partir de observaciones recogidas a lo largo del tiempo son extremadamente comunes. En los negocios, observamos las tasas de inter√©s de la semana, los precios de cierre de las acciones diarios, los √≠ndices de precios mensuales, las cifras de ventas anuales, y as√≠ sucesivamente. En meteorolog√≠a, observamos las temperaturas m√°ximas y m√≠nimas diarias, los √≠ndices anuales de precipitaci√≥n y de sequ√≠a, y las velocidades del viento por hora. En la agricultura, registramos las cifras anuales de producci√≥n agr√≠cola y ganadera, la erosi√≥n del suelo, y las ventas de exportaci√≥n. En las ciencias biol√≥gicas, observamos la actividad el√©ctrica del coraz√≥n en intervalos de milisegundos. La lista de las √°reas en las que se estudian las series de tiempo es pr√°cticamente interminable.\n¬øQu√© es una serie de tiempo?# Una serie de tiempo o serie temporal es una secuencia de datos, observaciones o valores, medidos en determinados momentos y ordenados cronol√≥gicamente. Los datos pueden estar espaciados a intervalos iguales o desiguales. Una vez que se captura una serie de tiempo, a menudo se realiza un an√°lisis sobre ella para identificar patrones en los datos, en esencia, lo que se busca es entender que suceda a medida que el tiempo va avanzando. Ser capaz de procesar datos de series de tiempo es una habilidad esencial en el mundo moderno. Uno de los usos m√°s habituales de las series de tiempo es su an√°lisis para predicci√≥n y pron√≥stico (as√≠ se hace por ejemplo con los datos clim√°ticos, las acciones de bolsa, o las series de datos demogr√°ficos). Resulta dif√≠cil imaginar una rama de las ciencias en la que no aparezcan datos que puedan ser considerados como series de tiempo.\n¬øQu√© las hace especiales?# Las caracter√≠sticas que hacen a las series de tiempo especiales y las diferencia de, por ejemplo, un problema de regresi√≥n son las siguientes:\nSon dependientes del tiempo; por lo tanto el supuesto b√°sico de los modelos de regresi√≥n de que las observaciones son independientes no se sostiene en este caso.\nSuelen tener una tendencia; la mayor√≠a de las series de tiempo suelen tener alg√∫n tipo de tendencia de estacionalidad, es decir, las variaciones propias de un per√≠odo de tiempo determinado.\nSuelen estar autocorrelacionadas; la mayor√≠a de los procesos f√≠sicos presentan una inercia y no cambian tan r√°pidamente. Esto, combinado con la frecuencia del muestreo, a menudo hace que las observaciones consecutivas est√©n correlacionadas. Esta correlaci√≥n entre observaciones consecutivas se llama autocorrelaci√≥n. Cuando los datos est√°n autocorrelacionados, la mayor√≠a de los m√©todos estad√≠sticos est√°ndares basados en la suposici√≥n de observaciones independientes pueden arrojar resultados enga√±osos o incluso ser in√∫tiles.\nSeries de tiempo estacionarias# Un tipo muy importante de series de tiempo son las series de tiempo estacionarias. Una series de tiempo se dice que es estrictamente estacionaria si sus propiedades no son afectadas por los cambios a lo largo del tiempo. Es decir, que se deber√≠an cumplir tres criterios b√°sicos para poder considerar a una series de tiempo como estacionaria:\nLa media de la serie no debe ser una funci√≥n de tiempo; sino que debe ser constante. La siguiente imagen muestra una serie que cumple con esta condici√≥n y otra que no la cumple. La varianza de la serie no debe ser una funci√≥n del tiempo. El siguiente gr√°fico representa una serie cuya varianza no esta afectada por el tiempo (es estacionaria) y otra que no cumple con esa condici√≥n. La covarianza de la serie no debe ser una funci√≥n del tiempo. En el gr√°fico de la derecha, se puede observar que la propagaci√≥n de la serie se va encogiendo a medida que aumenta el tiempo. Por lo tanto, la covarianza no es constante en el tiempo para la serie roja. ¬øPor qu√© son importantes las series de tiempo estacionarias?# La raz√≥n por la que estas series son importantes es que la mayor√≠a de los modelos de series de tiempo funcionan bajo el supuesto de que la serie es estacionaria. Intuitivamente, podemos suponer que si una serie tiene un comportamiento particular en el tiempo, hay una probabilidad muy alta de que se comportamiento contin√∫e en el futuro. Adem√°s, las teor√≠as relacionadas con las series estacionarias son m√°s maduras y m√°s f√°ciles de implementar en comparaci√≥n con series no estacionarias. A pesar de que el supuesto de que la serie es estacionaria se utiliza en muchos modelos, casi ninguna de las series de tiempo que encontramos en la pr√°ctica son estacionarias. Por tal motivo la estad√≠stica tuvo que desarrollar varias t√©cnicas para hacer estacionaria, o lo m√°s cercano posible a estacionaria, a una serie.\nSeries de tiempo con Python# Las principales librer√≠as que nos ofrece Python para trabajar con series de tiempo son:\nStatsmodels: Esta librer√≠a contiene muchos objetos y funciones de suma utilidad para el an√°lisis de series de tiempo. Algunos de los modelos que est√°n cubiertos por Statsmodels incluyen: el modelo autorregresivo (AR); el modelo autorregresivo de vectores (VAR); y el modelo autorregresivo de media m√≥vil (ARMA). Tambi√©n incluye funciones de estad√≠stica descriptiva de series de tiempo, como por ejemplo la autocorrelaci√≥n, as√≠ como las correspondientes propiedades te√≥ricas de ARMA o procesos relacionados. Por √∫ltimo, tambi√©n ofrece las pruebas estad√≠sticas relacionadas y algunas funciones auxiliares muy √∫tiles.\nPandas: Pandas proporciona un amplio soporte para trabajar con datos de series de tiempo. Generalmente cuando trabajamos con series de tiempo realizamos un amplio abanico de tareas, como: convertir fechas, estandarizar el tiempo de acuerdo a la zona horaria, crear secuencias a determinados intervalos o frecuencias, identificar datos faltantes, desplazar las fechas hacia atr√°s o hacia adelante por un determinado valor, calcular res√∫menes agregados de valores a medida que el tiempo cambia, etc. Pandas nos brinda las herramientas para poder realizar estas y muchas otras tareas en forma muy sencilla.\nVeamos algunos ejemplos de como podemos manipular y analizar series de tiempo con la ayuda de Python. En este caso, vamos a jugar un poco con la informaci√≥n de los precios de las acciones de Weatherford (WFT) de este a√±o.\nManipulando la serie de tiempo con pandas# Ver C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # importando modulos necesarios import numpy as np import pandas as pd import pandas.io.data as web import datetime as dt from pydataset import data import statsmodels.api as sm # librer√≠as de visualizaciones import seaborn as sns import matplotlib.pyplot as plt # graficos incrustados %matplotlib inline # pandas solo 4 decimales pd.set_option(\u0026#39;precision\u0026#39;, 4) # Ejemplo serie de tiempo con Pandas # Creando una serie de tiempo de las acciones de WFT desde yahoo finance wft = web.DataReader(\u0026#34;WFT\u0026#34;, \u0026#39;yahoo\u0026#39;, \u0026#39;2016-1-1\u0026#39;, \u0026#39;2016-9-30\u0026#39;) wft.head(5) Open High Low Close Volume Adj Close Date 2016-01-04 8.40 8.70 8.29 8.64 10719400 8.64 2016-01-05 8.67 8.80 8.13 8.26 9109100 8.26 2016-01-06 7.94 8.16 7.84 7.91 13203200 7.91 2016-01-07 7.69 7.83 7.34 7.34 12633800 7.34 2016-01-08 7.48 7.55 6.86 6.97 18547500 6.97 # filtrando s√≥lo del 2016-02-04 al 2016-02-18 wft[\u0026#39;2016-02-04\u0026#39;:\u0026#39;2016-02-18\u0026#39;] Open High Low Close Volume Adj Close Date 2016-02-04 7.10 7.82 6.99 7.39 34474500 7.39 2016-02-05 7.37 7.52 6.87 6.94 27775700 6.94 2016-02-08 6.68 6.79 6.41 6.74 17611300 6.74 2016-02-09 6.60 6.72 6.07 6.34 13741100 6.34 2016-02-10 6.28 6.59 6.11 6.24 8623900 6.24 2016-02-11 6.02 6.27 5.74 6.06 17133900 6.06 2016-02-12 6.14 6.66 6.06 6.47 13498600 6.47 2016-02-16 6.66 6.74 6.33 6.62 11453500 6.62 2016-02-17 6.70 7.13 6.55 6.72 29061300 6.72 2016-02-18 6.95 6.96 6.22 6.51 13587900 6.51 # valores al 2016-02-16 wft.loc[\u0026#39;2016-2-16\u0026#39;] Open 6.6600e+00 High 6.7400e+00 Low 6.3300e+00 Close 6.6200e+00 Volume 1.1454e+07 Adj Close 6.6200e+00 Name: 2016-02-16 00:00:00, dtype: float64 # valor de la columna Adj Close al 2016-2-16 wft[\u0026#39;Adj Close\u0026#39;][\u0026#39;2016-2-16\u0026#39;] 6.6200000000000001 # filtrando todo febrero de 2016 wft[\u0026#39;2016-2\u0026#39;] Open High Low Close Volume Adj Close Date 2016-02-01 6.51 6.67 5.90 6.33 36665900 6.33 2016-02-02 6.12 6.16 5.82 5.97 21091100 5.97 2016-02-03 6.04 6.40 5.60 6.27 24870400 6.27 2016-02-04 7.10 7.82 6.99 7.39 34474500 7.39 2016-02-05 7.37 7.52 6.87 6.94 27775700 6.94 2016-02-08 6.68 6.79 6.41 6.74 17611300 6.74 2016-02-09 6.60 6.72 6.07 6.34 13741100 6.34 2016-02-10 6.28 6.59 6.11 6.24 8623900 6.24 2016-02-11 6.02 6.27 5.74 6.06 17133900 6.06 2016-02-12 6.14 6.66 6.06 6.47 13498600 6.47 2016-02-16 6.66 6.74 6.33 6.62 11453500 6.62 2016-02-17 6.70 7.13 6.55 6.72 29061300 6.72 2016-02-18 6.95 6.96 6.22 6.51 13587900 6.51 2016-02-19 6.47 6.51 5.97 6.20 14541500 6.20 2016-02-22 6.20 6.92 6.20 6.75 11878300 6.75 2016-02-23 6.61 6.68 6.07 6.12 9486500 6.12 2016-02-24 5.93 6.14 5.77 6.08 8333800 6.08 2016-02-25 6.07 6.13 5.68 5.93 8972000 5.93 2016-02-26 6.13 6.55 6.07 6.43 12288100 6.43 2016-02-29 6.43 6.62 6.33 6.40 14120300 6.40 # Valores al cierre de cada mes. wft.asfreq(\u0026#39;M\u0026#39;, method=\u0026#39;ffill\u0026#39;) Open High Low Close Volume Adj Close Date 2016-01-31 6.26 6.77 6.20 6.74 17661000 6.74 2016-02-29 6.43 6.62 6.33 6.40 14120300 6.40 2016-03-31 7.62 7.86 7.55 7.78 13224600 7.78 2016-04-30 8.10 8.34 7.88 8.13 21137000 8.13 2016-05-31 5.61 5.74 5.55 5.61 8481400 5.61 2016-06-30 5.50 5.58 5.36 5.55 14896000 5.55 2016-07-31 5.64 5.81 5.59 5.68 19153500 5.68 2016-08-31 5.48 5.59 5.37 5.47 11293500 5.47 # Valores al cierre de cada mes (d√≠as laborales). wft.asfreq(\u0026#39;BM\u0026#39;) Open High Low Close Volume Adj Close Date 2016-01-29 6.26 6.77 6.20 6.74 17661000 6.74 2016-02-29 6.43 6.62 6.33 6.40 14120300 6.40 2016-03-31 7.62 7.86 7.55 7.78 13224600 7.78 2016-04-29 8.10 8.34 7.88 8.13 21137000 8.13 2016-05-31 5.61 5.74 5.55 5.61 8481400 5.61 2016-06-30 5.50 5.58 5.36 5.55 14896000 5.55 2016-07-29 5.64 5.81 5.59 5.68 19153500 5.68 2016-08-31 5.48 5.59 5.37 5.47 11293500 5.47 # valores al cierre de cada trimestre wft.asfreq(\u0026#39;BQ\u0026#39;) Open High Low Close Volume Adj Close Date 2016-03-31 7.62 7.86 7.55 7.78 13224600 7.78 2016-06-30 5.50 5.58 5.36 5.55 14896000 5.55 Desplazando los valores de la serie# Una operaci√≥n com√∫n en los datos de series de tiempo es desplazar los valores hacia atr√°s y adelante en el tiempo, como por ejemplo para calcular el cambio porcentual de una muestra a otra. En Pandas podemos utilizar el m√©todo .shift().\n# desplazando el 1 dia el valor de cierre desplazado = wft[\u0026#39;Adj Close\u0026#39;].shift(1) desplazado[:5] Date 2016-01-04 NaN 2016-01-05 8.64 2016-01-06 8.26 2016-01-07 7.91 2016-01-08 7.34 Name: Adj Close, dtype: float64 # calculando el porcentaje de variaci√≥n del d√≠a. variacion_diaria = wft[\u0026#39;Adj Close\u0026#39;] / wft[\u0026#39;Adj Close\u0026#39;].shift(1) - 1 wft[\u0026#39;var_diaria\u0026#39;] = variacion_diaria wft[\u0026#39;var_diaria\u0026#39;][:5] Date 2016-01-04 NaN 2016-01-05 -0.0440 2016-01-06 -0.0424 2016-01-07 -0.0721 2016-01-08 -0.0504 Name: var_diaria, dtype: float64 # mismo resultado utilizando pct_change() wft[\u0026#39;Adj Close\u0026#39;].pct_change()[:5] Date 2016-01-04 NaN 2016-01-05 -0.0440 2016-01-06 -0.0424 2016-01-07 -0.0721 2016-01-08 -0.0504 Name: Adj Close, dtype: float64 # calculando rendimiento acumulado diario rendimiento_diario = (1 + wft[\u0026#39;Adj Close\u0026#39;].pct_change()).cumprod() wft[\u0026#39;rend_diario\u0026#39;] = rendimiento_diario wft[\u0026#39;rend_diario\u0026#39;][:5] Date 2016-01-04 NaN 2016-01-05 0.9560 2016-01-06 0.9155 2016-01-07 0.8495 2016-01-08 0.8067 Name: rend_diario, dtype: float64 Visualizando las series de tiempo# Una operaci√≥n fundamental para entender el comportamiento de una serie de tiempo y poder determinar si se trata de una serie estacionaria o no; es realizar gr√°ficos de la misma. En Pandas esto lo podemos realizar en forma muy sencilla con el m√©todo .plot().\n# graficando Adj Close plot = wft[\u0026#39;Adj Close\u0026#39;].plot(figsize=(10, 8)) # Aplicando el filtro Hodrick-Prescott para separar en tendencia y # componente ciclico. wft_ciclo, wft_tend = sm.tsa.filters.hpfilter(wft[\u0026#39;Adj Close\u0026#39;]) wft[\u0026#39;tend\u0026#39;] = wft_tend # graficando la variacion del precio real con la tendencia. wft[[\u0026#39;Adj Close\u0026#39;, \u0026#39;tend\u0026#39;]].plot(figsize=(10, 8), fontsize=12); legend = plt.legend() legend.prop.set_size(14); # graficando rendimiento diario plot = wft[\u0026#39;var_diaria\u0026#39;].plot(figsize=(10, 8)) Promedios m√≥viles y descomposici√≥n# Pandas tambi√©n nos ofrece una serie de funciones para calcular estad√≠sticas m√≥viles, en ellas la funci√≥n estad√≠stica se calcula sobre una ventana de datos representados por un determinado per√≠odo de tiempo y luego se desplaza la ventana de datos por un intervalo especificado, calculando continuamente la estad√≠stica, siempre y cuando la ventana este dentro de las fechas de la serie de tiempo. El ejemplo m√°s utilizado es el de media m√≥vil, que se usa com√∫nmente en el an√°lisis de series de tiempo financieras para suavizar las fluctuaciones a corto plazo y poner de relieve las tendencias a largo plazo en los datos.\nOtra t√©cnica interesante que podemos intentar tambi√©n es la descomposici√≥n. Esta es una t√©cnica que trata de descomponer una serie de tiempo en su tendencia, su estacionalidad y sus factores residuales. Statsmodels viene con una funci√≥n de descomposici√≥n que nos facilita en sobremanera el trabajo. Veamos unos ejemplos.\n# Calculando promedios m√≥viles cada 5 d√≠as wft_ma = pd.rolling_mean(wft[\u0026#39;Adj Close\u0026#39;], 5) wft[\u0026#39;prod_mov\u0026#39;] = wft_ma plot = wft[[\u0026#39;Adj Close\u0026#39;, \u0026#39;prod_mov\u0026#39;]].plot(figsize=(10, 8), fontsize=12) # Ejemplo de descomposici√≥n de serie de tiempo descomposicion = sm.tsa.seasonal_decompose(wft[\u0026#39;Adj Close\u0026#39;], model=\u0026#39;additive\u0026#39;, freq=30) fig = descomposicion.plot() Pronosticando la serie con ARIMA# Como podemos observar en los gr√°ficos que realizamos anteriormente, el comportamiento de la serie de tiempo con la que estamos trabajando parece ser totalmente aleatorio y las medidas m√≥viles que calculamos tampoco parecen ser de mucha utilidad para acercar la serie a un comportamiento estacionario. De todas formas podemos intentar aplicar un modelo ARIMA sobre la serie y ver que tan bien nos va con el pronostico del modelo. El modelo ARIMA es similar a una regresi√≥n estad√≠stica pero aplicando los conceptos de las series de tiempo; por tanto, los pron√≥sticos del modelo vienen explicadas por los datos del pasado y no por variables independientes.\n# Modelo ARIMA sobre el valor de cierre de la acci√≥n. modelo = sm.tsa.ARIMA(wft[\u0026#39;Adj Close\u0026#39;].iloc[1:], order=(1, 0, 0)) resultados = modelo.fit(disp=-1) wft[\u0026#39;pronostico\u0026#39;] = resultados.fittedvalues plot = wft[[\u0026#39;Adj Close\u0026#39;, \u0026#39;pronostico\u0026#39;]].plot(figsize=(10, 8)) Aqu√≠ el modelo parece ser bastante efectivo, las l√≠neas en el gr√°fico son muy similares. Pero para armar el modelo hemos utilizado el valor de cierre de la acci√≥n, y la variaci√≥n de precio en el d√≠a a d√≠a es muy peque√±a en comparaci√≥n al precio absoluto. Lo que realmente nos interesa predecir es la variaci√≥n diaria del precio de la acci√≥n, por lo tanto deber√≠amos armar el modelo utilizando la columna de variaci√≥n diaria que calculamos previamente.\n# modelo ARIMA sobre variaci√≥n diaria modelo = sm.tsa.ARIMA(wft[\u0026#39;var_diaria\u0026#39;].iloc[1:], order=(1, 0, 0)) resultados = modelo.fit(disp=-1) wft[\u0026#39;pronostico\u0026#39;] = resultados.fittedvalues plot = wft[[\u0026#39;var_diaria\u0026#39;, \u0026#39;pronostico\u0026#39;]].plot(figsize=(10, 8)) En este gr√°fico podemos ver que es bastante obvio que el pron√≥stico esta muy lejos. Nuestro modelo predice variaciones muy peque√±as en comparaci√≥n con lo que ocurre en la realidad del d√≠a a d√≠a. Este era un resultado esperado ya que solo aplicamos un modelo sencillo de promedios m√≥viles a una serie no estacionaria; despu√©s de todo, si fuera tan f√°cil predecir el movimiento del mercado, todos ser√≠amos millonarios!. No hay suficiente informaci√≥n en los d√≠as anteriores para poder predecir con exactitud lo que va a pasar al d√≠a siguiente.\nAqu√≠ concluye este paseo por el mundo de las series de tiempo; como vimos, Pandas y Statsmodels pueden ser de mucha ayuda para trabajar con ellas. Espero que lo hayan encontrado √∫til.\nSaludos!\nEste post fue escrito utilizando IPython notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2016-09-26","id":15,"permalink":"/blog/2016/09/26/series-de-tiempo-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Los datos obtenidos a partir de observaciones recogidas a lo largo del tiempo son extremadamente comunes. En los negocios, observamos las tasas de inter√©s de la semana, los precios de cierre de las acciones diarios, los √≠ndices de precios mensuales, las cifras de ventas anuales, y as√≠ sucesivamente.","tags":["python","matematica","estadistica","finanazas","analisis de datos","series de tiempo"],"title":"Series de tiempo con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Las visualizaciones son una herramienta fundamental para entender y compartir ideas sobre los datos. La visualizaci√≥n correcta puede ayudar a expresar una idea central, o abrir un espacio para una m√°s profunda investigaci√≥n; con ella se puede conseguir que todo el mundo hable sobre un conjunto de datos, o compartir una visi√≥n sobre lo que los datos nos quieren decir.\nUna buena visualizaci√≥n puede dar a quien la observa un sentido rico y amplio de un conjunto de datos. Puede comunicar los datos de manera precisa a la vez que expone los lugares en d√≥nde se necesita m√°s informaci√≥n o d√≥nde una hip√≥tesis no se sostiene. Por otra parte, la visualizaci√≥n nos proporciona un lienzo para aplicar nuestras propias ideas, experiencias y conocimientos cuando observamos y analizamos datos, permitiendo realizar m√∫ltiples interpretaciones. Si como dice el dicho \u0026ldquo;una imagen vale m√°s que mil palabras\u0026rdquo;, un gr√°fico interactivo bien elegido entonces podr√≠a valer cientos de pruebas estad√≠sticas.\nLibrer√≠as para visualizar datos en Python# Como bien sabemos, la comunidad de Python es muy grande, por lo tanto vamos a poder encontrar un gran n√∫mero de librer√≠as para visualizar datos. Al tener tanta variedad de opciones, a veces se hace realmente dif√≠cil determinar cuando utilizar cada una de ellas. En este art√≠culo yo voy a presentar solo cuatro que creo que cubren un gran abanico de casos:\nMatplotlib: Que es la m√°s antigua y se convirti√≥ en la librer√≠a por defecto para visualizaciones de datos; muchas otras est√°n basadas en ella. Es extremadamente potente, pero con ese poder viene aparejada la complejidad. Se puede hacer pr√°cticamente de todo con Matplotlib pero no siempre es tan f√°cil de averiguar como hacerlo. Los que siguen el blog me habr√°n visto utilizarla en varios art√≠culos.\nBokeh: Una de las m√°s j√≥venes librer√≠as de visualizaciones, pero no por ello menos potente. Bokeh es una librer√≠a para visualizaciones interactivas dise√±ada para funcionar en los navegadores web modernos. Su objetivo es proporcionar una construcci√≥n elegante y concisa de gr√°ficos modernos al estilo de D3.js, y para ampliar esta capacidad con la interactividad y buen rendimiento sobre grandes vol√∫menes de datos. Bokeh puede ayudar a cualquier persona a crear en forma r√°pida y sencilla gr√°ficos interactivos, dashboards y aplicaciones de datos. Puede crear tanto gr√°ficos est√°ticos como gr√°ficos interactivos en el servidor de Bokeh.\nSeaborn: Si de gr√°ficos estad√≠sticos se trata, Seaborn es la librer√≠a que deber√≠amos utilizar, con ella podemos crear gr√°ficos estad√≠sticos informativos y atractivos de forma muy sencilla. Es una de las tantas librer√≠as que se basan en Matplotlib pero nos ofrece varias caracter√≠sticas interesantes tales como temas, paletas de colores, funciones y herramientas para visualizar distribuciones de una o varias variables aleatorias, regresiones lineales, series de tiempo, entre muchas otras. Con ella podemos construir visualizaciones complejas en forma sencilla.\nFolium: Si lo que necesitamos es visualizar datos de geolocalizaci√≥n en mapas interactivos, entonces Folium es una muy buena opci√≥n. Esta librer√≠a de Python es una herramienta sumamente poderosa para realizar mapas al estilo leaflet.js. El hecho de que los resultados de Folium son interactivos hace que esta librer√≠a sea √∫til para la construcci√≥n de dashboards.\n¬øC√≥mo elegir la visualizaci√≥n adecuada?# Una de las primeras preguntas que nos debemos realizar al explorar datos es ¬øqu√© m√©todo de visualizaci√≥n es m√°s efectivo?. Para intentar responder esta pregunta podemos utilizar la siguiente gu√≠a:\nComo podemos ver, la gu√≠a se divide en cuatro categor√≠as principales y luego se clasifican los distintos m√©todos de visualizaci√≥n que mejor representan cada una de esas categor√≠as. Veamos un poco m√°s en detalle cada una de ellas:\nDistribuciones: En esta categor√≠a intentamos comprender como los datos se distribuyen. Se suelen utilizar en el comienzo de la etapa de exploraci√≥n de datos, cuando queremos comprender las variables. Aqu√≠ tambi√©n nos vamos a encontrar con variables de dos tipos cuantitativas y categ√≥ricas. Dependiendo del tipo y cantidad de variables, el m√©todo de visualizaci√≥n que vamos a utilizar.\nComparaciones: En esta categor√≠a el objetivo es comparar valores a trav√©s de diferentes categor√≠as y con el tiempo (tendencia). Los tipos de gr√°ficos m√°s comunes en esta categor√≠a son los diagramas de barras para cuando estamos comparando elementos o categor√≠as y los diagramas de puntos y l√≠neas cuando comparamos variables cuantitativas.\nRelaciones: Aqu√≠ el objetivo es comprender la relaci√≥n entre dos o m√°s variables. La visualizaci√≥n m√°s utilizada en esta categor√≠a es el gr√°fico de dispersi√≥n.\nComposiciones: En esta categor√≠a el objetivo es comprender como esta compuesta o distribuida una variable; ya sea a trav√©s del tiempo o en forma est√°tica. Las visualizaciones m√°s comunes aqu√≠ son los diagramas de barras y los gr√°ficos de tortas.\nEjemplos en Python# Luego de esta introducci√≥n es hora de ensuciarse las manos y ponerse a jugar con algunos ejemplos en el uso de cada una de estas 4 librer√≠as que nos ofrece Python para visualizaci√≥n de datos. Obviamente los ejemplos van a ser sencillos ya que un tutorial exhaustivo sobre cada herramienta requerir√≠a mucho m√°s espacio.\nMatplotlib# Comencemos con Matplotlib; como les comentaba, es tal vez la librer√≠a m√°s utilizada para gr√°ficos en 2d. El objeto pyplot nos proporciona la interfase principal sobre la que podemos crear las visualizaciones de datos con esta librer√≠a.\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # importando modulos necesarios import numpy as np import pandas as pd #from pydataset import data import re # librer√≠as de visualizaciones import seaborn as sns import matplotlib.pyplot as plt from bokeh.io import output_notebook, show from bokeh.plotting import Histogram, Scatter import folium # graficos incrustados %matplotlib inline output_notebook() # Cargamos algunos datasets de ejemplo iris = data(\u0026#39;iris\u0026#39;) tips = data(\u0026#39;tips\u0026#39;) # Ejemplo matplotlib # graficanco funciones seno y coseno X = np.linspace(-np.pi, np.pi, 256, endpoint=True) C, S = np.cos(X), np.sin(X) # configurando el tama√±o de la figura plt.figure(figsize=(8, 6)) # dibujando las curvas plt.plot(X, C, color=\u0026#34;blue\u0026#34;, linewidth=2.5, linestyle=\u0026#34;-\u0026#34;, label=\u0026#34;coseno\u0026#34;) plt.plot(X, S, color=\u0026#34;red\u0026#34;, linewidth=2.5, linestyle=\u0026#34;-\u0026#34;, label=\u0026#34;seno\u0026#34;) # personalizando los valores de los ejes plt.xticks([-np.pi, -np.pi/2, 0, np.pi/2, np.pi], [r\u0026#39;$-\\pi$\u0026#39;, r\u0026#39;$-\\pi/2$\u0026#39;, r\u0026#39;$0$\u0026#39;, r\u0026#39;$+\\pi/2$\u0026#39;, r\u0026#39;$+\\pi$\u0026#39;]) plt.yticks([-1, 0, +1], [r\u0026#39;$-1$\u0026#39;, r\u0026#39;$0$\u0026#39;, r\u0026#39;$+1$\u0026#39;]) # agregando la leyenda plt.legend(loc=\u0026#39;upper left\u0026#39;) # moviendo los ejes de coordenadas ax = plt.gca() # get current axis ax.spines[\u0026#39;right\u0026#39;].set_color(\u0026#39;none\u0026#39;) ax.spines[\u0026#39;top\u0026#39;].set_color(\u0026#39;none\u0026#39;) ax.xaxis.set_ticks_position(\u0026#39;bottom\u0026#39;) ax.spines[\u0026#39;bottom\u0026#39;].set_position((\u0026#39;data\u0026#39;,0)) ax.yaxis.set_ticks_position(\u0026#39;left\u0026#39;) ax.spines[\u0026#39;left\u0026#39;].set_position((\u0026#39;data\u0026#39;,0)) # mostrando el resultado plt.show() ![png](/(DataViz_files/DataViz_6_0.png)\nEn este primer ejemplo vemos como podemos acceder a la API de Matplotlib desde el objeto pyplot e ir dando forma al gr√°fico. Veamos ahora unos ejemplos con el dataset iris.\n# Ejemplo con iris # histograma de Petal.Length iris.head() Sepal.Length Sepal.Width Petal.Length Petal.Width Species 1 5.1 3.5 1.4 0.2 setosa 2 4.9 3.0 1.4 0.2 setosa 3 4.7 3.2 1.3 0.2 setosa 4 4.6 3.1 1.5 0.2 setosa 5 5.0 3.6 1.4 0.2 setosa # separo en especies setosa = iris[iris.Species == \u0026#39;setosa\u0026#39;] versicolor = iris[iris.Species == \u0026#39;versicolor\u0026#39;] virginica = iris[iris.Species == \u0026#39;virginica\u0026#39;] # crear histograma plt.figure(figsize=(10, 8)) n, bins, patches = plt.hist(setosa[\u0026#39;Petal.Length\u0026#39;], 12, facecolor=\u0026#39;red\u0026#39;, label=\u0026#39;setosa\u0026#39;) n, bins, patches = plt.hist(versicolor[\u0026#39;Petal.Length\u0026#39;], 12, facecolor=\u0026#39;green\u0026#39;, label=\u0026#39;versicolor\u0026#39;) n, bins, patches = plt.hist(virginica[\u0026#39;Petal.Length\u0026#39;], 12, facecolor=\u0026#39;blue\u0026#39;, label=\u0026#39;virginica\u0026#39;) plt.legend(loc=\u0026#39;top_right\u0026#39;) plt.title(\u0026#39;Histograma largo del p√©talo\u0026#39;) plt.xlabel(\u0026#39;largo del p√©talo\u0026#39;) plt.ylabel(\u0026#39;cuenta largo del p√©talo\u0026#39;) plt.show() ![png](/(DataViz_files/DataViz_10_0.png)\n# Ejemplo diagrama de dispersion entre Petal.Length y Petal.Width plt.figure(figsize=(10, 8)) plt.scatter(setosa[\u0026#39;Petal.Length\u0026#39;], setosa[\u0026#39;Petal.Width\u0026#39;], c=\u0026#39;red\u0026#39;, label=\u0026#39;setosa\u0026#39;) plt.scatter(versicolor[\u0026#39;Petal.Length\u0026#39;], versicolor[\u0026#39;Petal.Width\u0026#39;], c=\u0026#39;green\u0026#39;, label=\u0026#39;versicolor\u0026#39;) plt.scatter(virginica[\u0026#39;Petal.Length\u0026#39;], virginica[\u0026#39;Petal.Width\u0026#39;], c=\u0026#39;blue\u0026#39;, label=\u0026#39;virginica\u0026#39;) plt.title(\u0026#39;Tama√±o del p√©talo\u0026#39;) plt.xlabel(\u0026#39;Largo del p√©talo (cm)\u0026#39;) plt.ylabel(\u0026#39;Ancho del p√©talo (cm)\u0026#39;) plt.legend(loc=\u0026#39;top_left\u0026#39;) plt.show() ![png](/(DataViz_files/DataViz_11_0.png)\nBokeh# Bokeh adem√°s de generar unos hermosos gr√°ficos interactivos nos permite realizar gr√°ficos complejos en forma muy sencilla. La interfase de alto nivel con la que vamos a trabajar principalmente para generar visualizaciones con esta librer√≠a es bokeh.charts. Repitamos los ejemplos que realizamos anteriormente sobre el dataset iris y veamos que sencillo que es realizarlos con Bokeh.\n# Ejemplo de histograma de Petal.Length # solo 2 lineas de c√≥digo hist = Histogram(iris, values=\u0026#34;Petal.Length\u0026#34;, color=\u0026#34;Species\u0026#34;, legend=\u0026#34;top_right\u0026#34;, bins=12) show(hist) \u0026lt;div class=\u0026quot;bk-root\u0026quot;\u0026gt; \u0026lt;div class=\u0026quot;plotdiv\u0026quot; id=\u0026quot;c195cf80-93de-4663-b5d3-f106a1051aec\u0026quot;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; # Ejemplo diagrama de dispersion entre Petal.Length y Petal.Width # solo 2 lineas de c√≥digo disp = Scatter(iris, x=\u0026#39;Petal.Length\u0026#39;, y=\u0026#39;Petal.Width\u0026#39;, color=\u0026#39;Species\u0026#39;, legend=\u0026#39;top_left\u0026#39;, marker=\u0026#39;Species\u0026#39;, title=\u0026#34;Tama√±o del petalo\u0026#34;) show(disp) \u0026lt;div class=\u0026quot;bk-root\u0026quot;\u0026gt; \u0026lt;div class=\u0026quot;plotdiv\u0026quot; id=\u0026quot;ab950219-9790-4c01-8a06-67fa0b5f89e7\u0026quot;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; Seaborn# Seaborn tiene su √©nfasis en los gr√°ficos estad√≠sticos. Nos permite realizar f√°cilmente gr√°ficos de regresi√≥n y de las principales distribuciones; pero donde realmente brilla Seaborn es en su capacidad de visualizar muchas caracter√≠sticas diferentes a la vez. Veamos algunos ejemplos\n# Ejemplo gr√°fico de distribuciones x = np.random.normal(size=100) dist= sns.distplot(x) ![png](/(DataViz_files/DataViz_16_0.png)\n# Ejemplo gr√°fico regresi√≥n con tips dataset tips.head() total_bill tip sex smoker day time size 1 16.99 1.01 Female No Sun Dinner 2 2 10.34 1.66 Male No Sun Dinner 3 3 21.01 3.50 Male No Sun Dinner 3 4 23.68 3.31 Male No Sun Dinner 2 5 24.59 3.61 Female No Sun Dinner 4 reg = sns.regplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, data=tips) ![png](/(DataViz_files/DataViz_18_0.png)\n# Ejemplo pairplot con datase iris g = sns.pairplot(iris, hue=\u0026#34;Species\u0026#34;, diag_kind=\u0026#34;hist\u0026#34;) ![png](/(DataViz_files/DataViz_19_0.png)\n# Ejemplo FacetGrid con iris g = sns.FacetGrid(iris, col=\u0026#34;Species\u0026#34;) g = g.map(plt.scatter, \u0026#34;Petal.Length\u0026#34;, \u0026#34;Petal.Width\u0026#34;) ![png](/(DataViz_files/DataViz_20_0.png)\nFolium# Por √∫ltimo, veamos un ejemplo de como utilizar Folium. Ya que yo soy adepto al uso de la bicicleta para moverme por la ciudad, y muchas veces se hace dif√≠cil encontrar una bicicleter√≠a en donde poder encontrar repuestos o reparar la bicicleta; en este ejemplo vamos a crear un mapa interactivo del barrio de Palermo en donde vamos a marcar la ubicaci√≥n de los negocios de bicicleter√≠as. Esta informaci√≥n la podemos extraer del padr√≥n que ofrece el gobierno de la Ciudad de Buenos Aires en su portal de datos.\n# dataset de bicicleter√≠as de Ciudad de Buenos Aires # descargado desde https://data.buenosaires.gob.ar/dataset/bicicleterias bici = pd.read_csv(\u0026#39;data/bicicleterias.csv\u0026#39;, sep=\u0026#39;;\u0026#39;) bici.head() WKT ID NOMBRE DIRECCION TELEFONO EMAIL WEB 0 POINT (-58.466041249451614 -34.557060215984805) 52 11 A FONDO CONGRESO 2757 45421835 info@11afondo.com https://WWW.11AFONDO.COM 1 POINT (-58.41279876038783 -34.591915372813645) 32 AMERICAN BIKE AV. CNEL. DIAZ 1664 48220889 info@americanbike.com.ar https://WWW.AMERICANBIKE.COM.AR/ 2 POINT (-58.425646989945932 -34.580365554062418) 30 ANDINO BIKES GUEMES 4818 47753677 andino_bike@hotmail.com https://WWW.ANDINOBIKE.COM.AR/ 3 POINT (-58.437608880680997 -34.6045094278806) 107 BABE BIKES WARNES 10 48549862 info@babebikes.com.ar 4 POINT (-58.439598908303168 -34.58547499220991) 118 BELGRAVIA TAILOR MADE BICYLES BONPLAND 1459 1544291001 hola@belgravia.com.ar WWW.FACEBOOK.COM/BELGRAVIABIKES # corregimos el campo de coordenadas del dataset. def coord(c): coor = re.findall(r\u0026#39;-?\\d+\\.\\d{7}\u0026#39;, c) coords = [float(s) for s in coor] return coords[::-1] bici[\u0026#39;WKT\u0026#39;] = bici[\u0026#39;WKT\u0026#39;].apply(coord) # filtramos solo las bicicleter√≠as de palermo bici_palermo = bici[bici.BARRIO == \u0026#39;PALERMO\u0026#39;][[\u0026#39;WKT\u0026#39;, \u0026#39;NOMBRE\u0026#39;]] # creamos el mapa con folium mapa = folium.Map(location=[-34.588889, -58.430556], zoom_start=13) # agregamos los markers con el nombre de cada bicicleter√≠a. for index, row in bici_palermo.iterrows(): mapa.simple_marker(row[\u0026#39;WKT\u0026#39;], popup=row[\u0026#39;NOMBRE\u0026#39;], marker_color=\u0026#39;red\u0026#39;, marker_icon=\u0026#39;info-sign\u0026#39;) # visualizamos el mapa con los markers mapa Aqu√≠ concluye este art√≠culo, ya no hay excusas para graficar sus datos, como vimos Python cuenta con herramientas que son f√°ciles de usar y muy poderosas. A divertirse!\nSaludos!\nEste post fue escrito utilizando IPython notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2016-09-18","id":16,"permalink":"/blog/2016/09/18/visualizaciones-de-datos-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Las visualizaciones son una herramienta fundamental para entender y compartir ideas sobre los datos. La visualizaci√≥n correcta puede ayudar a expresar una idea central, o abrir un espacio para una m√°s profunda investigaci√≥n; con ella se puede conseguir que todo el mundo hable sobre un conjunto de datos, o compartir una visi√≥n sobre lo que los datos nos quieren decir.","tags":["python","programacion","analisis de datos","machine learning","probabilidad","estadistica","distribuciones","visualizaciones","seaborn","matplotlib"],"title":"Visualizaciones de datos con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, Analisis de datos y Python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Cuando trabajamos en problemas de Machine Learning, muchas veces nos vamos a encontrar con enormes conjuntos de datos, con cientos o miles de caracter√≠sticas o features. Una forma simple de reducir las dimensiones de estas caracter√≠sticas es aplicar alguna t√©cnica de Factorizaci√≥n de matrices. La Factorizaci√≥n de matrices tiene enormes aplicaciones en todo tipo de problemas relacionados a la inteligencia artificial, ya que la reducci√≥n de dimensionalidad es la esencia de la cognici√≥n.\nAsimismo, la Factorizaci√≥n de matrices es tambi√©n un tema unificador dentro del √°lgebra lineal num√©rica. Una amplia variedad de algoritmos se han desarrollado a lo largo de muchas d√©cadas, proporcionando una plataforma num√©rica para operaciones de matrices tales como, la resoluci√≥n de sistemas de ecuaciones lineales, la descomposici√≥n espectral, y la identificaci√≥n de subespacios vectoriales. Algunos de estos algoritmos tambi√©n han demostrado ser de utilidad en problemas de an√°lisis estad√≠stico de datos, como es el caso de la descomposici√≥n en valores singulares o SVD, por sus siglas en ingl√©s, que es la base del an√°lisis de componentes principales o PCA, que es una t√©cnica muy utilizada para reducir el tama√±o de los datos. Muchas investigaciones actuales en Machine Learning han centrados sus esfuerzos en el uso de la Factorizaci√≥n de matrices para mejorar el rendimiento de los sistemas de aprendizaje. Principalmente en el estudio de la factorizaci√≥n de matrices no negativas (NMF), la cual se centra en el an√°lisis de matrices de datos cuyos elementos son positivos (no negativos), una ocurrencia muy com√∫n en los conjuntos de datos derivados de textos e im√°genes.\n¬øQu√© es la factorizaci√≥n de matrices?# En matem√°ticas, la factorizaci√≥n es una t√©cnica que consiste en la descomposici√≥n de una expresi√≥n matem√°tica (que puede ser un n√∫mero, una matriz, un tensor, etc.) en forma de producto. Existen distintos m√©todos de factorizaci√≥n, dependiendo de los objetos matem√°ticos estudiados; el objetivo es simplificar una expresi√≥n o reescribirla en t√©rminos de ¬´bloques fundamentales¬ª, que reciben el nombre de factores. As√≠ por ejemplo, el n√∫mero 6 se puede descomponer en el producto de 3 y 2. Si extendemos este concepto al mundo de las matrices, entonces podemos decir que la Factorizaci√≥n de matrices consiste en encontrar dos o m√°s matrices de manera tal que cuando se multipliquen nos devuelvan la matriz original. Por ejemplo:\n$$\\left(\\begin{matrix}3 \u0026 4 \u0026 5\\\\6 \u0026 8 \u0026 10 \\end{matrix}\\right) = \\left(\\begin{matrix}1\\\\2 \\end{matrix}\\right) \\cdot \\left(\\begin{matrix}3 \u0026 4 \u0026 5 \\end{matrix}\\right) $$ Los m√©todos de Factorizaci√≥n de matrices han ganado popularidad √∫ltimamente al haber sido aplicados con √©xito en sistemas de recomendaci√≥n para descubrir las caracter√≠sticas latentes que subyacen a las interacciones entre dos tipos de entidades, como por ejemplo usuarios y pel√≠culas. El algoritmo que gan√≥ el desaf√≠o de Netflix fue un sistema basado en m√©todos de Factorizaci√≥n de matrices.\nFactorizaci√≥n de matrices en sistemas de ecuaciones lineales# Dos de las Factorizaci√≥n de matrices m√°s utilizadas y que tal vez mucha gente las haya escuchado nombrar alguna vez son la factorizaci√≥n LU y la factorizaci√≥n QR; las cuales se utilizan a menudo para resolver sistemas de ecuaciones lineales.\nFactorizaci√≥n LU# $$A = LU$$ En √°lgebra lineal, la factorizaci√≥n o descomposici√≥n LU (del ingl√©s Lower-Upper) es una forma de factorizaci√≥n de una matriz como el producto de una matriz triangular inferior y una superior. La factorizaci√≥n LU expresa el m√©todo de Gauss en forma matricial. As√≠ por ejemplo, tenemos que \\(PA = LU\\) donde \\(P\\) es una matriz de permutaci√≥n. Una condici√≥n suficiente para que exista la factorizaci√≥n es que la matriz \\(A\\) sea una matriz no singular.\nEn Python podemos encontrar la descomposici√≥n LU con la ayuda de SciPy de la siguiente forma:\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # importando modulos necesarios import matplotlib.pyplot as plt import numpy as np import scipy.sparse as sp import scipy.linalg as la import nimfa from sklearn.decomposition import NMF, PCA from sklearn.preprocessing import StandardScaler import seaborn as sns import pandas as pd # graficos incrustados %matplotlib inline # par√°metros de estilo sns.set(style=\u0026#39;darkgrid\u0026#39;, palette=\u0026#39;muted\u0026#39;) pd.set_option(\u0026#39;display.mpl_style\u0026#39;, \u0026#39;default\u0026#39;) pd.set_option(\u0026#39;display.notebook_repr_html\u0026#39;, True) plt.rcParams[\u0026#39;figure.figsize\u0026#39;] = 8, 6 # Ejemplo factorizaci√≥n LU A = np.array([[7, 3, -1, 2] ,[3, 8, 1, -4] ,[-1, 1, 4, -1] ,[2, -4, -1, 6]]) P, L, U = la.lu(A) # Matriz A A array([[ 7, 3, -1, 2], [ 3, 8, 1, -4], [-1, 1, 4, -1], [ 2, -4, -1, 6]]) # Matriz de permutaci√≥n P array([[ 1., 0., 0., 0.], [ 0., 1., 0., 0.], [ 0., 0., 1., 0.], [ 0., 0., 0., 1.]]) # Matriz triangular inferior L array([[ 1. , 0. , 0. , 0. ], [ 0.42857143, 1. , 0. , 0. ], [-0.14285714, 0.21276596, 1. , 0. ], [ 0.28571429, -0.72340426, 0.08982036, 1. ]]) # Matriz triangular superior U array([[ 7. , 3. , -1. , 2. ], [ 0. , 6.71428571, 1.42857143, -4.85714286], [ 0. , 0. , 3.55319149, 0.31914894], [ 0. , 0. , 0. , 1.88622754]]) # A = LU L @ U array([[ 7., 3., -1., 2.], [ 3., 8., 1., -4.], [-1., 1., 4., -1.], [ 2., -4., -1., 6.]]) Factorizaci√≥n QR# $$A = QR$$ La descomposici√≥n o factorizaci√≥n QR consiste en la descomposici√≥n de una matriz como producto de una matriz ortogonal (\\(Q^T \\cdot Q = I\\)) por una matriz triangular superior. la factorizaci√≥n QR es ampliamente utilizada en las finanzas cuantitativas como base para la soluci√≥n del problema de los m√≠nimos cuadrados lineales, que a su vez se utiliza para el an√°lisis de regresi√≥n estad√≠stica.\nEn Python podemos encontrar la descomposici√≥n QR con la ayuda de SciPy de la siguiente forma:\n# Ejemplo factorizaci√≥n QR A = np.array([[12, -51, 4], [6, 167, -68], [-4, 24, -41]]) Q, R = la.qr(A) # Matriz A A array([[ 12, -51, 4], [ 6, 167, -68], [ -4, 24, -41]]) # Matriz ortogonal Q Q array([[-0.85714286, 0.39428571, 0.33142857], [-0.42857143, -0.90285714, -0.03428571], [ 0.28571429, -0.17142857, 0.94285714]]) # Matriz triangular superior R R array([[ -14., -21., 14.], [ 0., -175., 70.], [ 0., 0., -35.]]) # A = QR Q @ R array([[ 12., -51., 4.], [ 6., 167., -68.], [ -4., 24., -41.]]) Matrices dispersas y no negativas# En muchas ocasiones cuando trabajamos con matrices para representar el mundo f√≠sico, nos vamos a encontrar con que muchas de estas matrices est√°n dominadas por mayor√≠a de elementos que son cero. Este tipo de matrices son llamadas matrices dispersas, es decir, matrices de gran tama√±o en que la mayor parte de sus elementos son cero. Como ser√≠a ineficiente almacenar en la memoria de la computadora todos los elementos en cero, en las matrices dispersas solo vamos a almacenar los valores que no son cero y alguna informaci√≥n adicional acerca de su ubicaci√≥n.\nAsimismo muchos datos del mundo real son no negativos y los componentes ocultos correspondientes tienen un sentido f√≠sico solamente cuando son no negativos. En la pr√°ctica, ambas caracter√≠sticas, ser dispersas y no negativas son a menudo deseable o necesario cuando los componentes subyacentes tienen una interpretaci√≥n f√≠sica. Por ejemplo, en el procesamiento de im√°genes y visi√≥n artificial, las variables involucradas y los par√°metros pueden corresponder a p√≠xeles, y la factorizaci√≥n de matrices dispersas y no negativas est√° relacionada con la extracci√≥n de las partes m√°s relevantes de las im√°genes. La representaci√≥n dispersa de los datos por un n√∫mero limitado de componentes es un problema importante en la investigaci√≥n. En Machine Learning, las matrices dispersas est√° estrechamente relacionada con la selecci√≥n de atributos y ciertas generalizaciones en algoritmos de aprendizaje; mientras que la no negatividad se relaciona a las distribuciones de probabilidad.\nEn Python, podemos representar a las matrices dispersas con la ayuda de scipy.sparse.\n# Ejemplo matriz dispersa con scipy A = sp.diags([1, -2, 1], [1, 0, -1], shape=[10, 10], format=\u0026#39;csc\u0026#39;) A \u0026lt;10x10 sparse matrix of type '\u0026lt;class 'numpy.float64'\u0026gt;' with 28 stored elements in Compressed Sparse Column format\u0026gt; A.data array([-2., 1., 1., -2., 1., 1., -2., 1., 1., -2., 1., 1., -2., 1., 1., -2., 1., 1., -2., 1., 1., -2., 1., 1., -2., 1., 1., -2.]) A.indices array([0, 1, 0, 1, 2, 1, 2, 3, 2, 3, 4, 3, 4, 5, 4, 5, 6, 5, 6, 7, 6, 7, 8, 7, 8, 9, 8, 9], dtype=int32) A.indptr array([ 0, 2, 5, 8, 11, 14, 17, 20, 23, 26, 28], dtype=int32) A.todense() matrix([[-2., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [ 1., -2., 1., 0., 0., 0., 0., 0., 0., 0.], [ 0., 1., -2., 1., 0., 0., 0., 0., 0., 0.], [ 0., 0., 1., -2., 1., 0., 0., 0., 0., 0.], [ 0., 0., 0., 1., -2., 1., 0., 0., 0., 0.], [ 0., 0., 0., 0., 1., -2., 1., 0., 0., 0.], [ 0., 0., 0., 0., 0., 1., -2., 1., 0., 0.], [ 0., 0., 0., 0., 0., 0., 1., -2., 1., 0.], [ 0., 0., 0., 0., 0., 0., 0., 1., -2., 1.], [ 0., 0., 0., 0., 0., 0., 0., 0., 1., -2.]]) La descomposici√≥n en valores singulares (SVD) y el an√°lisis de componentes principales (PCA)# El SVD y el PCA son herramientas ampliamente utilizadas, por ejemplo, en el an√°lisis de im√°genes m√©dicas para la reducci√≥n de dimensionalidad, la construcci√≥n de modelos, y la comprensi√≥n y exploraci√≥n de datos. Tienen aplicaciones en pr√°cticamente todas las √°reas de la ciencia, machine learning, procesamiento de im√°genes, ingenier√≠a, gen√©tica, computaci√≥n cognitiva, qu√≠mica, meteorolog√≠a, y redes neuronales, s√≥lo por nombrar algunas; en d√≥nde nos encontramos con grandes conjuntos de datos. El prop√≥sito del an√°lisis de componentes principales PCA es derivar un n√∫mero relativamente peque√±o de combinaciones lineales no correlacionadas (componentes principales) de una conjunto de variables aleatorias de media cero mientras que conserva la mayor cantidad de informaci√≥n de las variables originales como sea posible. Entre los objetivos del PCA podemos encontrar los siguientes:\nReducci√≥n de dimensionalidad. Determinaci√≥n de combinaciones lineales de variables. Selecci√≥n de caracter√≠sticas o features: la elecci√≥n de las variables m√°s √∫tiles. Visualizaci√≥n de datos multidimensionales. Identificaci√≥n de las variables subyacentes. Identificaci√≥n grupos de objetos o de valores at√≠picos. Veamos algunos ejemplos con Python\n# Ejemplo SVD con scipy.linalg.svd # Matriz A a factorizar A = np.array([[2, 4] ,[1, 3] ,[0, 0] ,[0, 0]]) A array([[2, 4], [1, 3], [0, 0], [0, 0]]) # Factorizaci√≥n con svd # svd factoriza la matriz A en dos matrices unitarias U y Vh, y una # matriz s de valores singulares (reales, no negativo) de tal manera que # A == U * S * Vh, donde S es una matriz con s como principal diagonal y ceros U, s, Vh = la.svd(A) U.shape, Vh.shape, s.shape ((4, 4), (2, 2), (2,)) # Matriz unitaria U array([[-0.81741556, -0.57604844, 0. , 0. ], [-0.57604844, 0.81741556, 0. , 0. ], [ 0. , 0. , 1. , 0. ], [ 0. , 0. , 0. , 1. ]]) # Valores singulares s array([ 5.4649857 , 0.36596619]) # Matriz unitaria Vh array([[-0.40455358, -0.9145143 ], [-0.9145143 , 0.40455358]]) # Generando S S = la.diagsvd(s, 4, 2) S array([[ 5.4649857 , 0. ], [ 0. , 0.36596619], [ 0. , 0. ], [ 0. , 0. ]]) # Reconstruyendo la Matriz A. U @ S @ Vh array([[ 2., 4.], [ 1., 3.], [ 0., 0.], [ 0., 0.]]) Ejemplo de SVD y PCA con el dataset Iris# El dataset iris contiene mediciones de 150 flores de iris de tres especies diferentes.\nLas tres clases en el dataset son:\nsetosa (n = 50). versicolor (n = 50). virginica (n = 50). Las cuales est√°n representadas por cuatro caracter√≠sticas:\nlongitud en cm del s√©palo. ancho en cm del s√©palo. longitud en cm del p√©talo. ancho en cm del p√©talo. # Ejemplo svd con iris dataset iris = sns.load_dataset(\u0026#34;iris\u0026#34;) print(iris.shape) iris.head() (150, 5) sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa # Aplicando svd U, s, Vh = sp.linalg.svds((iris - iris.mean()).iloc[:,:-1],2) # Creando los componentes principales pc = U @ np.diag(s) pc = pc[:,::-1] # graficando el dataset reducido a 2 componenetes iris_svd = pd.concat((pd.DataFrame(pc, index=iris.index , columns=(\u0026#39;c0\u0026#39;,\u0026#39;c1\u0026#39;)), iris.loc[:,\u0026#39;species\u0026#39;]),1) g = sns.lmplot(\u0026#39;c0\u0026#39;, \u0026#39;c1\u0026#39;, iris_svd, hue=\u0026#39;species\u0026#39;, fit_reg=False, size=8 ,scatter_kws={\u0026#39;alpha\u0026#39;:0.7,\u0026#39;s\u0026#39;:60}) # Ejemplo de PCA con Scikit-Learn e Iris dataset # Divido el dataset en datos y clases X = iris.ix[:,0:4].values y = iris.ix[:,4].values # Estandarizo los datos X_std = StandardScaler().fit_transform(X) pca = PCA(n_components=2) Y_pca = pca.fit_transform(X_std) # Visualizo el resultado for lab, col in zip((\u0026#39;setosa\u0026#39;, \u0026#39;versicolor\u0026#39;, \u0026#39;virginica\u0026#39;), (\u0026#39;blue\u0026#39;, \u0026#39;red\u0026#39;, \u0026#39;green\u0026#39;)): plt.scatter(Y_pca[y==lab, 0], Y_pca[y==lab, 1], label=lab, c=col) plt.xlabel(\u0026#39;Componente 1\u0026#39;) plt.ylabel(\u0026#39;Componente 2\u0026#39;) plt.legend(loc=\u0026#39;lower center\u0026#39;) plt.tight_layout() plt.title(\u0026#39;Ejemplo PCA\u0026#39;) plt.show() Factorizaci√≥n de matrices en sistemas de recomendaci√≥n# En un sistemas de recomendaci√≥n como Netflix o MovieLens, hay un grupo de usuarios y un conjunto de elementos (pel√≠culas en los dos sistemas anteriores). Teniendo en cuenta que cada usuario ha valorado algunos elementos en el sistema, nos gustar√≠a predecir c√≥mo los usuarios calificar√≠an los art√≠culos que a√∫n no se han valorado, de tal manera que podemos hacer recomendaciones a los usuarios. En este caso, toda la informaci√≥n que tenemos sobre las calificaciones existentes pueden ser representados en una matriz. Supongamos ahora que tenemos 5 usuarios y 4 pel√≠culas y las calificaciones son n√∫meros enteros de 1 a 5, la matriz resultante puede ser algo como la siguiente (el gui√≥n significa que el usuario a√∫n no ha calificado la pel√≠cula):\nP1 P2 P3 P4 U1 5 3 - 1 U2 4 - - 1 U3 1 1 - 5 U4 1 - - 4 U5 - 1 5 4 Por lo tanto, la tarea de predecir las calificaciones que faltan se puede considerar como un problema de llenar los espacios en blanco (los guiones en la matriz) de tal manera que los valores resultantes ser√≠an consistentes con las calificaciones existentes en la matriz.\nLa intuici√≥n detr√°s de usar Factorizaci√≥n de matrices para resolver este problema es que deber√≠an existir algunas caracter√≠sticas latentes que determinen c√≥mo un usuario califica una pel√≠cula. Por ejemplo, dos usuarios dar√≠an una alta calificaci√≥n a una cierta pel√≠cula si a ambos les gusta los actores / actrices de la pel√≠cula, o si la pel√≠cula es una pel√≠cula de acci√≥n, que es un g√©nero preferido por ambos usuarios. Por lo tanto, si podemos descubrir estas caracter√≠sticas latentes, deber√≠amos ser capaces de predecir una calificaci√≥n con respecto a un determinado usuario y una cierta pel√≠cula, porque las caracter√≠sticas asociadas con el usuario deben coincidir con las caracter√≠sticas asociadas con la pel√≠cula.\nAl tratar de descubrir las diferentes caracter√≠sticas latentes, estamos suponiendo impl√≠citamente que el n√∫mero de caracter√≠sticas va a ser menor que el n√∫mero de usuarios y el n√∫mero de elementos. No deber√≠a ser dif√≠cil de entender este supuesto ya que no ser√≠a razonable que cada usuario est√° asociado con una caracter√≠stica √∫nica (aunque esto no es imposible). Y de todos modos, si este es el caso, no tendr√≠a sentido hacer recomendaciones, porque ninguno de estos usuarios estar√≠an interesados en los art√≠culos calificados por otros usuarios.\nSi llevamos este ejemplo sencillo a Python, podr√≠amos modelarlo utilizando Scikit-learn NMF o Nimfa.\n# Ejemplo en python # Matriz de ratings de los usuarios R = np.array([[5, 3, 0, 1] ,[4, 0, 0, 1] ,[1, 1, 0, 5] ,[1, 0, 0, 4] ,[0, 1, 5, 4]]) # Armado del modelo modelo = NMF(n_components= 2, init=\u0026#39;random\u0026#39;,random_state=1982, alpha=0.0002, beta=0.02, max_iter=5000) U = modelo.fit_transform(R) V = modelo.components_ # Reconstrucci√≥n de la matriz U @ V array([[ 5.25534386, 1.99295604, 0. , 1.45508737], [ 3.50398031, 1.32879578, 0. , 0.97017391], [ 1.31288534, 0.94413861, 1.9495384 , 3.94596646], [ 0.98125417, 0.7217855 , 1.52757221, 3.07874315], [ 0. , 0.65011225, 2.84008987, 5.21892884]]) # Error del modelo modelo.reconstruction_err_ 4.276529876124528 # Ejemplo utilizando nimfa snmf = nimfa.Snmf(R, seed=\u0026#34;random_vcol\u0026#34;, rank=2, max_iter=30, version=\u0026#39;r\u0026#39;, eta=1., beta=1e-4, i_conv=10, w_min_change=0) # Armando el modelo fit = snmf() U = fit.basis() V = fit.coef() # Reconstruyendo la matriz np.around(U @ V, decimals=2) array([[ 5.18, 1.96, 0. , 1.44], [ 3.45, 1.31, 0. , 0.96], [ 1.32, 0.93, 1.91, 3.87], [ 0.98, 0.71, 1.5 , 3.02], [ 0. , 0.63, 2.79, 5.11]]) Librer√≠as de Python para factorizaci√≥n de matrices# Para concluir, podemos enumerar algunas de las librer√≠as de Python que nos pueden ser de mucha utilidad a la hora de trabajar con Factorizaci√≥n de matrices, como ser:\nNumPy: El popular paquete matem√°tico de Python, nos va a permitir crear vectores, matrices y tensores; y poder manipularlos y realizar operaciones sobre ellos con mucha facilidad. SciPy: El paquete que agrupa un gran n√∫mero de herramientas cient√≠ficas en Python. Nos va a permitir crear matrices dispersas y realizar factorizaciones con facilidad. Scikit-learn: Es una librer√≠a especializada en algoritmos para data mining y machine learning. Principalmente el subm√≥dulo de Descomposiciones en d√≥nde vamos a encontrar las herramientas para reducciones de dimensionalidad. SymPy: Esta librer√≠a nos permite trabajar con matem√°tica simb√≥lica, convierte a Python en un sistema algebraico computacional. Nos va a permitir trabajar con las factorizaciones en forma anal√≠tica. Nimfa: Nimfa es una librer√≠a para factorizaci√≥n de matrices no negativas. Incluye las implementaciones de varios m√©todos de factorizaci√≥n, enfoques de inicializaci√≥n y calificaci√≥n de calidad. Soporta representaciones tanto de matrices densas como dispersas. Se distribuye bajo licencia BSD. Con esto termina este art√≠culo, espero les haya parecido interesante y les sea de utilidad en sus proyectos.\nSaludos!\nEste post fue escrito utilizando IPython notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2016-09-13","id":17,"permalink":"/blog/2016/09/13/factorizacion-de-matrices-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, Analisis de datos y Python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Cuando trabajamos en problemas de Machine Learning, muchas veces nos vamos a encontrar con enormes conjuntos de datos, con cientos o miles de caracter√≠sticas o features. Una forma simple de reducir las dimensiones de estas caracter√≠sticas es aplicar alguna t√©cnica de Factorizaci√≥n de matrices.","tags":["python","algebra","programacion","machine learning","redes neuronales","matematica"],"title":"Factorizaci√≥n de Matrices con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, Analisis de datos y Python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# De m√°s esta decir que el sentido de la visi√≥n es uno de los grandes prodigios de la Naturaleza. En fracciones de segundos, podemos identificar objetos dentro de nuestro campo de visi√≥n, sin siquiera detenernos a pensar en ello. Pero no s√≥lo podemos nombrar estos objetos que observamos, sino que tambi√©n podemos percibir su profundidad, distinguir perfectamente sus contornos, y separarlos de sus fondos. De alguna manera los ojos captan datos de p√≠xeles, pero el cerebro transforma esa informaci√≥n en caracter√≠sticas m√°s significativas - l√≠neas, curvas y formas - que podr√≠an indicar, por ejemplo, que estamos mirando a una persona.\nGracias a que el √°rea del cerebro responsable de la visi√≥n es una de las zonas m√°s estudiadas y que m√°s conocemos; sabemos que la corteza visual contiene una disposici√≥n jer√°rquica compleja de neuronas. Por ejemplo, la informaci√≥n visual es introducida en la corteza a trav√©s del √°rea visual primaria, llamada V1. Las neuronas de V1 se ocupan de caracter√≠sticas visuales de bajo nivel, tales como peque√±os segmentos de contorno, componentes de peque√±a escala del movimiento, disparidad binocular, e informaci√≥n b√°sica de contraste y color. V1 luego alimenta de informaci√≥n a otras √°reas, como V2, V4 y V5. Cada una de estas √°reas se ocupa de los aspectos m√°s espec√≠ficos o abstractas de la informaci√≥n. Por ejemplo, las neuronas en V4 se ocupan de objetos de mediana complejidad, tales como formas de estrellas en diferentes colores. La corteza visual de los animales es el m√°s potente sistema de procesamiento visual que conocemos, por lo que suena l√≥gico inspirarse en ella para crear una variante de redes neuronales artificiales que ayude a identificar im√°genes; es as√≠ como surgen las redes neuronales convolucionales.\n¬øQu√© son las Redes Neuronales Convolucionales?# Las redes neuronales convolucionales son muy similares a las redes neuronales ordinarias como el perceptron multicapa que vimos en el art√≠culo anterior; se componen de neuronas que tienen pesos y sesgos que pueden aprender. Cada neurona recibe algunas entradas, realiza un producto escalar y luego aplica una funci√≥n de activaci√≥n. Al igual que en el perceptron multicapa tambi√©n vamos a tener una funci√≥n de p√©rdida o costo (por ejemplo SVM / Softmax) sobre la √∫ltima capa, la cual estar√° totalmente conectada. Lo que diferencia a las redes neuronales convolucionales es que suponen expl√≠citamente que las entradas son im√°genes, lo que nos permite codificar ciertas propiedades en la arquitectura; permitiendo ganar en eficiencia y reducir la cantidad de par√°metros en la red. Las redes neuronales convolucionales vienen a solucionar el problema de que las redes neuronales ordinarias no escalan bien para im√°genes de mucha definici√≥n; por ejemplo en el problema de MNIST, las im√°genes son de 28x28; por lo que una sola neurona plenamente conectado en una primera capa oculta de una red neuronal ordinaria tendr√≠a 28 x 28 = 784 pesos. Esta cantidad todav√≠a parece manejable, pero es evidente que esta estructura totalmente conectado no funciona bien con im√°genes m√°s grandes. Si tomamos el caso de una imagen de mayor tama√±o, por ejemplo de 200x200 con colores RGB, dar√≠a lugar a neuronas que tienen 200 x 200 x 3 = 120.000 pesos. Por otra parte, el contar con tantos par√°metros, tambi√©n ser√≠a un desperdicio de recursos y conducir√≠a r√°pidamente a sobreajuste.\nLas redes neuronales convolucionales trabajan modelando de forma consecutiva peque√±as piezas de informaci√≥n, y luego combinando esta informaci√≥n en las capas m√°s profundas de la red. Una manera de entenderlas es que la primera capa intentar√° detectar los bordes y establecer patrones de detecci√≥n de bordes. Luego, las capas posteriores trataran de combinarlos en formas m√°s simples y, finalmente, en patrones de las diferentes posiciones de los objetos, iluminaci√≥n, escalas, etc. Las capas finales intentar√°n hacer coincidir una imagen de entrada con todas los patrones y arribar a una predicci√≥n final como una suma ponderada de todos ellos. De esta forma las redes neuronales convolucionales son capaces de modelar complejas variaciones y comportamientos dando predicciones bastantes precisas.\nEstructura de las Redes Neuronales Convolucionales# En general, las redes neuronales convolucionales van a estar construidas con una estructura que contendr√° 3 tipos distintos de capas:\nUna capa convolucional, que es la que le da le nombre a la red. Una capa de reducci√≥n o de pooling, la cual va a reducir la cantidad de par√°metros al quedarse con las caracter√≠sticas m√°s comunes. Una capa clasificadora totalmente conectada, la cual nos va dar el resultado final de la red. Profundicemos un poco en cada una de ellas.\nCapa convolucional# Como dijimos anteriormente, lo que distingue a las redes neuronales convolucionales de cualquier otra red neuronal es utilizan un operaci√≥n llamada convoluci√≥n en alguna de sus capas; en lugar de utilizar la multiplicaci√≥n de matrices que se aplica generalmente. La operaci√≥n de convoluci√≥n recibe como entrada o input la imagen y luego aplica sobre ella un filtro o kernel que nos devuelve un mapa de las caracter√≠sticas de la imagen original, de esta forma logramos reducir el tama√±o de los par√°metros. La convoluci√≥n aprovecha tres ideas importantes que pueden ayudar a mejorar cualquier sistema de machine learning, ellas son:\ninteracciones dispersas, ya que al aplicar un filtro de menor tama√±o sobre la entrada original podemos reducir dr√°sticamente la cantidad de par√°metros y c√°lculos; los par√°metros compartidos, que hace referencia a compartir los par√°metros entre los distintos tipos de filtros, ayudando tambi√©n a mejorar la eficiencia del sistema; y las representaciones equivariante, que indican que si las entradas cambian, las salidas van a cambiar tambi√©n en forma similar. Por otra parte, la convoluci√≥n proporciona un medio para trabajar con entradas de tama√±o variable, lo que puede ser tambi√©n muy conveniente.\nCapa de reducci√≥n o pooling# La capa de reducci√≥n o pooling se coloca generalmente despu√©s de la capa convolucional. Su utilidad principal radica en la reducci√≥n de las dimensiones espaciales (ancho x alto) del volumen de entrada para la siguiente capa convolucional. No afecta a la dimensi√≥n de profundidad del volumen. La operaci√≥n realizada por esta capa tambi√©n se llama reducci√≥n de muestreo, ya que la reducci√≥n de tama√±o conduce tambi√©n a la p√©rdida de informaci√≥n. Sin embargo, una p√©rdida de este tipo puede ser beneficioso para la red por dos razones:\nla disminuci√≥n en el tama√±o conduce a una menor sobrecarga de c√°lculo para las pr√≥ximas capas de la red; tambi√©n trabaja para reducir el sobreajuste. La operaci√≥n que se suele utilizar en esta capa es max-pooling, que divide a la imagen de entrada en un conjunto de rect√°ngulos y, respecto de cada subregi√≥n, se va quedando con el m√°ximo valor.\nCapa clasificadora totalmente conectada# Al final de las capas convolucional y de pooling, las redes utilizan generalmente capas completamente conectados en la que cada pixel se considera como una neurona separada al igual que en una red neuronal regular. Esta √∫ltima capa clasificadora tendr√° tantas neuronas como el n√∫mero de clases que se debe predecir.\nEjemplo en TensorFlow# Luego de toda esta introducci√≥n te√≥rica es tiempo de pasar a la acci√≥n y ver como podemos aplicar todo lo que hemos aprendimos para crear una red neuronal convolucional con la ayuda de TensorFlow. Para esto vamos volver a utilizar el conjunto de datos MNIST, pero esta vez vamos a clasificar los digitos utilizando una red neuronal convolucional.\n# importamos la libreria import tensorflow as tf # importando el dataset from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(\u0026#34;MNIST_data/\u0026#34;, one_hot=True) Extracting MNIST_data/train-images-idx3-ubyte.gz Extracting MNIST_data/train-labels-idx1-ubyte.gz Extracting MNIST_data/t10k-images-idx3-ubyte.gz Extracting MNIST_data/t10k-labels-idx1-ubyte.gz # Parametros tasa_aprendizaje = 0.001 epocas = 200000 lote = 128 mostrar_paso = 100 # Parametros de la red n_entradas = 784 # datos de MNIST(forma img: 28*28) n_clases = 10 # Total de clases a clasificar (0-9 digitos) dropout = 0.75 # Dropout, probabilidad para quedarse con unidades # input para los grafos x = tf.placeholder(tf.float32, [None, n_entradas]) y = tf.placeholder(tf.float32, [None, n_clases]) keep_prob = tf.placeholder(tf.float32) #dropout # Creaci√≥n del modelo def conv2d(x, W, b, strides=1): # capa convolucional con activacion relu x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding=\u0026#39;SAME\u0026#39;) x = tf.nn.bias_add(x, b) return tf.nn.relu(x) def maxpool2d(x, k=2): # capa de pooling con max pooling return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding=\u0026#39;SAME\u0026#39;) # armado de la red def conv_net(x, weights, biases, dropout): # cambiar la forma de la imagen x = tf.reshape(x, shape=[-1, 28, 28, 1]) # capa convolucional conv1 = conv2d(x, pesos[\u0026#39;pc1\u0026#39;], sesgo[\u0026#39;sc1\u0026#39;]) # Max Pooling (reducci√≥n de muestreo) conv1 = maxpool2d(conv1, k=2) # capa convolucional conv2 = conv2d(conv1, pesos[\u0026#39;pc2\u0026#39;], sesgo[\u0026#39;sc2\u0026#39;]) # Max Pooling (reducci√≥n de muestreo) conv2 = maxpool2d(conv2, k=2) # capa clasificadora totalmente conectada fc1 = tf.reshape(conv2, [-1, pesos[\u0026#39;pd1\u0026#39;].get_shape().as_list()[0]]) fc1 = tf.add(tf.matmul(fc1, pesos[\u0026#39;pd1\u0026#39;]), sesgo[\u0026#39;sd1\u0026#39;]) fc1 = tf.nn.relu(fc1) # aplicar descarte fc1 = tf.nn.dropout(fc1, dropout) # Output, prediccion de la clase out = tf.add(tf.matmul(fc1, pesos[\u0026#39;out\u0026#39;]), sesgo[\u0026#39;out\u0026#39;]) return out # Definimos los pesos y sesgo de cada capa pesos = { # 5x5 conv, 1 input, 32 outputs \u0026#39;pc1\u0026#39;: tf.Variable(tf.random_normal([5, 5, 1, 32])), # 5x5 conv, 32 inputs, 64 outputs \u0026#39;pc2\u0026#39;: tf.Variable(tf.random_normal([5, 5, 32, 64])), # totalmente conectada, 7*7*64 inputs, 1024 outputs \u0026#39;pd1\u0026#39;: tf.Variable(tf.random_normal([7*7*64, 1024])), # 1024 inputs, 10 outputs (prediccion de clase) \u0026#39;out\u0026#39;: tf.Variable(tf.random_normal([1024, n_clases])) } sesgo = { \u0026#39;sc1\u0026#39;: tf.Variable(tf.random_normal([32])), \u0026#39;sc2\u0026#39;: tf.Variable(tf.random_normal([64])), \u0026#39;sd1\u0026#39;: tf.Variable(tf.random_normal([1024])), \u0026#39;out\u0026#39;: tf.Variable(tf.random_normal([n_clases])) } # Construct model pred = conv_net(x, pesos, sesgo, keep_prob) # Define loss and optimizer costo = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y)) optimizador = tf.train.AdamOptimizer(learning_rate=tasa_aprendizaje ).minimize(costo) # Evaluate model pred_correcta = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1)) precision = tf.reduce_mean(tf.cast(pred_correcta, tf.float32)) # Initializing the variables init = tf.initialize_all_variables() # Launch the graph with tf.Session() as sess: sess.run(init) paso = 1 # Keep training until reach max iterations while paso * lote \u0026lt; epocas: batch_x, batch_y = mnist.train.next_batch(lote) # Run optimization op (backprop) sess.run(optimizador, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout}) if paso % mostrar_paso == 0: # Calculate batch loss and accuracy loss, acc = sess.run([costo, precision], feed_dict={x: batch_x, y: batch_y, keep_prob: 1.}) print(\u0026#34;Iteraci√≥n: {0: 04d} costo = {1:.6f} precision = {2:.5f}\u0026#34; .format(paso*lote, loss, acc)) paso += 1 print(\u0026#34;Optimizaci√≥n terminada!\u0026#34;) # Calculala precisi√≥n sobre los datos de evaluaci√≥n print(\u0026#34;Precisi√≥n evaluci√≥n: {0:.2f}\u0026#34;.format( sess.run(precision, feed_dict={x: mnist.test.images[:256], y: mnist.test.labels[:256], keep_prob: 1.}))) Iteraci√≥n: 12800 costo = 4224.402344 precision = 0.78906 Iteraci√≥n: 25600 costo = 1844.304932 precision = 0.89844 Iteraci√≥n: 38400 costo = 289.222961 precision = 0.95312 Iteraci√≥n: 51200 costo = 442.070557 precision = 0.95312 Iteraci√≥n: 64000 costo = 275.925751 precision = 0.96094 Iteraci√≥n: 76800 costo = 124.911362 precision = 0.96875 Iteraci√≥n: 89600 costo = 302.610291 precision = 0.96875 Iteraci√≥n: 102400 costo = 377.000092 precision = 0.95312 Iteraci√≥n: 115200 costo = 348.436188 precision = 0.99219 Iteraci√≥n: 128000 costo = 131.974304 precision = 0.99219 Iteraci√≥n: 140800 costo = 442.505676 precision = 0.96875 Iteraci√≥n: 153600 costo = 10.694885 precision = 0.98438 Iteraci√≥n: 166400 costo = 39.013718 precision = 0.98438 Iteraci√≥n: 179200 costo = 278.081543 precision = 0.96094 Iteraci√≥n: 192000 costo = 91.298866 precision = 0.97656 Optimizaci√≥n terminada! Precisi√≥n evaluci√≥n: 0.98 Como podemos ver, utilizando redes neuronales convolucionales en lugar de un peceptron multicapa como hicimos en el art√≠culo anterior; logramos una precisi√≥n de 98%, una mejora bastante significativa.\nAqu√≠ termina el art√≠culo, espero que les haya resultado interesante y los motive a explorar el fascinante mundo de las redes neuronales.\nSaludos!\nEste post fue escrito utilizando IPython notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2016-08-02","id":18,"permalink":"/blog/2016/08/02/redes-neuronales-convolucionales-con-tensorflow/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, Analisis de datos y Python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# De m√°s esta decir que el sentido de la visi√≥n es uno de los grandes prodigios de la Naturaleza. En fracciones de segundos, podemos identificar objetos dentro de nuestro campo de visi√≥n, sin siquiera detenernos a pensar en ello. Pero no s√≥lo podemos nombrar estos objetos que observamos, sino que tambi√©n podemos percibir su profundidad, distinguir perfectamente sus contornos, y separarlos de sus fondos.","tags":["python","programacion","analisis de datos","machine learning","redes neuronales","tensorflow"],"title":"Redes neuronales convolucionales con TensorFlow"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Las variables aleatorias han llegado a desempe√±ar un papel importante en casi todos los campos de estudio: en la F√≠sica, la Qu√≠mica y la Ingenier√≠a; y especialmente en las ciencias biol√≥gicas y sociales. Estas variables aleatorias son medidas y analizadas en t√©rminos de sus propiedades estad√≠sticas y probabil√≠sticas, de las cuales una caracter√≠stica subyacente es su funci√≥n de distribuci√≥n. A pesar de que el n√∫mero potencial de distribuciones puede ser muy grande, en la pr√°ctica, un n√∫mero relativamente peque√±o se utilizan; ya sea porque tienen caracter√≠sticas matem√°ticas que las hace f√°ciles de usar o porque se asemejan bastante bien a una porci√≥n de la realidad, o por ambas razones combinadas.\n¬øPor qu√© es importante conocer las distribuciones?# Muchos resultados en las ciencias se basan en conclusiones que se extraen sobre una poblaci√≥n general a partir del estudio de una muestra de esta poblaci√≥n. Este proceso se conoce como inferencia estad√≠stica; y este tipo de inferencia con frecuencia se basa en hacer suposiciones acerca de la forma en que los datos se distribuyen, o requiere realizar alguna transformaci√≥n de los datos para que se ajusten mejor a alguna de las distribuciones conocidas y estudiadas en profundidad.\nLas distribuciones de probabilidad te√≥ricas son √∫tiles en la inferencia estad√≠stica porque sus propiedades y caracter√≠sticas son conocidas. Si la distribuci√≥n real de un conjunto de datos dado es razonablemente cercana a la de una distribuci√≥n de probabilidad te√≥rica, muchos de los c√°lculos se pueden realizar en los datos reales utilizando hip√≥tesis extra√≠das de la distribuci√≥n te√≥rica.\nGraficando distribuciones# Histogramas# Una de las mejores maneras de describir una variable es representar los valores que aparecen en el conjunto de datos y el n√∫mero de veces que aparece cada valor. La representaci√≥n m√°s com√∫n de una distribuci√≥n es un histograma, que es un gr√°fico que muestra la frecuencia de cada valor.\nEn Python, podemos graficar f√°cilmente un histograma con la ayuda de la funci√≥n hist de matplotlib, simplemente debemos pasarle los datos y la cantidad de contenedores en los que queremos dividirlos. Por ejemplo, podr√≠amos graficar el histograma de una distribuci√≥n normal del siguiente modo.\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # importando modulos necesarios %matplotlib inline import matplotlib.pyplot as plt import numpy as np from scipy import stats import seaborn as sns np.random.seed(2016) # replicar random # parametros esteticos de seaborn sns.set_palette(\u0026#34;deep\u0026#34;, desat=.6) sns.set_context(rc={\u0026#34;figure.figsize\u0026#34;: (8, 4)}) # Graficando histograma mu, sigma = 0, 0.2 # media y desvio estandar datos = np.random.normal(mu, sigma, 1000) #creando muestra de datos # histograma de distribuci√≥n normal. cuenta, cajas, ignorar = plt.hist(datos, 20) plt.ylabel(\u0026#39;frequencia\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.title(\u0026#39;Histograma\u0026#39;) plt.show() Funci√≥n de Masa de Probabilidad# Otra forma de representar a las distribuciones discretas es utilizando su Funci√≥n de Masa de Probabilidad o FMP, la cual relaciona cada valor con su probabilidad en lugar de su frecuencia como vimos anteriormente. Esta funci√≥n es normalizada de forma tal que el valor total de probabilidad sea 1. La ventaja que nos ofrece utilizar la FMP es que podemos comparar dos distribuciones sin necesidad de ser confundidos por las diferencias en el tama√±o de las muestras. Tambi√©n debemos tener en cuenta que FMP funciona bien si el n√∫mero de valores es peque√±o; pero a medida que el n√∫mero de valores aumenta, la probabilidad asociada a cada valor se hace cada vez m√°s peque√±a y el efecto del ruido aleatorio aumenta. Veamos un ejemplo con Python.\n# Graficando FMP n, p = 30, 0.4 # parametros de forma de la distribuci√≥n binomial n_1, p_1 = 20, 0.3 # parametros de forma de la distribuci√≥n binomial x = np.arange(stats.binom.ppf(0.01, n, p), stats.binom.ppf(0.99, n, p)) x_1 = np.arange(stats.binom.ppf(0.01, n_1, p_1), stats.binom.ppf(0.99, n_1, p_1)) fmp = stats.binom.pmf(x, n, p) # Funci√≥n de Masa de Probabilidad fmp_1 = stats.binom.pmf(x_1, n_1, p_1) # Funci√≥n de Masa de Probabilidad plt.plot(x, fmp, \u0026#39;--\u0026#39;) plt.plot(x_1, fmp_1) plt.vlines(x, 0, fmp, colors=\u0026#39;b\u0026#39;, lw=5, alpha=0.5) plt.vlines(x_1, 0, fmp_1, colors=\u0026#39;g\u0026#39;, lw=5, alpha=0.5) plt.title(\u0026#39;Funci√≥n de Masa de Probabilidad\u0026#39;) plt.ylabel(\u0026#39;probabilidad\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.show() Funci√≥n de Distribuci√≥n Acumulada# Si queremos evitar los problemas que se generan con FMP cuando el n√∫mero de valores es muy grande, podemos recurrir a utilizar la Funci√≥n de Distribuci√≥n Acumulada o FDA, para representar a nuestras distribuciones, tanto discretas como continuas. Esta funci√≥n relaciona los valores con su correspondiente percentil; es decir que va a describir la probabilidad de que una variable aleatoria X sujeta a cierta ley de distribuci√≥n de probabilidad se sit√∫e en la zona de valores menores o iguales a x.\n# Graficando Funci√≥n de Distribuci√≥n Acumulada con Python x_1 = np.linspace(stats.norm(10, 1.2).ppf(0.01), stats.norm(10, 1.2).ppf(0.99), 100) fda_binom = stats.binom.cdf(x, n, p) # Funci√≥n de Distribuci√≥n Acumulada fda_normal = stats.norm(10, 1.2).cdf(x_1) # Funci√≥n de Distribuci√≥n Acumulada plt.plot(x, fda_binom, \u0026#39;--\u0026#39;, label=\u0026#39;FDA binomial\u0026#39;) plt.plot(x_1, fda_normal, label=\u0026#39;FDA nomal\u0026#39;) plt.title(\u0026#39;Funci√≥n de Distribuci√≥n Acumulada\u0026#39;) plt.ylabel(\u0026#39;probabilidad\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.legend(loc=4) plt.show() Funci√≥n de Densidad de Probabilidad# Por √∫ltimo, el equivalente a la FMP para distribuciones continuas es la Funci√≥n de Densidad de Probabilidad o FDP. Esta funci√≥n es la derivada de la Funci√≥n de Distribuci√≥n Acumulada. Por ejemplo, para la distribuci√≥n normal que graficamos anteriormente, su FDP es la siguiente. La t√≠pica forma de campana que caracteriza a esta distribuci√≥n.\n# Graficando Funci√≥n de Densidad de Probibilidad con Python FDP_normal = stats.norm(10, 1.2).pdf(x_1) # FDP plt.plot(x_1, FDP_normal, label=\u0026#39;FDP nomal\u0026#39;) plt.title(\u0026#39;Funci√≥n de Densidad de Probabilidad\u0026#39;) plt.ylabel(\u0026#39;probabilidad\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.show() Distribuciones# Ahora que ya conocemos como podemos hacer para representar a las distribuciones; pasemos a analizar cada una de ellas en m√°s detalle para conocer su forma, sus principales aplicaciones y sus propiedades. Comencemos por las distribuciones discretas.\nDistribuciones Discretas# Las distribuciones discretas son aquellas en las que la variable puede tomar solo algunos valores determinados. Los principales exponentes de este grupo son las siguientes:\nDistribuci√≥n Poisson# La Distribuci√≥n Poisson esta dada por la formula:\n$$p(r; \\mu) = \\frac{\\mu^r e^{-\\mu}}{r!}$$ En d√≥nde \\(r\\) es un entero (\\(r \\ge 0\\)) y \\(\\mu\\) es un n√∫mero real positivo. La Distribuci√≥n Poisson describe la probabilidad de encontrar exactamente \\(r\\) eventos en un lapso de tiempo si los acontecimientos se producen de forma independiente a una velocidad constante \\(\\mu\\). Es una de las distribuciones m√°s utilizadas en estad√≠stica con varias aplicaciones; como por ejemplo describir el n√∫mero de fallos en un lote de materiales o la cantidad de llegadas por hora a un centro de servicios.\nEn Python la podemos generar f√°cilmente con la ayuda de scipy.stats, paquete que utilizaremos para representar a todas las restantes distribuciones a lo largo de todo el art√≠culo.\n# Graficando Poisson mu = 3.6 # parametro de forma poisson = stats.poisson(mu) # Distribuci√≥n x = np.arange(poisson.ppf(0.01), poisson.ppf(0.99)) fmp = poisson.pmf(x) # Funci√≥n de Masa de Probabilidad plt.plot(x, fmp, \u0026#39;--\u0026#39;) plt.vlines(x, 0, fmp, colors=\u0026#39;b\u0026#39;, lw=5, alpha=0.5) plt.title(\u0026#39;Distribuci√≥n Poisson\u0026#39;) plt.ylabel(\u0026#39;probabilidad\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.show() # histograma aleatorios = poisson.rvs(1000) # genera aleatorios cuenta, cajas, ignorar = plt.hist(aleatorios, 20) plt.ylabel(\u0026#39;frequencia\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.title(\u0026#39;Histograma Poisson\u0026#39;) plt.show() Distribuci√≥n Binomial# La Distribuci√≥n Binomial esta dada por la formula:\n$$p(r; N, p) = \\left(\\begin{array}{c} N \\\\ r \\end{array}\\right) p^r(1 - p)^{N - r} $$ En d√≥nde \\(r\\) con la condici√≥n \\(0 \\le r \\le N\\) y el par√°metro \\(N\\) (\\(N \u0026gt; 0\\)) son enteros; y el par√°metro \\(p\\) (\\(0 \\le p \\le 1\\)) es un n√∫mero real. La Distribuci√≥n Binomial describe la probabilidad de exactamente \\(r\\) √©xitos en \\(N\\) pruebas si la probabilidad de √©xito en una sola prueba es \\(p\\).\n# Graficando Binomial N, p = 30, 0.4 # parametros de forma binomial = stats.binom(N, p) # Distribuci√≥n x = np.arange(binomial.ppf(0.01), binomial.ppf(0.99)) fmp = binomial.pmf(x) # Funci√≥n de Masa de Probabilidad plt.plot(x, fmp, \u0026#39;--\u0026#39;) plt.vlines(x, 0, fmp, colors=\u0026#39;b\u0026#39;, lw=5, alpha=0.5) plt.title(\u0026#39;Distribuci√≥n Binomial\u0026#39;) plt.ylabel(\u0026#39;probabilidad\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.show() # histograma aleatorios = binomial.rvs(1000) # genera aleatorios cuenta, cajas, ignorar = plt.hist(aleatorios, 20) plt.ylabel(\u0026#39;frequencia\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.title(\u0026#39;Histograma Binomial\u0026#39;) plt.show() Distribuci√≥n Geom√©trica# La Distribuci√≥n Geom√©trica esta dada por la formula:\n$$p(r; p) = p(1- p)^{r-1} $$ En d√≥nde \\(r \\ge 1\\) y el par√°metro \\(p\\) (\\(0 \\le p \\le 1\\)) es un n√∫mero real. La Distribuci√≥n Geom√©trica expresa la probabilidad de tener que esperar exactamente \\(r\\) pruebas hasta encontrar el primer √©xito si la probabilidad de √©xito en una sola prueba es \\(p\\). Por ejemplo, en un proceso de selecci√≥n, podr√≠a definir el n√∫mero de entrevistas que deber√≠amos realizar antes de encontrar al primer candidato aceptable.\n# Graficando Geom√©trica p = 0.3 # parametro de forma geometrica = stats.geom(p) # Distribuci√≥n x = np.arange(geometrica.ppf(0.01), geometrica.ppf(0.99)) fmp = geometrica.pmf(x) # Funci√≥n de Masa de Probabilidad plt.plot(x, fmp, \u0026#39;--\u0026#39;) plt.vlines(x, 0, fmp, colors=\u0026#39;b\u0026#39;, lw=5, alpha=0.5) plt.title(\u0026#39;Distribuci√≥n Geom√©trica\u0026#39;) plt.ylabel(\u0026#39;probabilidad\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.show() # histograma aleatorios = geometrica.rvs(1000) # genera aleatorios cuenta, cajas, ignorar = plt.hist(aleatorios, 20) plt.ylabel(\u0026#39;frequencia\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.title(\u0026#39;Histograma Geom√©trica\u0026#39;) plt.show() Distribuci√≥n Hipergeom√©trica# La Distribuci√≥n Hipergeom√©trica esta dada por la formula:\n$$p(r; n, N, M) = \\frac{\\left(\\begin{array}{c} M \\\\ r \\end{array}\\right)\\left(\\begin{array}{c} N - M\\\\ n -r \\end{array}\\right)}{\\left(\\begin{array}{c} N \\\\ n \\end{array}\\right)} $$ En d√≥nde el valor de \\(r\\) esta limitado por \\(\\max(0, n - N + M)\\) y \\(\\min(n, M)\\) inclusive; y los par√°metros \\(n\\) (\\(1 \\le n \\le N\\)), \\(N\\) (\\(N \\ge 1\\)) y \\(M\\) (\\(M \\ge 1\\)) son todos n√∫meros enteros. La Distribuci√≥n Hipergeom√©trica describe experimentos en donde se seleccionan los elementos al azar sin reemplazo (se evita seleccionar el mismo elemento m√°s de una vez). M√°s precisamente, supongamos que tenemos \\(N\\) elementos de los cuales \\(M\\) tienen un cierto atributo (y \\(N - M\\) no tiene). Si escogemos \\(n\\) elementos al azar sin reemplazo, \\(p(r)\\) es la probabilidad de que exactamente \\(r\\) de los elementos seleccionados provienen del grupo con el atributo.\n# Graficando Hipergeom√©trica M, n, N = 30, 10, 12 # parametros de forma hipergeometrica = stats.hypergeom(M, n, N) # Distribuci√≥n x = np.arange(0, n+1) fmp = hipergeometrica.pmf(x) # Funci√≥n de Masa de Probabilidad plt.plot(x, fmp, \u0026#39;--\u0026#39;) plt.vlines(x, 0, fmp, colors=\u0026#39;b\u0026#39;, lw=5, alpha=0.5) plt.title(\u0026#39;Distribuci√≥n Hipergeom√©trica\u0026#39;) plt.ylabel(\u0026#39;probabilidad\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.show() # histograma aleatorios = hipergeometrica.rvs(1000) # genera aleatorios cuenta, cajas, ignorar = plt.hist(aleatorios, 20) plt.ylabel(\u0026#39;frequencia\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.title(\u0026#39;Histograma Hipergeom√©trica\u0026#39;) plt.show() Distribuci√≥n de Bernoulli# La Distribuci√≥n de Bernoulli esta dada por la formula:\n$$p(r;p) = \\left\\{ \\begin{array}{ll} 1 - p = q \u0026 \\mbox{si } r = 0 \\ \\mbox{(fracaso)}\\\\ p \u0026 \\mbox{si } r = 1 \\ \\mbox{(√©xito)} \\end{array} \\right.$$ En d√≥nde el par√°metro \\(p\\) es la probabilidad de √©xito en un solo ensayo, la probabilidad de fracaso por lo tanto va a ser \\(1 - p\\) (muchas veces expresada como \\(q\\)). Tanto \\(p\\) como \\(q\\) van a estar limitados al intervalo de cero a uno. La Distribuci√≥n de Bernoulli describe un experimento probabil√≠stico en donde el ensayo tiene dos posibles resultados, √©xito o fracaso. Desde esta distribuci√≥n se pueden deducir varias Funciones de Densidad de Probabilidad de otras distribuciones que se basen en una serie de ensayos independientes.\n# Graficando Bernoulli p = 0.5 # parametro de forma bernoulli = stats.bernoulli(p) x = np.arange(-1, 3) fmp = bernoulli.pmf(x) # Funci√≥n de Masa de Probabilidad fig, ax = plt.subplots() ax.plot(x, fmp, \u0026#39;bo\u0026#39;) ax.vlines(x, 0, fmp, colors=\u0026#39;b\u0026#39;, lw=5, alpha=0.5) ax.set_yticks([0., 0.2, 0.4, 0.6]) plt.title(\u0026#39;Distribuci√≥n Bernoulli\u0026#39;) plt.ylabel(\u0026#39;probabilidad\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.show() # histograma aleatorios = bernoulli.rvs(1000) # genera aleatorios cuenta, cajas, ignorar = plt.hist(aleatorios, 20) plt.ylabel(\u0026#39;frequencia\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.title(\u0026#39;Histograma Bernoulli\u0026#39;) plt.show() Distribuciones continuas# Ahora que ya conocemos las principales distribuciones discretas, podemos pasar a describir a las distribuciones continuas; en ellas a diferencia de lo que ve√≠amos antes, la variable puede tomar cualquier valor dentro de un intervalo espec√≠fico. Dentro de este grupo vamos a encontrar a las siguientes:\nDistribuci√≥n de Normal# La Distribuci√≥n Normal, o tambi√©n llamada Distribuci√≥n de Gauss, es aplicable a un amplio rango de problemas, lo que la convierte en la distribuci√≥n m√°s utilizada en estad√≠stica; esta dada por la formula:\n$$p(x;\\mu, \\sigma^2) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{\\frac{-1}{2}\\left(\\frac{x - \\mu}{\\sigma} \\right)^2} $$ En d√≥nde \\(\\mu\\) es el par√°metro de ubicaci√≥n, y va a ser igual a la media aritm√©tica y \\(\\sigma^2\\) es el desv√≠o est√°ndar. Algunos ejemplos de variables asociadas a fen√≥menos naturales que siguen el modelo de la Distribuci√≥n Normal son:\ncaracter√≠sticas morfol√≥gicas de individuos, como la estatura; caracter√≠sticas sociol√≥gicas, como el consumo de cierto producto por un mismo grupo de individuos; caracter√≠sticas psicol√≥gicas, como el cociente intelectual; nivel de ruido en telecomunicaciones; errores cometidos al medir ciertas magnitudes; etc. # Graficando Normal mu, sigma = 0, 0.2 # media y desvio estandar normal = stats.norm(mu, sigma) x = np.linspace(normal.ppf(0.01), normal.ppf(0.99), 100) fp = normal.pdf(x) # Funci√≥n de Probabilidad plt.plot(x, fp) plt.title(\u0026#39;Distribuci√≥n Normal\u0026#39;) plt.ylabel(\u0026#39;probabilidad\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.show() # histograma aleatorios = normal.rvs(1000) # genera aleatorios cuenta, cajas, ignorar = plt.hist(aleatorios, 20) plt.ylabel(\u0026#39;frequencia\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.title(\u0026#39;Histograma Normal\u0026#39;) plt.show() Distribuci√≥n Uniforme# La Distribuci√≥n Uniforme es un caso muy simple expresada por la funci√≥n:\n$$f(x; a, b) = \\frac{1}{b -a} \\ \\mbox{para} \\ a \\le x \\le b $$ Su funci√≥n de distribuci√≥n esta entonces dada por:\n$$ p(x;a, b) = \\left\\{ \\begin{array}{ll} 0 \u0026 \\mbox{si } x \\le a \\\\ \\frac{x-a}{b-a} \u0026 \\mbox{si } a \\le x \\le b \\\\ 1 \u0026 \\mbox{si } b \\le x \\end{array} \\right. $$ Todos los valore tienen pr√°cticamente la misma probabilidad.\n# Graficando Uniforme uniforme = stats.uniform() x = np.linspace(uniforme.ppf(0.01), uniforme.ppf(0.99), 100) fp = uniforme.pdf(x) # Funci√≥n de Probabilidad fig, ax = plt.subplots() ax.plot(x, fp, \u0026#39;--\u0026#39;) ax.vlines(x, 0, fp, colors=\u0026#39;b\u0026#39;, lw=5, alpha=0.5) ax.set_yticks([0., 0.2, 0.4, 0.6, 0.8, 1., 1.2]) plt.title(\u0026#39;Distribuci√≥n Uniforme\u0026#39;) plt.ylabel(\u0026#39;probabilidad\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.show() # histograma aleatorios = uniforme.rvs(1000) # genera aleatorios cuenta, cajas, ignorar = plt.hist(aleatorios, 20) plt.ylabel(\u0026#39;frequencia\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.title(\u0026#39;Histograma Uniforme\u0026#39;) plt.show() Distribuci√≥n de Log-normal# La Distribuci√≥n Log-normal esta dada por la formula:\n$$p(x;\\mu, \\sigma) = \\frac{1}{ x \\sigma \\sqrt{2 \\pi}} e^{\\frac{-1}{2}\\left(\\frac{\\ln x - \\mu}{\\sigma} \\right)^2} $$ En d√≥nde la variable \\(x \u0026gt; 0\\) y los par√°metros \\(\\mu\\) y \\(\\sigma \u0026gt; 0\\) son todos n√∫meros reales. La Distribuci√≥n Log-normal es aplicable a variables aleatorias que est√°n limitadas por cero, pero tienen pocos valores grandes. Es una distribuci√≥n con asimetr√≠a positiva. Algunos de los ejemplos en que la solemos encontrar son:\nEl peso de los adultos. La concentraci√≥n de los minerales en dep√≥sitos. Duraci√≥n de licencia por enfermedad. Distribuci√≥n de riqueza Tiempos muertos de maquinarias. # Graficando Log-Normal sigma = 0.6 # parametro lognormal = stats.lognorm(sigma) x = np.linspace(lognormal.ppf(0.01), lognormal.ppf(0.99), 100) fp = lognormal.pdf(x) # Funci√≥n de Probabilidad plt.plot(x, fp) plt.title(\u0026#39;Distribuci√≥n Log-normal\u0026#39;) plt.ylabel(\u0026#39;probabilidad\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.show() # histograma aleatorios = lognormal.rvs(1000) # genera aleatorios cuenta, cajas, ignorar = plt.hist(aleatorios, 20) plt.ylabel(\u0026#39;frequencia\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.title(\u0026#39;Histograma Log-normal\u0026#39;) plt.show() Distribuci√≥n de Exponencial# La Distribuci√≥n Exponencial esta dada por la formula:\n$$p(x;\\alpha) = \\frac{1}{ \\alpha} e^{\\frac{-x}{\\alpha}} $$ En d√≥nde tanto la variable \\(x\\) como el par√°metro \\(\\alpha\\) son n√∫meros reales positivos. La Distribuci√≥n Exponencial tiene bastantes aplicaciones, tales como la desintegraci√≥n de un √°tomo radioactivo o el tiempo entre eventos en un proceso de Poisson donde los acontecimientos suceden a una velocidad constante.\n# Graficando Exponencial exponencial = stats.expon() x = np.linspace(exponencial.ppf(0.01), exponencial.ppf(0.99), 100) fp = exponencial.pdf(x) # Funci√≥n de Probabilidad plt.plot(x, fp) plt.title(\u0026#39;Distribuci√≥n Exponencial\u0026#39;) plt.ylabel(\u0026#39;probabilidad\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.show() # histograma aleatorios = exponencial.rvs(1000) # genera aleatorios cuenta, cajas, ignorar = plt.hist(aleatorios, 20) plt.ylabel(\u0026#39;frequencia\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.title(\u0026#39;Histograma Exponencial\u0026#39;) plt.show() Distribuci√≥n Gamma# La Distribuci√≥n Gamma esta dada por la formula:\n$$p(x;a, b) = \\frac{a(a x)^{b -1} e^{-ax}}{\\Gamma(b)} $$ En d√≥nde los par√°metros \\(a\\) y \\(b\\) y la variable \\(x\\) son n√∫meros reales positivos y \\(\\Gamma(b)\\) es la funci√≥n gamma. La Distribuci√≥n Gamma comienza en el origen de coordenadas y tiene una forma bastante flexible. Otras distribuciones son casos especiales de ella.\n# Graficando Gamma a = 2.6 # parametro de forma. gamma = stats.gamma(a) x = np.linspace(gamma.ppf(0.01), gamma.ppf(0.99), 100) fp = gamma.pdf(x) # Funci√≥n de Probabilidad plt.plot(x, fp) plt.title(\u0026#39;Distribuci√≥n Gamma\u0026#39;) plt.ylabel(\u0026#39;probabilidad\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.show() # histograma aleatorios = gamma.rvs(1000) # genera aleatorios cuenta, cajas, ignorar = plt.hist(aleatorios, 20) plt.ylabel(\u0026#39;frequencia\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.title(\u0026#39;Histograma Gamma\u0026#39;) plt.show() Distribuci√≥n Beta# La Distribuci√≥n Beta esta dada por la formula:\n$$p(x;p, q) = \\frac{1}{B(p, q)} x^{p-1}(1 - x)^{q-1} $$ En d√≥nde los par√°metros \\(p\\) y \\(q\\) son n√∫meros reales positivos, la variable \\(x\\) satisface la condici√≥n \\(0 \\le x \\le 1\\) y \\(B(p, q)\\) es la funci√≥n beta. Las aplicaciones de la Distribuci√≥n Beta incluyen el modelado de variables aleatorias que tienen un rango finito de \\(a\\) hasta \\(b\\). Un ejemplo de ello es la distribuci√≥n de los tiempos de actividad en las redes de proyectos. La Distribuci√≥n Beta se utiliza tambi√©n con frecuencia como una probabilidad a priori para proporciones [binomiales]((https://es.wikipedia.org/wiki/Distribuci%C3%B3n_binomial) en el an√°lisis bayesiano.\n# Graficando Beta a, b = 2.3, 0.6 # parametros de forma. beta = stats.beta(a, b) x = np.linspace(beta.ppf(0.01), beta.ppf(0.99), 100) fp = beta.pdf(x) # Funci√≥n de Probabilidad plt.plot(x, fp) plt.title(\u0026#39;Distribuci√≥n Beta\u0026#39;) plt.ylabel(\u0026#39;probabilidad\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.show() # histograma aleatorios = beta.rvs(1000) # genera aleatorios cuenta, cajas, ignorar = plt.hist(aleatorios, 20) plt.ylabel(\u0026#39;frequencia\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.title(\u0026#39;Histograma Beta\u0026#39;) plt.show() Distribuci√≥n Chi cuadrado# La Distribuci√≥n Chi cuadrado esta dada por la funci√≥n:\n$$p(x; n) = \\frac{\\left(\\frac{x}{2}\\right)^{\\frac{n}{2}-1} e^{\\frac{-x}{2}}}{2\\Gamma \\left(\\frac{n}{2}\\right)} $$ En d√≥nde la variable \\(x \\ge 0\\) y el par√°metro \\(n\\), el n√∫mero de grados de libertad, es un n√∫mero entero positivo. Una importante aplicaci√≥n de la Distribuci√≥n Chi cuadrado es que cuando un conjunto de datos es representado por un modelo te√≥rico, esta distribuci√≥n puede ser utilizada para controlar cuan bien se ajustan los valores predichos por el modelo, y los datos realmente observados.\n# Graficando Chi cuadrado df = 34 # parametro de forma. chi2 = stats.chi2(df) x = np.linspace(chi2.ppf(0.01), chi2.ppf(0.99), 100) fp = chi2.pdf(x) # Funci√≥n de Probabilidad plt.plot(x, fp) plt.title(\u0026#39;Distribuci√≥n Chi cuadrado\u0026#39;) plt.ylabel(\u0026#39;probabilidad\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.show() # histograma aleatorios = chi2.rvs(1000) # genera aleatorios cuenta, cajas, ignorar = plt.hist(aleatorios, 20) plt.ylabel(\u0026#39;frequencia\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.title(\u0026#39;Histograma Chi cuadrado\u0026#39;) plt.show() Distribuci√≥n T de Student# La Distribuci√≥n t de Student esta dada por la funci√≥n:\n$$p(t; n) = \\frac{\\Gamma(\\frac{n+1}{2})}{\\sqrt{n\\pi}\\Gamma(\\frac{n}{2})} \\left( 1 + \\frac{t^2}{2} \\right)^{-\\frac{n+1}{2}} $$ En d√≥nde la variable \\(t\\) es un n√∫mero real y el par√°metro \\(n\\) es un n√∫mero entero positivo. La Distribuci√≥n t de Student es utilizada para probar si la diferencia entre las medias de dos muestras de observaciones es estad√≠sticamente significativa. Por ejemplo, las alturas de una muestra aleatoria de los jugadores de baloncesto podr√≠a compararse con las alturas de una muestra aleatoria de jugadores de f√∫tbol; esta distribuci√≥n nos podr√≠a ayudar a determinar si un grupo es significativamente m√°s alto que el otro.\n# Graficando t de Student df = 50 # parametro de forma. t = stats.t(df) x = np.linspace(t.ppf(0.01), t.ppf(0.99), 100) fp = t.pdf(x) # Funci√≥n de Probabilidad plt.plot(x, fp) plt.title(\u0026#39;Distribuci√≥n t de Student\u0026#39;) plt.ylabel(\u0026#39;probabilidad\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.show() # histograma aleatorios = t.rvs(1000) # genera aleatorios cuenta, cajas, ignorar = plt.hist(aleatorios, 20) plt.ylabel(\u0026#39;frequencia\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.title(\u0026#39;Histograma t de Student\u0026#39;) plt.show() Distribuci√≥n de Pareto# La Distribuci√≥n de Pareto esta dada por la funci√≥n:\n$$p(x; \\alpha, k) = \\frac{\\alpha k^{\\alpha}}{x^{\\alpha + 1}} $$ En d√≥nde la variable \\(x \\ge k\\) y el par√°metro \\(\\alpha \u0026gt; 0\\) son n√∫meros reales. Esta distribuci√≥n fue introducida por su inventor, Vilfredo Pareto, con el fin de explicar la distribuci√≥n de los salarios en la sociedad. La Distribuci√≥n de Pareto se describe a menudo como la base de la regla 80/20. Por ejemplo, el 80% de las quejas de los clientes con respecto al funcionamiento de su veh√≠culo por lo general surgen del 20% de los componentes.\n# Graficando Pareto k = 2.3 # parametro de forma. pareto = stats.pareto(k) x = np.linspace(pareto.ppf(0.01), pareto.ppf(0.99), 100) fp = pareto.pdf(x) # Funci√≥n de Probabilidad plt.plot(x, fp) plt.title(\u0026#39;Distribuci√≥n de Pareto\u0026#39;) plt.ylabel(\u0026#39;probabilidad\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.show() # histograma aleatorios = pareto.rvs(1000) # genera aleatorios cuenta, cajas, ignorar = plt.hist(aleatorios, 20) plt.ylabel(\u0026#39;frequencia\u0026#39;) plt.xlabel(\u0026#39;valores\u0026#39;) plt.title(\u0026#39;Histograma de Pareto\u0026#39;) plt.show() ¬øC√≥mo elegir la distribuci√≥n que mejor se ajusta a mis datos?# Ahora ya tenemos un conocimiento general de las principales distribuciones con que nos podemos encontrar; pero ¬øc√≥mo determinamos que distribuci√≥n debemos utilizar?\nUn modelo que podemos seguir cuando nos encontramos con datos que necesitamos ajustar a una distribuci√≥n, es comenzar con los datos sin procesar y responder a cuatro preguntas b√°sicas acerca de los mismos, que nos pueden ayudar a caracterizarlos. La primer pregunta se refiere a si los datos pueden tomar valores discretos o continuos. La segunda pregunta que nos debemos hacer, hace referencia a la simetr√≠a de los datos y si hay asimetr√≠a, en qu√© direcci√≥n se encuentra; en otras palabras, son los valores at√≠picos positivos y negativos igualmente probables o es uno m√°s probable que el otro. La tercer pregunta abarca los l√≠mites superiores e inferiores en los datos; hay algunos datos, como los ingresos, que no pueden ser inferiores a cero, mientras que hay otros, como los m√°rgenes de operaci√≥n que no puede exceder de un valor (100%). La √∫ltima pregunta se refiere a la posibilidad de observar valores extremos en la distribuci√≥n; en algunos casos, los valores extremos ocurren con muy poca frecuencia, mientras que en otros, se producen con mayor frecuencia. Este proceso, lo podemos resumir en el siguiente gr√°fico:\nCon la ayuda de estas preguntas fundamentales, m√°s el conocimiento de las distintas distribuciones deber√≠amos estar en condiciones de poder caracterizar cualquier conjunto de datos.\nCon esto concluyo este tour por las principales distribuciones utilizadas en estad√≠stica. Para m√°s informaci√≥n tambi√©n pueden visitar mi art√≠culo Probabilidad y Estad√≠stica con Python o la categor√≠a estad√≠stica del blog. Espero les resulte √∫til.\nSaludos!\nEste post fue escrito utilizando Jupyter notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2016-06-29","id":19,"permalink":"/blog/2016/06/29/distribuciones-de-probabilidad-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Las variables aleatorias han llegado a desempe√±ar un papel importante en casi todos los campos de estudio: en la F√≠sica, la Qu√≠mica y la Ingenier√≠a; y especialmente en las ciencias biol√≥gicas y sociales. Estas variables aleatorias son medidas y analizadas en t√©rminos de sus propiedades estad√≠sticas y probabil√≠sticas, de las cuales una caracter√≠stica subyacente es su funci√≥n de distribuci√≥n.","tags":["python","programacion","estadistica","probabilidad","distribuciones","analisis de datos"],"title":"Distribuciones de probabilidad con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, Analisis de datos y Python. El contenido esta bajo la licencia BSD.\nSobre TensoFlow# TensorFlow es una biblioteca open source desarrollada por Google que nos permite realizar c√°lculos num√©ricos usando diagramas de flujo de datos. Los nodos del grafo representan operaciones matem√°ticas, mientras que los arcos del grafo representan los arreglos de datos multidimensionales (tensores) comunicados entre ellos. Esta arquitectura flexible nos permite realizar los c√°lculos en m√°s de un CPU o GPU utilizando la misma API.\n¬øQu√© es un diagrama de flujo de datos?# Los diagramas de flujo de datos describen c√°lculos matem√°ticos con un grafo de nodos y arcos. Los nodos normalmente implementan operaciones matem√°ticas, pero tambi√©n pueden representar los puntos para alimentarse de datos, devolver resultados, o leer / escribir variables persistentes. Los arcos o aristas describen las relaciones de entrada / salida entre los nodos. Estos arcos est√°n representados por los arreglos de datos multidimensionales o tensores. El flujo de los tensores a trav√©s del grafo es de donde TensorFlow recibe su nombre. Los nodos se asignan a los dispositivos computacionales y se ejecutan de forma asincr√≥nica y en paralelo una vez que todos los tensores en los arcos de entrada est√°n disponibles.\nIntroducci√≥n a TensorFlow# Para poder utilizar TensorFlow primero es necesario entender c√≥mo la librer√≠a:\nRepresenta c√°lculos en forma de grafos. Ejecuta los grafos en el contexto de Sesiones. Representa los datos como tensores. Mantiene el estado con variables. Se alimenta de datos y devuelve los resultados de cada operaci√≥n. Funcionamiento general# TensorFlow es un sistema de programaci√≥n en el que representamos c√°lculos en forma de grafos. Los nodos en el grafo se llaman ops (abreviatura de operaciones). Una op tiene cero o m√°s tensores, realiza alg√∫n c√°lculo, y produce cero o m√°s tensores.\nUn grafo de TensorFlow es una descripci√≥n de c√°lculos. Para calcular cualquier cosa dentro de TensorFlow, el grafo debe ser lanzado dentro de una sesi√≥n. La Sesi√≥n coloca las operaciones del grafo en los diferentes dispositivos, tales como CPU o GPU, y proporciona m√©todos para ejecutarlas.\nCreando un Grafo# Para construir un grafo simple, podemos comenzar con ops que no necesitan ning√∫n dato de entrada, como son las constantes y luego le pasamos su salida a ops que realizan c√°lculos.\n# importamos la libreria import tensorflow as tf # importamos librer√≠as adicionales import numpy as np import matplotlib.pyplot as plt import matplotlib.cm as cm import pandas as pd %matplotlib inline Constantes# Podemos construir ops de constantes utilizando constant, su API es bastante simple:\nconstant(value, dtype=None, shape=None, name='Const')\nLe debemos pasar un valor, el cual puede ser cualquier tipo de tensor (un escalar, un vector, una matriz, etc) y luego opcionalmente le podemos pasar el tipo de datos, la forma y un nombre.\n# Creaci√≥n de Constantes # El valor que retorna el constructor es el valor de la constante. # creamos constantes a=2 y b=3 a = tf.constant(2) b = tf.constant(3) # creamos matrices de 3x3 matriz1 = tf.constant([[1, 3, 2], [1, 0, 0], [1, 2, 2]]) matriz2 = tf.constant([[1, 0, 5], [7, 5, 0], [2, 1, 1]]) # Realizamos algunos c√°lculos con estas constantes suma = tf.add(a, b) mult = tf.mul(a, b) cubo_a = a**3 # suma de matrices suma_mat = tf.add(matriz1, matriz2) # producto de matrices mult_mat = tf.matmul(matriz1, matriz2) Sesiones# Ahora que ya definimos algunas ops constantes y algunos c√°lculos con ellas, debemos lanzar el grafo dentro de una Sesi√≥n. Para realizar esto utilizamos el objeto Session. Este objeto va a encapsular el ambiente en el que las operaciones que definimos en el grafo van a ser ejecutadas y los tensores son evaluados.\n# Todo en TensorFlow ocurre dentro de una Sesi√≥n # creamos la sesion y realizamos algunas operaciones con las constantes # y lanzamos la sesi√≥n with tf.Session() as sess: print(\u0026#34;Suma de las constantes: {}\u0026#34;.format(sess.run(suma))) print(\u0026#34;Multiplicaci√≥n de las constantes: {}\u0026#34;.format(sess.run(mult))) print(\u0026#34;Constante elevada al cubo: {}\u0026#34;.format(sess.run(cubo_a))) print(\u0026#34;Suma de matrices: \\n{}\u0026#34;.format(sess.run(suma_mat))) print(\u0026#34;Producto de matrices: \\n{}\u0026#34;.format(sess.run(mult_mat))) Suma de las constantes: 5 Multiplicaci√≥n de las constantes: 6 Constante elevada al cubo: 8 Suma de matrices: [[2 3 7] [8 5 0] [3 3 3]] Producto de matrices: [[26 17 7] [ 1 0 5] [19 12 7]] Las Sesiones deben ser cerradas para liberar los recursos, por lo que es una buena pr√°ctica incluir la Sesi√≥n dentro de un bloque \u0026ldquo;with\u0026rdquo; que la cierra autom√°ticamente cuando el bloque termina de ejecutar.\nPara ejecutar las operaciones y evaluar los tensores utilizamos Session.run().\nVariables persistentes# Las Variables mantienen el estado a trav√©s de las ejecuciones del grafo. Son buffers en memoria que contienen tensores. Se deben inicializar expl√≠citamente y se pueden guardar en el disco para luego restaurar su estado de necesitarlo. Se crean utilizando el objeto Variable.\n# Creamos una variable y la inicializamos con 0 estado = tf.Variable(0, name=\u0026#34;contador\u0026#34;) # Creamos la op que le va a sumar uno a la Variable `estado`. uno = tf.constant(1) nuevo_valor = tf.add(estado, uno) actualizar = tf.assign(estado, nuevo_valor) # Las Variables deben ser inicializadas por la operaci√≥n `init` luego de # lanzar el grafo. Debemos agregar la op `init` a nuestro grafo. init = tf.initialize_all_variables() # Lanzamos la sesion y ejecutamos las operaciones with tf.Session() as sess: # Ejecutamos la op `init` sess.run(init) # imprimir el valor de la Variable estado. print(sess.run(estado)) # ejecutamos la op que va a actualizar a `estado`. for _ in range(3): sess.run(actualizar) print(sess.run(estado)) 0 1 2 3 Variables simb√≥licas (contenedores)# Las Variables simb√≥licas o Contenedores nos van a permitir alimentar a las operaciones con los datos durante la ejecuci√≥n del grafo. Estos contenedores deben ser alimentados antes de ser evaluados en la sesi√≥n, sino obtendremos un error.\n# Ejemplo variables simb√≥licas en los grafos # El valor que devuelve el constructor representa la salida de la # variable (la entrada de la variable se define en la sesion) # Creamos un contenedor del tipo float. Un tensor de 4x4. x = tf.placeholder(tf.float32, shape=(4, 4)) y = tf.matmul(x, x) with tf.Session() as sess: # print(sess.run(y)) # ERROR: va a fallar porque no alimentamos a x. rand_array = np.random.rand(4, 4) print(sess.run(y, feed_dict={x: rand_array})) # ahora esta correcto. [[ 2.27301431 2.39163661 1.22738445 1.87839973] [ 2.66718912 2.76533985 1.08909523 1.96862805] [ 2.38245845 2.37843513 1.0873785 1.67218387] [ 1.68678236 1.77147484 1.0363127 1.36901033]] Ahora ya conocemos en l√≠neas generales como es la mec√°nica detr√°s del funcionamiento de TensorFlow y como deber√≠amos proceder para crear las operaciones dentro de los grafos. Veamos si podemos implementar modelos de neuronas simples con la ayuda de esta librer√≠a.\nEjemplo de neuronas simples# Una neurona simple, va a tener una forma similar al siguiente diagrama:\nEn donde sus componentes son:\n\\(x_1, x_2, \\dots, x_n\\): son los datos de entrada en la neurona, los cuales tambi√©n puede ser que sean producto de la salida de otra neurona de la red.\n\\(x_0\\): Es la unidad de sesgo; un valor constante que se le suma a la entrada de la funci√≥n de activaci√≥n de la neurona. Generalmente tiene el valor 1. Este valor va a permitir cambiar la funci√≥n de activaci√≥n hacia la derecha o izquierda, otorg√°ndole m√°s flexibilidad para aprender a la neurona.\n\\(w_0, w_1, w_2, \\dots, w_n\\): Los pesos relativos de cada entrada. Tener en cuenta que incluso la unidad de sesgo tiene un peso.\na: La salida de la neurona. Que va a ser calculada de la siguiente forma:\n$$a = f\\left(\\sum_{i=0}^n w_i \\cdot x_i \\right)$$ Aqu√≠ \\(f\\) es la funci√≥n de activaci√≥n de la neurona. Esta funci√≥n es la que le otorga tanta flexibilidad a las redes neuronales y le permite estimar complejas relaciones no lineales en los datos. Puede ser tanto una funci√≥n lineal, una funci√≥n log√≠stica, hiperb√≥lica, etc.\nAhora que ya conocemos como se construye una neurona tratemos de implementar con este modelo las funciones l√≥gicas AND, OR y XNOR. Podemos pensar a estas funciones como un problema de clasificaci√≥n en el que la salida va a ser 0 o 1, de acuerdo a la combinaci√≥n de las diferentes entradas.\nLas podemos modelar linealmente con la siguiente funci√≥n de activaci√≥n:\n$$f(x) = \\left\\{ \\begin{array}{ll} 0 \u0026 \\mbox{si } x \u003c 0 \\\\ 1 \u0026 \\mbox{si } x \\ge 0 \\end{array} \\right.$$ Neurona AND# La neurona AND puede ser modelada con el siguiente esquema:\nLa salida de esta neurona entonces va a ser:\n$$a = f(-1.5 + x_1 + x_2)$$ Veamos como la podemos implementar en TensorFlow.\n# Neurona con TensorFlow # Defino las entradas entradas = tf.placeholder(\u0026#34;float\u0026#34;, name=\u0026#39;Entradas\u0026#39;) datos = np.array([[0, 0] ,[1, 0] ,[0, 1] ,[1, 1]]) # Defino las salidas uno = lambda: tf.constant(1.0) cero = lambda: tf.constant(0.0) with tf.name_scope(\u0026#39;Pesos\u0026#39;): # Definiendo pesos y sesgo pesos = tf.placeholder(\u0026#34;float\u0026#34;, name=\u0026#39;Pesos\u0026#39;) sesgo = tf.placeholder(\u0026#34;float\u0026#34;, name=\u0026#39;Sesgo\u0026#39;) with tf.name_scope(\u0026#39;Activacion\u0026#39;): # Funci√≥n de activaci√≥n activacion = tf.reduce_sum(tf.add(tf.matmul(entradas, pesos), sesgo)) with tf.name_scope(\u0026#39;Neurona\u0026#39;): # Defino la neurona def neurona(): return tf.case([(tf.less(activacion, 0.0), cero)], default=uno) # Salida a = neurona() # path de logs logs_path = \u0026#39;/tmp/tensorflow_logs/neurona\u0026#39; # Lanzar la Sesion with tf.Session() as sess: # para armar el grafo summary_writer = tf.train.SummaryWriter(logs_path, graph=sess.graph) # para armar tabla de verdad x_1 = [] x_2 = [] out = [] act = [] for i in range(len(datos)): t = datos[i].reshape(1, 2) salida, activ = sess.run([a, activacion], feed_dict={entradas: t, pesos:np.array([[1.],[1.]]), sesgo: -1.5}) # armar tabla de verdad en DataFrame x_1.append(t[0][0]) x_2.append(t[0][1]) out.append(salida) act.append(activ) tabla_info = np.array([x_1, x_2, act, out]).transpose() tabla = pd.DataFrame(tabla_info, columns=[\u0026#39;x1\u0026#39;, \u0026#39;x2\u0026#39;, \u0026#39;f(x)\u0026#39;, \u0026#39;x1 AND x2\u0026#39;]) tabla x1 x2 f(x) x1 AND x2 0 0.0 0.0 -1.5 0.0 1 1.0 0.0 -0.5 0.0 2 0.0 1.0 -0.5 0.0 3 1.0 1.0 0.5 1.0 Aqu√≠ podemos ver los datos de entrada de \\(x_1\\) y \\(x_2\\), el resultado de la funci√≥n de activaci√≥n y la decisi√≥n final que toma la neurona de acuerdo este √∫ltimo resultado. Como podemos ver en la tabla de verdad, la neurona nos dice que \\(x_1\\) and \\(x_2\\) solo es verdad cuando ambos son verdaderos, lo que es correcto.\nNeurona OR# La neurona OR puede ser modelada con el siguiente esquema:\nLa salida de esta neurona entonces va a ser:\n$$a = f(-0.5 + x_1 + x_2)$$ Como se puede ver a simple vista, el modelo de esta neurona es similar a la de la neurona AND, con el √∫nico cambio en el valor del sesgo, por lo tanto solo tendr√≠amos que cambiar ese valor en nuestro modelo anterior para crear esta nueva neurona.\n# Neurona OR, solo cambiamos el valor del sesgo with tf.Session() as sess: # para armar el grafo summary_writer = tf.train.SummaryWriter(logs_path, graph=sess.graph) # para armar tabla de verdad x_1 = [] x_2 = [] out = [] act = [] for i in range(len(datos)): t = datos[i].reshape(1, 2) salida, activ = sess.run([a, activacion], feed_dict={entradas: t, pesos:np.array([[1.],[1.]]), sesgo: -0.5}) # sesgo ahora -0.5 # armar tabla de verdad en DataFrame x_1.append(t[0][0]) x_2.append(t[0][1]) out.append(salida) act.append(activ) tabla_info = np.array([x_1, x_2, act, out]).transpose() tabla = pd.DataFrame(tabla_info, columns=[\u0026#39;x1\u0026#39;, \u0026#39;x2\u0026#39;, \u0026#39;f(x)\u0026#39;, \u0026#39;x1 OR x2\u0026#39;]) tabla x1 x2 f(x) x1 OR x2 0 0.0 0.0 -0.5 0.0 1 1.0 0.0 0.5 1.0 2 0.0 1.0 0.5 1.0 3 1.0 1.0 1.5 1.0 Como vemos, cambiando simplemente el peso del sesgo, convertimos a nuestra neurona AND en una neurona OR. Como muestra la tabla de verdad, el √∫nico caso en que \\(x_1\\) OR \\(x_2\\) es falso es cuando ambos son falsos.\nRed Neuronal XNOR# El caso de la funci√≥n XNOR, ya es m√°s complicado y no puede modelarse utilizando una sola neurona como hicimos con los ejemplos anteriores. \\(x_1\\) XNOR \\(x_2\\) va a ser verdadero cuando ambos son verdaderos o ambos son falsos, para implementar esta funci√≥n l√≥gica debemos crear una red con dos capas, la primer capa tendr√° dos neuronas cuya salida servir√° de entrada para una nueva neurona que nos dar√° el resultado final. Esta red la podemos modelar de acuerdo al siguiente esquema:\nVeamos entonces si podemos implementar este modelo en TensorFlow.\n# Red Neuronal XNOR con TensorFlow # Defino las entradas entradas = tf.placeholder(\u0026#34;float\u0026#34;, name=\u0026#39;Entradas\u0026#39;) datos = np.array([[0, 0] ,[1, 0] ,[0, 1] ,[1, 1]]) # Defino las salidas uno = lambda: tf.constant(1.0) cero = lambda: tf.constant(0.0) with tf.name_scope(\u0026#39;Pesos\u0026#39;): # Definiendo pesos y sesgo pesos = { \u0026#39;a1\u0026#39;: tf.constant([[-1.0], [-1.0]], name=\u0026#39;peso_a1\u0026#39;), \u0026#39;a2\u0026#39;: tf.constant([[1.0], [1.0]], name=\u0026#39;peso_a2\u0026#39;), \u0026#39;a3\u0026#39;: tf.constant([[1.0], [1.0]], name=\u0026#39;peso_a3\u0026#39;) } sesgo = { \u0026#39;a1\u0026#39;: tf.constant(0.5, name=\u0026#39;sesgo_a1\u0026#39;), \u0026#39;a2\u0026#39;: tf.constant(-1.5, name=\u0026#39;sesgo_a2\u0026#39;), \u0026#39;a3\u0026#39;: tf.constant(-0.5, name=\u0026#39;sesgo_a3\u0026#39;) } with tf.name_scope(\u0026#39;Red_neuronal\u0026#39;): # Defino las capas def capa1(entradas, pesos, sesgo): # activacion a1 a1 = tf.reduce_sum(tf.add(tf.matmul(entradas, pesos[\u0026#39;a1\u0026#39;]), sesgo[\u0026#39;a1\u0026#39;])) a1 = tf.case([(tf.less(a1, 0.0), cero)], default=uno) # activacion a2 a2 = tf.reduce_sum(tf.add(tf.matmul(entradas, pesos[\u0026#39;a2\u0026#39;]), sesgo[\u0026#39;a2\u0026#39;])) a2 = tf.case([(tf.less(a2, 0.0), cero)], default=uno) return a1, a2 def capa2(entradas, pesos, sesgo): # activacion a3 a3 = tf.reduce_sum(tf.add(tf.matmul(entradas, pesos[\u0026#39;a3\u0026#39;]), sesgo[\u0026#39;a3\u0026#39;])) a3 = tf.case([(tf.less(a3, 0.0), cero)], default=uno) return a3 # path de logs logs_path = \u0026#39;/tmp/tensorflow_logs/redXNOR\u0026#39; # Sesion red neuronal XNOR with tf.Session() as sess: # para armar el grafo summary_writer = tf.train.SummaryWriter(logs_path, graph=sess.graph) # para armar tabla de verdad x_1 = [] x_2 = [] out = [] for i in range(len(datos)): t = datos[i].reshape(1, 2) # obtenos resultados 1ra capa a1, a2 = sess.run(capa1(entradas, pesos, sesgo), feed_dict={entradas: t}) # pasamos resultados a la 2da capa ent_a3 = np.array([[a1, a2]]) salida = sess.run(capa2(ent_a3, pesos, sesgo)) # armar tabla de verdad en DataFrame x_1.append(t[0][0]) x_2.append(t[0][1]) out.append(salida) tabla_info = np.array([x_1, x_2, out]).transpose() tabla = pd.DataFrame(tabla_info, columns=[\u0026#39;x1\u0026#39;, \u0026#39;x2\u0026#39;, \u0026#39;x1 XNOR x2\u0026#39;]) tabla x1 x2 x1 XNOR x2 0 0.0 0.0 1.0 1 1.0 0.0 0.0 2 0.0 1.0 0.0 3 1.0 1.0 1.0 Como vemos, la red neuronal nos da el resultado correcto para la funci√≥n l√≥gica XNOR, solo es verdadera si ambos valores son verdaderos, o ambos son falsos.\nHasta aqu√≠ implementamos simples neuronas y les pasamos los valores de sus pesos y sesgo a mano; esto es sencillo para los ejemplos; pero en la vida real, si queremos utilizar redes neuronales necesitamos implementar un procesos que vaya actualizando los pesos a medida que la red vaya aprendiendo con el entrenamiento. Este proceso se conoce con el nombre de propagaci√≥n hacia atr√°s o backpropagation.\nPropagaci√≥n hacia atr√°s# La propagaci√≥n hacia atr√°s o backpropagation es un algoritmo que funciona mediante la determinaci√≥n de la p√©rdida (o error) en la salida y luego propag√°ndolo de nuevo hacia atr√°s en la red. De esta forma los pesos se van actualizando para minimizar el error resultante de cada neurona. Este algoritmo es lo que les permite a las redes neuronales aprender.\nVeamos un ejemplo de como podemos implementar una red neuronal que pueda aprender por s√≠ sola con la ayuda de TensorFlow.\nEjemplo de Perceptron multicapa para reconocer d√≠gitos escritos# En este ejemplo vamos a construir un peceptron multicapa para clasificar d√≠gitos escritos. Antes de pasar a la construcci√≥n del modelo, exploremos un poco el conjunto de datos con el que vamos a trabajar en la clasificaci√≥n.\nMNIST dataset# MNIST es un simple conjunto de datos para reconocimiento de im√°genes por computadora. Se compone de im√°genes de d√≠gitos escritos a mano como los siguientes:\nPara m√°s informaci√≥n sobre el dataset pueden visitar el siguiente enlace, en donde hacen un an√°lisis detallado del mismo.\n# importando el dataset from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(\u0026#34;MNIST_data/\u0026#34;, one_hot=True) Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes. Extracting MNIST_data/train-images-idx3-ubyte.gz Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes. Extracting MNIST_data/train-labels-idx1-ubyte.gz Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes. Extracting MNIST_data/t10k-images-idx3-ubyte.gz Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes. Extracting MNIST_data/t10k-labels-idx1-ubyte.gz Explorando MNIST dataset# # forma del dataset 55000 imagenes mnist.train.images.shape (55000, 784) # cada imagen es un array de 28x28 con cada pixel # definido como escala de grises. digito1 = mnist.train.images[0].reshape((28, 28)) # visualizando el primer digito plt.imshow(digito1, cmap = cm.Greys) plt.show() # valor correcto mnist.train.labels[0].nonzero()[0][0] 7 # visualizando imagenes de 5 en 5 def visualizar_imagenes(dataset, cant_img): img_linea = 5 lineas = int(cant_img / img_linea) imagenes = [] for i in range(lineas): datos = [] for img in dataset[img_linea* i:img_linea* (i+1)]: datos.append(img.reshape((28,28))) imgs = np.hstack(datos) imagenes.append(imgs) data = np.vstack(imagenes) plt.imshow(data, cmap = cm.Greys ) plt.show() # visualizando los primeros 30 d√≠gitos plt.figure(figsize=(8, 8)) visualizar_imagenes(mnist.train.images, 30) Construyendo el perceptron multicapa# Ahora que ya conocemos los datos con los que vamos a trabajar, ya estamos en condiciones de construir el modelo. Vamos a construir un peceptron multicapa que es una de las redes neuronales m√°s simples. El modelo va a tener dos capas ocultas, que se van a activar con la funci√≥n de activaci√≥n ReLU y vamos a optimizar los pesos reduciendo la entrop√≠a cruzada utilizando el algoritmo Adam que es un m√©todo para optimizaci√≥n estoc√°stica.\n# Parametros tasa_aprendizaje = 0.001 epocas = 15 lote = 100 display_step = 1 logs_path = \u0026#34;/tmp/tensorflow_logs/perceptron\u0026#34; # Parametros de la red n_oculta_1 = 256 # 1ra capa de atributos n_oculta_2 = 256 # 2ra capa de atributos n_entradas = 784 # datos de MNIST(forma img: 28*28) n_clases = 10 # Total de clases a clasificar (0-9 digitos) # input para los grafos x = tf.placeholder(\u0026#34;float\u0026#34;, [None, n_entradas], name=\u0026#39;DatosEntrada\u0026#39;) y = tf.placeholder(\u0026#34;float\u0026#34;, [None, n_clases], name=\u0026#39;Clases\u0026#39;) # Creamos el modelo def perceptron_multicapa(x, pesos, sesgo): # Funci√≥n de activaci√≥n de la capa escondida capa_1 = tf.add(tf.matmul(x, pesos[\u0026#39;h1\u0026#39;]), sesgo[\u0026#39;b1\u0026#39;]) # activacion relu capa_1 = tf.nn.relu(capa_1) # Funci√≥n de activaci√≥n de la capa escondida capa_2 = tf.add(tf.matmul(capa_1, pesos[\u0026#39;h2\u0026#39;]), sesgo[\u0026#39;b2\u0026#39;]) # activaci√≥n relu capa_2 = tf.nn.relu(capa_2) # Salida con activaci√≥n lineal salida = tf.matmul(capa_2, pesos[\u0026#39;out\u0026#39;]) + sesgo[\u0026#39;out\u0026#39;] return salida # Definimos los pesos y sesgo de cada capa. pesos = { \u0026#39;h1\u0026#39;: tf.Variable(tf.random_normal([n_entradas, n_oculta_1])), \u0026#39;h2\u0026#39;: tf.Variable(tf.random_normal([n_oculta_1, n_oculta_2])), \u0026#39;out\u0026#39;: tf.Variable(tf.random_normal([n_oculta_2, n_clases])) } sesgo = { \u0026#39;b1\u0026#39;: tf.Variable(tf.random_normal([n_oculta_1])), \u0026#39;b2\u0026#39;: tf.Variable(tf.random_normal([n_oculta_2])), \u0026#39;out\u0026#39;: tf.Variable(tf.random_normal([n_clases])) } with tf.name_scope(\u0026#39;Modelo\u0026#39;): # Construimos el modelo pred = perceptron_multicapa(x, pesos, sesgo) with tf.name_scope(\u0026#39;Costo\u0026#39;): # Definimos la funcion de costo costo = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y)) with tf.name_scope(\u0026#39;optimizador\u0026#39;): # Algoritmo de optimizaci√≥n optimizar = tf.train.AdamOptimizer( learning_rate=tasa_aprendizaje).minimize(costo) with tf.name_scope(\u0026#39;Precision\u0026#39;): # Evaluar el modelo pred_correcta = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1)) # Calcular la precisi√≥n Precision = tf.reduce_mean(tf.cast(pred_correcta, \u0026#34;float\u0026#34;)) # Inicializamos todas las variables init = tf.initialize_all_variables() # Crear sumarizaci√≥n para controlar el costo tf.scalar_summary(\u0026#34;Costo\u0026#34;, costo) # Crear sumarizaci√≥n para controlar la precisi√≥n tf.scalar_summary(\u0026#34;Precision\u0026#34;, Precision) # Juntar los resumenes en una sola operaci√≥n merged_summary_op = tf.merge_all_summaries() # Lanzamos la sesi√≥n with tf.Session() as sess: sess.run(init) # op to write logs to Tensorboard summary_writer = tf.train.SummaryWriter( logs_path, graph=tf.get_default_graph()) # Entrenamiento for epoca in range(epocas): avg_cost = 0. lote_total = int(mnist.train.num_examples/lote) for i in range(lote_total): lote_x, lote_y = mnist.train.next_batch(lote) # Optimizaci√≥n por backprop y funcion de costo _, c, summary = sess.run([optimizar, costo, merged_summary_op], feed_dict={x: lote_x, y: lote_y}) # escribir logs en cada iteracion summary_writer.add_summary(summary, epoca * lote_total + i) # perdida promedio avg_cost += c / lote_total # imprimir informaci√≥n de entrenamiento if epoca % display_step == 0: print(\u0026#34;Iteraci√≥n: {0: 04d} costo = {1:.9f}\u0026#34;.format(epoca+1, avg_cost)) print(\u0026#34;Optimizaci√≥n Terminada!\\n\u0026#34;) print(\u0026#34;Precisi√≥n: {0:.2f}\u0026#34;.format(Precision.eval({x: mnist.test.images, y: mnist.test.labels}))) print(\u0026#34;Ejecutar el comando:\\n\u0026#34;, \u0026#34;--\u0026gt; tensorboard --logdir=/tmp/tensorflow_logs \u0026#34;, \u0026#34;\\nLuego abir https://0.0.0.0:6006/ en el navegador\u0026#34;) Iteraci√≥n: 001 costo = 190.739139247 Iteraci√≥n: 002 costo = 42.639138275 Iteraci√≥n: 003 costo = 26.239370855 Iteraci√≥n: 004 costo = 18.236157751 Iteraci√≥n: 005 costo = 13.129509245 Iteraci√≥n: 006 costo = 9.765473726 Iteraci√≥n: 007 costo = 7.159448563 Iteraci√≥n: 008 costo = 5.309303818 Iteraci√≥n: 009 costo = 3.940411947 Iteraci√≥n: 010 costo = 2.904317733 Iteraci√≥n: 011 costo = 2.179349244 Iteraci√≥n: 012 costo = 1.597618810 Iteraci√≥n: 013 costo = 1.215200688 Iteraci√≥n: 014 costo = 0.875238173 Iteraci√≥n: 015 costo = 0.760177279 Optimizaci√≥n Terminada! Precisi√≥n: 0.94 Ejecutar el comando: --\u0026gt; tensorboard --logdir=/tmp/tensorflow_logs Luego abir https://0.0.0.0:6006/ en el navegador Como vemos TensorFlow nos da mucha flexibilidad para construir el modelo, modificando muy pocas l√≠neas podr√≠amos cambiar el algoritmo de optimizaci√≥n o el calculo del error y obtener otros resultados; de esta forma vamos a poder personalizar el modelo para alcanzar mayores niveles de precisi√≥n.\nTensorBoard# Otra gran herramienta que nos proporciona TensorFlow es TensorBoard que nos permite visualizar nuestros grafos y nos ayudan a alcanzar un mayor entendimiento del flujo de c√°lculos que ocurre en nuestro modelo.\nPara crear la informaci√≥n de la que se va a nutrir el TensorBoard, podemos definir algunos scopes utilizando tf.name_scope; tambi√©n podemos incluir algunos gr√°ficos sumarizados con tf.scalar_summary y luego llamamos a la funci√≥n tf.train.SummaryWriter dentro de una Sesi√≥n.\nLuego podemos iniciar el board con el comando tensorboard --logdir=logpath como se puede ver en la salida del √∫ltimo ejemplo.\nLos grafos de los casos que vimos por ejemplo, se ven as√≠.\nLos invito a explorar la herramienta y adentrarse en el fascinante mundo de las redes neuronales.\nSaludos!\nEste post fue escrito utilizando IPython notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2016-06-05","id":20,"permalink":"/blog/2016/06/05/tensorflow-y-redes-neuronales/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, Analisis de datos y Python. El contenido esta bajo la licencia BSD.\nSobre TensoFlow# TensorFlow es una biblioteca open source desarrollada por Google que nos permite realizar c√°lculos num√©ricos usando diagramas de flujo de datos. Los nodos del grafo representan operaciones matem√°ticas, mientras que los arcos del grafo representan los arreglos de datos multidimensionales (tensores) comunicados entre ellos.","tags":["python","programacion","analisis de datos","machine learning","redes neuronales","tensorflow"],"title":"TensorFlow y Redes Neuronales"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, Analisis de datos y Python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Uno de los conceptos m√°s importantes en Machine Learning es el overfitting o sobreajuste del modelo. Comprender como un modelo se ajusta a los datos es muy importante para entender las causas de baja precisi√≥n en las predicciones. Un modelo va a estar sobreajustado cuando vemos que se desempe√±a bien con los datos de entrenamiento, pero su precisi√≥n es notablemente m√°s baja con los datos de evaluaci√≥n; esto se debe a que el modelo ha memorizado los datos que ha visto y no pudo generalizar las reglas para predecir los datos que no ha visto. De aqu√≠ tambi√©n la importancia de siempre contar con dos conjuntos de datos distintos, uno para entrenar el modelo y otro para evaluar su precisi√≥n; ya que si utilizamos el mismo dataset para las dos tareas, no tendr√≠amos forma de determinar como el modelo se comporta con datos que nunca ha visto.\n¬øC√≥mo reconocer el sobreajuste?# En l√≠neas generales el sobreajuste va a estar relacionado con la complejidad del modelo, mientras m√°s complejidad le agreguemos, mayor va a ser la tendencia a sobreajustarse a los datos, ya que va a contar con mayor flexibilidad para realizar las predicciones y puede ser que los patrones que encuentre est√©n relacionados con el ruido (peque√±os errores aleatorios) en los datos y no con la verdadera se√±al o relaci√≥n subyacente.\nNo existe una regla general para establecer cual es el nivel ideal de complejidad que le podemos otorgar a nuestro modelo sin caer en el sobreajuste; pero podemos valernos de algunas herramientas anal√≠ticas para intentar entender como el modelo se ajusta a los datos y reconocer el sobreajuste. Veamos un ejemplo.\n√Årboles de Decisi√≥n y sobreajuste# Los √Årboles de Decisi√≥n pueden ser muchas veces una herramienta muy precisa, pero tambi√©n con mucha tendencia al sobreajuste. Para construir estos modelos aplicamos un procedimiento recursivo para encontrar los atributos que nos proporcionan m√°s informaci√≥n sobre distintos subconjuntos de datos, cada vez m√°s peque√±os. Si aplicamos este procedimiento en forma reiterada, eventualmente podemos llegar a un √°rbol en el que cada hoja tenga una sola instancia de nuestra variable objetivo a clasificar. En este caso extremo, el √Årbol de Decisi√≥n va a tener una pobre generalizaci√≥n y estar bastante sobreajustado; ya que cada instancia de los datos de entrenamiento va a encontrar el camino que lo lleve eventualmente a la hoja que lo contiene, alcanzando as√≠ una precisi√≥n del 100% con los datos de entrenamiento. Veamos un ejemplo sencillo con la ayuda de Python.\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # Importando las librer√≠as que vamos a utilizar import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.cross_validation import train_test_split from sklearn.datasets import make_classification from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier import random; random.seed(1982) # graficos incrustados %matplotlib inline # parametros esteticos de seaborn sns.set_palette(\u0026#34;deep\u0026#34;, desat=.6) sns.set_context(rc={\u0026#34;figure.figsize\u0026#34;: (8, 4)}) # Ejemplo en python - √°rboles de decisi√≥n # dummy data con 100 atributos y 2 clases X, y = make_classification(10000, 100, n_informative=3, n_classes=2, random_state=1982) # separ los datos en train y eval x_train, x_eval, y_train, y_eval = train_test_split(X, y, test_size=0.35, train_size=0.65, random_state=1982) # creando el modelo sin control de profundidad, va a continuar hasta # que todas las hojas sean puras arbol = DecisionTreeClassifier(criterion=\u0026#39;entropy\u0026#39;) # Ajustando el modelo arbol.fit(x_train, y_train) DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None, max_features=None, max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best') # precisi√≥n del modelo en datos de entrenamiento. print(\u0026#34;precisi√≥n entranamiento: {0: .2f}\u0026#34;.format( arbol.score(x_train, y_train))) precisi√≥n entranamiento: 1.00 Logramos una precisi√≥n del 100 %, incre√≠ble, este modelo no se equivoca! deber√≠amos utilizarlo para jugar a la loter√≠a y ver si ganamos algunos millones; o tal vez, no?. Veamos como se comporta con los datos de evaluaci√≥n.\n# precisi√≥n del modelo en datos de evaluaci√≥n. print(\u0026#34;precisi√≥n evaluaci√≥n: {0: .2f}\u0026#34;.format( arbol.score(x_eval, y_eval))) precisi√≥n evaluaci√≥n: 0.87 Ah, ahora nuestro modelo ya no se muestra tan preciso, esto se debe a que seguramente esta sobreajustado, ya que dejamos crecer el √°rbol hasta que cada hoja estuviera pura (es decir que solo contenga datos de una sola de las clases a predecir). Una alternativa para reducir el sobreajuste y ver si podemos lograr que generalice mejor y por tanto tenga m√°s precisi√≥n para datos nunca vistos, es tratar de reducir la complejidad del modelo por medio de controlar la profundidad que puede alcanzar el √Årbol de Decisi√≥n.\n# profundidad del arbol de decisi√≥n. arbol.tree_.max_depth 22 Este caso nuestro modelo tiene una profundidad de 22 nodos; veamos si reduciendo esa cantidad podemos mejorar la precisi√≥n en los datos de evaluaci√≥n. Por ejemplo, pongamos un m√°ximo de profundidad de tan solo 5 nodos.\n# modelo dos, con control de profundiad de 5 nodos arbol2 = DecisionTreeClassifier(criterion=\u0026#39;entropy\u0026#39;, max_depth=5) # Ajustando el modelo arbol2.fit(x_train, y_train) DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5, max_features=None, max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best') # precisi√≥n del modelo en datos de entrenamiento. print(\u0026#34;precisi√≥n entranamiento: {0: .2f}\u0026#34;.format( arbol2.score(x_train, y_train))) precisi√≥n entranamiento: 0.92 Ahora podemos ver que ya no tenemos un modelo con 100% de precisi√≥n en los datos de entrenamiento, sino que la precisi√≥n es bastante inferior, 92%, sin embargo si ahora medimos la precisi√≥n con los datos de evaluaci√≥n vemos que la precisi√≥n es del 90%, 3 puntos por arriba de lo que hab√≠amos conseguido con el primer modelo que nunca se equivocaba en los datos de entrenamiento.\n# precisi√≥n del modelo en datos de evaluaci√≥n. print(\u0026#34;precisi√≥n evaluaci√≥n: {0: .2f}\u0026#34;.format( arbol2.score(x_eval, y_eval))) precisi√≥n evaluaci√≥n: 0.90 Esta diferencia se debe a que reducimos la complejidad del modelo para intentar ganar en generalizaci√≥n. Tambi√©n debemos tener en cuenta que si seguimos reduciendo la complejidad, podemos crear un modelo demasiado simple que en vez de estar sobreajustado puede tener un desempe√±o muy por debajo del que podr√≠a tener; podr√≠amos decir que el modelo estar√≠a infraajustado y tendr√≠a un alto nivel de sesgo. Para ayudarnos a encontrar el t√©rmino medio entre la complejidad del modelo y su ajuste a los datos, podemos ayudarnos de herramientas gr√°ficas. Por ejemplo podr√≠amos crear diferentes modelos, con distintos grados de complejidad y luego graficar la precisi√≥n en funci√≥n de la complejidad.\n# Grafico de ajuste del √°rbol de decisi√≥n train_prec = [] eval_prec = [] max_deep_list = list(range(3, 23)) for deep in max_deep_list: arbol3 = DecisionTreeClassifier(criterion=\u0026#39;entropy\u0026#39;, max_depth=deep) arbol3.fit(x_train, y_train) train_prec.append(arbol3.score(x_train, y_train)) eval_prec.append(arbol3.score(x_eval, y_eval)) # graficar los resultados. plt.plot(max_deep_list, train_prec, color=\u0026#39;r\u0026#39;, label=\u0026#39;entrenamiento\u0026#39;) plt.plot(max_deep_list, eval_prec, color=\u0026#39;b\u0026#39;, label=\u0026#39;evaluacion\u0026#39;) plt.title(\u0026#39;Grafico de ajuste arbol de decision\u0026#39;) plt.legend() plt.ylabel(\u0026#39;precision\u0026#39;) plt.xlabel(\u0026#39;cant de nodos\u0026#39;) plt.show() El gr√°fico que acabamos de construir se llama gr√°fico de ajuste y muestra la precisi√≥n del modelo en funci√≥n de su complejidad. En nuestro ejemplo, podemos ver que el punto con mayor precisi√≥n, en los datos de evaluaci√≥n, lo obtenemos con un nivel de profundidad de aproximadamente 5 nodos; a partir de all√≠ el modelo pierde en generalizaci√≥n y comienza a estar sobreajustado. Tambi√©n podemos crear un gr√°fico similar con la ayuda de Scikit-learn, utilizando validation_curve.\n# utilizando validation curve de sklearn from sklearn.learning_curve import validation_curve train_prec, eval_prec = validation_curve(estimator=arbol, X=x_train, y=y_train, param_name=\u0026#39;max_depth\u0026#39;, param_range=max_deep_list, cv=5) train_mean = np.mean(train_prec, axis=1) train_std = np.std(train_prec, axis=1) test_mean = np.mean(eval_prec, axis=1) test_std = np.std(eval_prec, axis=1) # graficando las curvas plt.plot(max_deep_list, train_mean, color=\u0026#39;r\u0026#39;, marker=\u0026#39;o\u0026#39;, markersize=5, label=\u0026#39;entrenamiento\u0026#39;) plt.fill_between(max_deep_list, train_mean + train_std, train_mean - train_std, alpha=0.15, color=\u0026#39;r\u0026#39;) plt.plot(max_deep_list, test_mean, color=\u0026#39;b\u0026#39;, linestyle=\u0026#39;--\u0026#39;, marker=\u0026#39;s\u0026#39;, markersize=5, label=\u0026#39;evaluacion\u0026#39;) plt.fill_between(max_deep_list, test_mean + test_std, test_mean - test_std, alpha=0.15, color=\u0026#39;b\u0026#39;) plt.grid() plt.legend(loc=\u0026#39;center right\u0026#39;) plt.xlabel(\u0026#39;Cant de nodos\u0026#39;) plt.ylabel(\u0026#39;Precision\u0026#39;) plt.show() En este gr√°fico, tambi√©n podemos ver que nuestro modelo tiene bastante varianza, representada por el √°rea esfumada.\nM√©todos para reducir el Sobreajuste# Algunas de las t√©cnicas que podemos utilizar para reducir el Sobreajuste, son:\nUtilizar validaci√≥n cruzada. Recolectar m√°s datos. Introducir una penalizaci√≥n a la complejidad con alguna t√©cnica de regularizaci√≥n. Optimizar los par√°metros del modelo con grid search. Reducir la dimensi√≥n de los datos. Aplicar t√©cnicas de selecci√≥n de atributos. Utilizar modelos ensamblados. Veamos algunos ejemplos.\nValidaci√≥n cruzada# La validaci√≥n cruzada se inicia mediante el fraccionamiento de un conjunto de datos en un n√∫mero \\(k\\) de particiones (generalmente entre 5 y 10) llamadas pliegues. La validaci√≥n cruzada luego itera entre los datos de evaluaci√≥n y entrenamiento \\(k\\) veces, de un modo particular. En cada iteraci√≥n de la validaci√≥n cruzada, un pliegue diferente se elige como los datos de evaluaci√≥n. En esta iteraci√≥n, los otros pliegues \\(k-1\\) se combinan para formar los datos de entrenamiento. Por lo tanto, en cada iteraci√≥n tenemos \\((k-1) / k\\) de los datos utilizados para el entrenamiento y \\(1 / k\\) utilizado para la evaluaci√≥n. Cada iteraci√≥n produce un modelo, y por lo tanto una estimaci√≥n del rendimiento de la generalizaci√≥n, por ejemplo, una estimaci√≥n de la precisi√≥n. Una vez finalizada la validaci√≥n cruzada, todos los ejemplos se han utilizado s√≥lo una vez para evaluar pero \\(k -1\\) veces para entrenar. En este punto tenemos estimaciones de rendimiento de todos los pliegues y podemos calcular la media y la desviaci√≥n est√°ndar de la precisi√≥n del modelo. Veamos un ejemplo\n# Ejemplo cross-validation from sklearn import cross_validation # creando pliegues kpliegues = cross_validation.StratifiedKFold(y=y_train, n_folds=10, random_state=2016) # iterando entre los plieges precision = [] for k, (train, test) in enumerate(kpliegues): arbol2.fit(x_train[train], y_train[train]) score = arbol2.score(x_train[test], y_train[test]) precision.append(score) print(\u0026#39;Pliegue: {0:}, Dist Clase: {1:}, Prec: {2:.3f}\u0026#39;.format(k+1, np.bincount(y_train[train]), score)) # imprimir promedio y desvio estandar print(\u0026#39;Precision promedio: {0: .3f} +/- {1: .3f}\u0026#39;.format(np.mean(precision), np.std(precision))) Pliegue: 1, Dist Clase: [2918 2931], Prec: 0.909 Pliegue: 2, Dist Clase: [2918 2931], Prec: 0.896 Pliegue: 3, Dist Clase: [2918 2931], Prec: 0.897 Pliegue: 4, Dist Clase: [2919 2931], Prec: 0.920 Pliegue: 5, Dist Clase: [2919 2931], Prec: 0.895 Pliegue: 6, Dist Clase: [2919 2931], Prec: 0.912 Pliegue: 7, Dist Clase: [2919 2931], Prec: 0.871 Pliegue: 8, Dist Clase: [2919 2932], Prec: 0.906 Pliegue: 9, Dist Clase: [2919 2932], Prec: 0.884 Pliegue: 10, Dist Clase: [2919 2932], Prec: 0.891 Precision promedio: 0.898 +/- 0.014 En este ejemplo, utilizamos el iterador StratifiedKFold que nos proporciona Scikit-learn. Este iterador es una versi√≥n mejorada de la validaci√≥n cruzada, ya que cada pliegue va a estar estratificado para mantener las proporciones entre las clases del conjunto de datos original, lo que suele dar mejores estimaciones del sesgo y la varianza del modelo. Tambi√©n podr√≠amos utilizar cross_val_score que ya nos proporciona los resultados de la precisi√≥n que tuvo el modelo en cada pliegue.\n# Ejemplo con cross_val_score precision = cross_validation.cross_val_score(estimator=arbol2, X=x_train, y=y_train, cv=10, n_jobs=-1) print(\u0026#39;precisiones: {}\u0026#39;.format(precision)) print(\u0026#39;Precision promedio: {0: .3f} +/- {1: .3f}\u0026#39;.format(np.mean(precision), np.std(precision))) precisiones: [ 0.906298 0.89708141 0.89708141 0.91846154 0.89538462 0.91230769 0.87076923 0.90755008 0.8844376 0.89060092] Precision promedio: 0.898 +/- 0.013 M√°s datos y curvas de aprendizaje# Muchas veces, reducir el Sobreajuste es tan f√°cil como conseguir m√°s datos, dame m√°s datos y te predecir√© el futuro!. Aunque en la vida real nunca es una tarea tan sencilla conseguir m√°s datos. Otra herramienta anal√≠tica que nos ayuda a entender como reducimos el Sobreajuste con la ayuda de m√°s datos, son las curvas de aprendizaje, las cuales grafican la precisi√≥n en funci√≥n del tama√±o de los datos de entrenamiento. Veamos como podemos graficarlas con la ayuda de Python.\n# Ejemplo Curvas de aprendizaje from sklearn.learning_curve import learning_curve train_sizes, train_scores, test_scores = learning_curve(estimator=arbol2, X=x_train, y=y_train, train_sizes=np.linspace(0.1, 1.0, 10), cv=10, n_jobs=-1) train_mean = np.mean(train_scores, axis=1) train_std = np.std(train_scores, axis=1) test_mean = np.mean(test_scores, axis=1) test_std = np.std(test_scores, axis=1) # graficando las curvas plt.plot(train_sizes, train_mean, color=\u0026#39;r\u0026#39;, marker=\u0026#39;o\u0026#39;, markersize=5, label=\u0026#39;entrenamiento\u0026#39;) plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color=\u0026#39;r\u0026#39;) plt.plot(train_sizes, test_mean, color=\u0026#39;b\u0026#39;, linestyle=\u0026#39;--\u0026#39;, marker=\u0026#39;s\u0026#39;, markersize=5, label=\u0026#39;evaluacion\u0026#39;) plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color=\u0026#39;b\u0026#39;) plt.grid() plt.title(\u0026#39;Curva de aprendizaje\u0026#39;) plt.legend(loc=\u0026#39;upper right\u0026#39;) plt.xlabel(\u0026#39;Cant de ejemplos de entrenamiento\u0026#39;) plt.ylabel(\u0026#39;Precision\u0026#39;) plt.show() En este gr√°fico podemos ver claramente como con pocos datos la precisi√≥n entre los datos de entrenamiento y los de evaluaci√≥n son muy distintas y luego a medida que la cantidad de datos va aumentando, el modelo puede generalizar mucho mejor y las precisiones se comienzan a emparejar. Este gr√°fico tambi√©n puede ser importante a la hora de decidir invertir en la obtenci√≥n de m√°s datos, ya que por ejemplo nos indica que a partir las 2500 muestras, el modelo ya no gana mucha m√°s precisi√≥n a pesar de obtener m√°s datos.\nOptimizaci√≥n de par√°metros con Grid Search# La mayor√≠a de los modelos de Machine Learning cuentan con varios par√°metros para ajustar su comportamiento, por lo tanto otra alternativa que tenemos para reducir el Sobreajuste es optimizar estos par√°metros por medio de un proceso conocido como grid search e intentar encontrar la combinaci√≥n ideal que nos proporcione mayor precisi√≥n. El enfoque que utiliza grid search es bastante simple, se trata de una b√∫squeda exhaustiva por el paradigma de fuerza bruta en el que se especifica una lista de valores para diferentes par√°metros, y la computadora eval√∫a el rendimiento del modelo para cada combinaci√≥n de √©stos par√°metros para obtener el conjunto √≥ptimo que nos brinda el mayor rendimiento.\nVeamos un ejemplo utilizando un modelo de SVM o M√°quinas de vectores de soporte, la idea va a ser optimizar los par√°metros gamma y C de este modelo. El par√°metro gamma define cuan lejos llega la influencia de un solo ejemplo de entrenamiento, con valores bajos que significan \u0026ldquo;lejos\u0026rdquo; y los valores altos significan \u0026ldquo;cerca\u0026rdquo;. El par√°metro C es el que establece la penalizaci√≥n por error en la clasificaci√≥n un valor bajo de este par√°metro hace que la superficie de decisi√≥n sea m√°s lisa, mientras que un valor alto tiene como objetivo que todos los ejemplos se clasifiquen correctamente, d√°ndole m√°s libertad al modelo para elegir m√°s ejemplos como vectores de soporte. Tengan en cuenta que como todo proceso por fuerza bruta, puede tomar bastante tiempo seg√∫n la cantidad de par√°metros que utilicemos para la optimizaci√≥n.\n# Ejemplo de grid search con SVM. from sklearn.grid_search import GridSearchCV # creaci√≥n del modelo svm = SVC(random_state=1982) # rango de parametros rango_C = np.logspace(-2, 10, 10) rango_gamma = np.logspace(-9, 3, 10) param_grid = dict(gamma=rango_gamma, C=rango_C) # crear grid search gs = GridSearchCV(estimator=svm, param_grid=param_grid, scoring=\u0026#39;accuracy\u0026#39;, cv=5,n_jobs=-1) # comenzar el ajuste gs = gs.fit(x_train, y_train) # imprimir resultados print(gs.best_score_) print(gs.best_params_) 0.870461538462 {'C': 4.6415888336127775, 'gamma': 0.0046415888336127729} # utilizando el mejor modelo mejor_modelo = gs.best_estimator_ mejor_modelo.fit(x_train, y_train) print(\u0026#39;Precisi√≥n: {0:.3f}\u0026#39;.format(mejor_modelo.score(x_eval, y_eval))) Precisi√≥n: 0.864 En este ejemplo, primero utilizamos el objeto GridSearchCV que nos permite realizar grid search junto con validaci√≥n cruzada, luego comenzamos a ajustar el modelo con las diferentes combinaciones de los valores de los par√°metros gamma y C. Finalmente imprimimos el mejor resultado de precisi√≥n y los valores de los par√°metros que utilizamos para obtenerlos; por √∫ltimo utilizamos este mejor modelo para realizar las predicciones con los datos de evaluaci√≥n. Podemos ver que la precisi√≥n que obtuvimos con los datos de evaluaci√≥n es casi id√©ntica a la que nos indic√≥ grid search, lo que indica que el modelo generaliza muy bien.\nAqu√≠ termina este art√≠culo, sobre la selecci√≥n de atributos, pueden visitar el art√≠culo que dedique a ese tema en este link; en cuando a modelos ensamblados y reducci√≥n de dimensiones de los datos, espero escribir sobre esos temas en art√≠culos futuros, no se los pierdan!\nGracias por visitar el blog y saludos!\nEste post fue escrito utilizando IPython notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2016-05-29","id":21,"permalink":"/blog/2016/05/29/machine-learning-con-python-sobreajuste/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, Analisis de datos y Python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Uno de los conceptos m√°s importantes en Machine Learning es el overfitting o sobreajuste del modelo. Comprender como un modelo se ajusta a los datos es muy importante para entender las causas de baja precisi√≥n en las predicciones. Un modelo va a estar sobreajustado cuando vemos que se desempe√±a bien con los datos de entrenamiento, pero su precisi√≥n es notablemente m√°s baja con los datos de evaluaci√≥n; esto se debe a que el modelo ha memorizado los datos que ha visto y no pudo generalizar las reglas para predecir los datos que no ha visto.","tags":["python","estadistica","programacion","machine learning","analisis de datos","overfitting","sobreajuste"],"title":"Machine Learning con Python - Sobreajuste"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, Analisis de datos y Python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Continuando donde nos quedamos en el art√≠culo anterior Ejemplo de Machine Learning - preprocesamiento y exploraci√≥n; ahora es tiempo de ingresar en el terreno de la selecci√≥n de atributos.\n¬øEn qu√© consiste la selecci√≥n de atributos?# La selecci√≥n de atributos es el proceso por el cual seleccionamos un subconjunto de atributos (representados por cada una de las columnas en un dataset de forma tabular) que son m√°s relevantes para la construcci√≥n del modelo predictivo sobre el que estamos trabajando.\nEste proceso, no se debe confundir con el de reducci√≥n de dimensiones; si bien ambos procesos buscan reducir el n√∫mero de atributos en nuestro dataset; este √∫ltimo lo hace por medio de la creaci√≥n de nuevos atributos que son combinaciones de los anteriores; mientras que en el proceso de selecci√≥n de atributos, intentamos incluir y excluir los atributos pr√°cticamente sin modificarlos.\nEl proceso de selecci√≥n de atributos es tanto un arte como una ciencia, en donde el conocimiento sobre el problema y la intuici√≥n son sumamente importantes. El objetivo de la selecci√≥n de atributos es triple: mejorar la capacidad predictiva de nuestro modelo, proporcionando modelos predictivos m√°s r√°pidos y eficientes, y proporcionar una mejor comprensi√≥n del proceso subyacente que gener√≥ los datos. Los m√©todos de selecci√≥n de atributos se pueden utilizar para identificar y eliminar los atributos innecesarios, irrelevantes y redundantes que no contribuyen a la exactitud del modelo predictivo o incluso puedan disminuir su precisi√≥n.\nBeneficios de la selecci√≥n de atributos# Uno de los principales beneficios de la selecci√≥n de atributos esta plasmado por la famosa frase \u0026ldquo;Menos es m√°s\u0026rdquo; del arquitecto Ludwig Mies van der Rohe, precursor del minimalismo. Menos atributos son deseables ya que reduce la complejidad del modelo, y un modelo m√°s simple es m√°s f√°cil de entender y explicar.\nOtros beneficios adicionales que nos proporciona una buena selecci√≥n de atributos antes de comenzar con el armado del modelo, son:\nReduce el sobreentrenamiento: Menos datos redundantes significan menos oportunidades para tomar decisiones sobre la base de ruido. Mejora la precisi√≥n: Menos datos enga√±osos se convierten en una mejora en la exactitud del modelo. Reduce el tiempo de entrenamiento: Menos datos significa que los algoritmos aprenden m√°s r√°pidamente. Selecci√≥n de atributos univariante o multivariante# Una cosa que no debemos pasar por alto en el proceso de selecci√≥n de atributos, es la relaci√≥n que puede existir entre ellos. Es decir que debemos considerar seleccionar o eliminar un atributo en forma individual (univariante) o un un grupo de atributos en forma conjunta (multivariante). Esto tambi√©n va a depender del problema con el que estemos tratando y del modelo que elijamos. Por ejemplo si elegimos como modelo un clasificador bayesiano ingenuo, el modelo asume que cada atributo es independiente del resto, por lo tanto, podr√≠amos utilizar un enfoque univariante sin problemas; en cambio si elegimos como modelo una red neuronal, este √∫ltimo no asume la independencia de los atributos, sino que utiliza todas la que dispone; por lo tanto aqu√≠ deber√≠amos seguir un enfoque multivariante para seleccionar los atributos.\nAlgoritmos para selecci√≥n de atributos# Podemos encontrar dos clases generales de algoritmos de selecci√≥n de atributos: los m√©todos de filtrado, y los m√©todos empaquetados.\nM√©todos de filtrado# Estos m√©todos aplican una medida estad√≠stica para asignar una puntuaci√≥n a cada atributo. Los atributos luego son clasificados de acuerdo a su puntuaci√≥n y son, o bien seleccionados para su conservaci√≥n o eliminados del conjunto de datos. Los m√©todos de filtrado son a menudo univariantes y consideran a cada atributo en forma independiente, o con respecto a la variable dependiente.\nEjemplos de estos m√©todos son: prueba de Chi cuadrado, prueba F de Fisher, ratio de ganancia de informaci√≥n y los coeficientes de correlaci√≥n.\nM√©todos empaquetados# Estos m√©todos consideran la selecci√≥n de un conjunto de atributos como un problema de b√∫squeda, en donde las diferentes combinaciones son evaluadas y comparadas. Para hacer estas evaluaciones se utiliza un modelo predictivo y luego se asigna una puntuaci√≥n a cada combinaci√≥n basada en la precisi√≥n del modelo.\nUn ejemplo de este m√©todo es el algoritmo de eliminaci√≥n recursiva de atributos.\nEjemplo# Pasamos ahora a ver como podemos aplicar todo esto al caso en el que ven√≠amos trabajando en el el art√≠culo anterior. Pero antes, terminemos con algunas tareas de preprocesamiento adicionales.\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # Importando las librer√≠as que vamos a utilizar import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.cross_validation import train_test_split from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import f_classif from sklearn.feature_selection import RFE from sklearn.ensemble import ExtraTreesClassifier # graficos incrustados %matplotlib inline # parametros esteticos de seaborn sns.set_palette(\u0026#34;deep\u0026#34;, desat=.6) sns.set_context(rc={\u0026#34;figure.figsize\u0026#34;: (8, 4)}) # importando el dataset preprocesado. ONG_data = pd.read_csv(\u0026#39;LEARNING_procesado.csv\u0026#39;, header=0) # Agregando la columna AGE2 AGE2 = pd.cut(ONG_data[\u0026#39;AGE\u0026#39;], range(0, 100, 10)) ONG_data[\u0026#39;AGE2\u0026#39;] = AGE2 # Eliminar columnas con donaciones superiores a 60 (at√≠picos) ONG_data = ONG_data[ONG_data.DONOR_AMOUNT \u0026lt; 60] # Convertir datos categoricos a numericos tipos = ONG_data.columns.to_series().groupby(ONG_data.dtypes).groups ctext = tipos[np.dtype(\u0026#39;object\u0026#39;)] for c in ctext: ONG_data[c], _ = pd.factorize(ONG_data[c]) ONG_data[\u0026#39;AGE2\u0026#39;], _ = pd.factorize(ONG_data[\u0026#39;AGE2\u0026#39;]) Con estas manipulaciones lo que hicimos es cargar en memoria el dataset que prepocesamos anteriormente, le agregamos la nueva columna AGE2, ya que es mejor tener la edad agrupada en rangos en lugar de individualmente, luego eliminamos los valores at√≠picos que hab√≠amos detectado; y por √∫ltimo, reemplazamos con su equivalente num√©rico a todas las variables categ√≥ricas; ya que para los algoritmos de Scikit-learn es mucho m√°s eficiente trabajar con variables num√©ricas.\nAhora s√≠, ya estamos en condiciones de poder comenzar a aplicar algunos de los algoritmos de selecci√≥n de atributos, comencemos con un simple algoritmo univariante que aplica el m√©todo de filtrado. Para esto vamos a utilizar los objetos SelectKBest y f_classif del paquete sklearn.feature_selection.\n# Separamos las columnas objetivo donor_flag = ONG_data[\u0026#39;DONOR_FLAG\u0026#39;] donor_amount = ONG_data[\u0026#39;DONOR_AMOUNT\u0026#39;] indice = ONG_data[\u0026#39;IDX\u0026#39;] # Aplicando el algoritmo univariante de prueba F. k = 15 # n√∫mero de atributos a seleccionar entrenar = ONG_data.drop([\u0026#39;DONOR_FLAG\u0026#39;, \u0026#39;DONOR_AMOUNT\u0026#39;, \u0026#39;IDX\u0026#39;], axis=1) columnas = list(entrenar.columns.values) seleccionadas = SelectKBest(f_classif, k=k).fit(entrenar, donor_flag) atrib = seleccionadas.get_support() atributos = [columnas[i] for i in list(atrib.nonzero()[0])] atributos ['ODATEDW', 'PEPSTRFL', 'HVP3', 'CARDPROM', 'NUMPROM', 'RAMNT_8', 'RAMNT_16', 'NGIFTALL', 'CARDGIFT', 'LASTGIFT', 'LASTDATE', 'FISTDATE', 'AVGGIFT', 'RFA_2F', 'RFA_2A'] Como podemos ver, el algoritmo nos seleccion√≥ la cantidad de atributos que le indicamos; en este ejemplo decidimos seleccionar solo 15; obviamente, cuando armemos nuestro modelo final vamos a tomar un n√∫mero mayor de atributos.\n¬øC√≥mo funciona?# Este algoritmo selecciona a los mejores atributos bas√°ndose en una prueba estad√≠stica univariante. Al objeto SelectKBest le pasamos la prueba estad√≠stica que vamos a a aplicar, en este caso una prueba F definida por el objeto f_classif, junto con el n√∫mero de atributos a seleccionar. El algoritmo va a aplicar la prueba a todos los atributos y va a seleccionar los que mejor resultado obtuvieron.\nAhora veamos como funciona el algoritmo de Eliminaci√≥n Recursiva de atributos. Para este caso, vamos a utilizar como nuestro modelo predictivo el algoritmo ExtraTreesClassifier.\n# Algoritmo de Eliminaci√≥n Recursiva de atributos con ExtraTrees modelo = ExtraTreesClassifier() era = RFE(modelo, 15) # n√∫mero de atributos a seleccionar era = era.fit(entrenar, donor_flag) # imprimir resultados atrib = era.support_ atributos = [columnas[i] for i in list(atrib.nonzero()[0])] atributos ['OSOURCE', 'ZIP', 'VIETVETS', 'WWIIVETS', 'POP901', 'HV1', 'PEC2', 'TPE13', 'EIC4', 'EIC14', 'VC2', 'CARDPROM', 'MINRDATE', 'MAXRDATE', 'TIMELAG'] ¬øC√≥mo funciona?# En este algoritmo, dado un modelo predictivo que asigna un coeficiente de importancia a cada atributo (como ExtraTreesClassifier), el objetivo de la Eliminaci√≥n Recursiva de atributos es ir seleccionado en forma recursiva un n√∫mero cada vez m√°s peque√±o de atributos. Primero comienza con todos los atributos del dataset y luego en cada pasada va eliminando aquellos que tenga el menor coeficiente de importancia hasta alcanzar el n√∫mero de atributos deseado.\nSi vemos los 15 atributos seleccionados por este otro algoritmo, existen muchas diferencias con los que selecciono el modelo anterior; en general, la Eliminaci√≥n Recursiva de atributos suele ser mucho m√°s precisa, pero tambi√©n consume mucho m√°s tiempo y recursos, ya que requiere que entrenemos a un modelo predictivo para poder obtener sus resultados.\nPor √∫ltimo, tambi√©n podr√≠amos utilizar ese coeficiente de importancia que nos proporciona el modelo como una gu√≠a adicional para refinar nuestra selecci√≥n de atributos.\n# Importancia de atributos. modelo.fit(entrenar, donor_flag) modelo.feature_importances_[:15] array([ 2.59616058e-03, 3.03252110e-03, 2.71464477e-03, 2.51952243e-03, 2.34433609e-03, 6.01722535e-04, 4.82782938e-04, 2.24732045e-03, 2.07872966e-03, 1.29249803e-03, 1.17189225e-03, 9.77218420e-06, 6.12421074e-04, 5.79137133e-05, 2.40594405e-03]) # 15 coeficientes m√°s altos np.sort(modelo.feature_importances_)[::-1][:15] array([ 0.00369165, 0.00353173, 0.00348298, 0.00336095, 0.00331946, 0.00331828, 0.00331345, 0.00326505, 0.00316758, 0.00316587, 0.00313431, 0.00311334, 0.00310065, 0.00307309, 0.00304518]) Analicemos algunos de estos atributos en forma individual para tener una idea de cuanto puede ser que aporten a la exactitud del modelo. Podr√≠amos comparar por ejemplo, el promedio de donaciones que podr√≠amos obtener con este atributo contra el promedio de todo el dataset. Tomemos por ejemplo al atributo LASTGIFT que representa el importe de la √∫ltima donaci√≥n que realiz√≥ cada persona incluida en el conjunto de datos. En principio parece l√≥gico que este atributo sea significativo para el modelo, ya que si don√≥ en el pasado, hay bastantes posibilidades de que vuelva a donar en esta oportunidad.\n# Probabilidad de ser donante de todo el dataset. prob_gral = (ONG_data[ONG_data.DONOR_AMOUNT \u0026gt; 0][\u0026#39;DONOR_AMOUNT\u0026#39;].count() \\ / ONG_data[\u0026#39;DONOR_AMOUNT\u0026#39;].count()) * 100.0 prob_gral 5.0377358490566033 # Probabilidad de realizar donanci√≥n con LASTGIFT \u0026lt;= 10 lastgift10 = (ONG_data[(ONG_data.DONOR_AMOUNT \u0026gt; 0) \u0026amp; (ONG_data.LASTGIFT \u0026lt;= 10)][\u0026#39;DONOR_AMOUNT\u0026#39;].count() \\ / ONG_data[ONG_data.LASTGIFT \u0026lt;= 10][\u0026#39;DONOR_AMOUNT\u0026#39;].count()) * 100.0 lastgift10 6.9347104389524157 # graficando los resultados lastgift = pd.Series({\u0026#39;promedio gral\u0026#39;: prob_gral, \u0026#39;lastgift\u0026lt;=10\u0026#39;: lastgift10}) plot=lastgift.plot(kind=\u0026#39;barh\u0026#39;, color=[\u0026#39;blue\u0026#39;, \u0026#39;green\u0026#39;]).set_title(\u0026#39;Pobabilidad de donar\u0026#39;) Este √∫ltimo gr√°fico nos muestra claramente que con un valor del atributo LASTGIFT menor o igual a 10 las probabilidades de que esa persona realice una donaci√≥n mejoran, pero veamos que pasa con el importe de la donaci√≥n.\n# importe promedio de donaci√≥n general donacion_prom = ONG_data[ONG_data.DONOR_AMOUNT \u0026gt; 0][\u0026#39;DONOR_AMOUNT\u0026#39;].mean() donacion_prom 14.889109446525177 # importe promedio de donaci√≥n lastgift \u0026lt;= 10 lastgift10_imp = ONG_data[(ONG_data.DONOR_AMOUNT \u0026gt; 0) \u0026amp; (ONG_data.LASTGIFT \u0026lt;= 10)][\u0026#39;DONOR_AMOUNT\u0026#39;].mean() lastgift10_imp 8.7553191489361701 # graficando los resultados lastgift = pd.Series({\u0026#39;imp promedio gral\u0026#39;: donacion_prom, \u0026#39;lastgift\u0026lt;=10\u0026#39;: lastgift10_imp}) plot=lastgift.plot(kind=\u0026#39;barh\u0026#39;, color=[\u0026#39;blue\u0026#39;, \u0026#39;green\u0026#39;] ).set_title(\u0026#39;importe promedio de donaci√≥n\u0026#39;) Aqu√≠ vemos, que si bien las probabilidades de que sea un donador mejoran, el importe que se dona esta por debajo del promedio. En el caso de este atributo podemos ver que existe una correlaci√≥n inversa entre el importe de donaci√≥n y la probabilidad de hacer una donaci√≥n a la ONG.\nHasta aqu√≠ llegamos en este art√≠culo, la idea es que luego, cuando tengamos armado el modelo, podamos jugar con distintas combinaciones de atributos y ver como se comporta el modelo hasta alcanzar la combinaci√≥n ideal de atributos. No se pierdan los pr√≥ximos art√≠culos!\nSaludos!\nEste post fue escrito utilizando IPython notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2016-04-15","id":22,"permalink":"/blog/2016/04/15/ejemplo-de-machine-learning-con-python-seleccion-de-atributos/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, Analisis de datos y Python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Continuando donde nos quedamos en el art√≠culo anterior Ejemplo de Machine Learning - preprocesamiento y exploraci√≥n; ahora es tiempo de ingresar en el terreno de la selecci√≥n de atributos.\n¬øEn qu√© consiste la selecci√≥n de atributos?# La selecci√≥n de atributos es el proceso por el cual seleccionamos un subconjunto de atributos (representados por cada una de las columnas en un dataset de forma tabular) que son m√°s relevantes para la construcci√≥n del modelo predictivo sobre el que estamos trabajando.","tags":["python","estadistica","programacion","machine learning","analisis de datos"],"title":"Ejemplo de Machine Learning con Python - Selecci√≥n de atributos"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, Analisis de datos y Python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# En mi art√≠culo Machine Learning con Python, hice una breve introducci√≥n a los principales conceptos que debemos conocer de Machine Learning. En este art√≠culo, la idea es profundizar un poco m√°s en ellos y presentar algunos conceptos nuevos con la ayuda de un ejemplo pr√°ctico.\nDescripci√≥n del ejemplo# En el ejemplo que vamos a utilizar, vamos a imaginarnos que una organizaci√≥n sin fines de lucro soporta su operaci√≥n mediante la organizaci√≥n peri√≥dica de una campa√±a para recaudar fondos por correo. Esta organizaci√≥n ha creado una base de datos con m√°s de 40 mil personas que por lo menos una vez en el pasado ha sido donante. La campa√±a de recaudaci√≥n de fondos se realiza mediante el env√≠o a una lista de correo (o un subconjunto de ella) de un regalo simb√≥lico y la solicitud de una donaci√≥n. Una vez que se planifica la campa√±a, el costo total de la misma se conoce de forma autom√°tica:\n[n√∫mero de potenciales donantes a contactar] x ([costo de regalo] + [costo de correo])\nSin embargo, el resultado de la recaudaci√≥n de fondos depende tanto del n√∫mero de donantes que responde a la campa√±a, como del importe medio de dinero que es donado.\nLa idea es que, utilizando las t√©cnicas de Machine Learning sobre la base de datos de esta organizaci√≥n, podamos ayudarla a maximizar los beneficios de la campa√±a de recaudaci√≥n, esto es, lograr el m√°ximo importe posible de dinero recaudado, minimizando lo m√°s que se pueda el costo total de la campa√±a. Debemos tener en cuenta que un miembro de la organizaci√≥n le enviar√° el correo a un potencial donante, siempre que el rendimiento esperado del pedido excede el costo del correo con la solicitud de donaci√≥n. Para nuestro ejemplo, el costo por donante de la campa√±a va a ser igual al [costo de regalo] + [costo de correo], y esto va a ser igual a $ 0.75 por correo enviado. Los ingresos netos de la campa√±a se calculan como la suma (importe de donaci√≥n real - $ 0.75) sobre todos los donantes a los que se ha enviado el correo. Nuestro objetivo es ayudar a esta organizaci√≥n sin fines de lucro a seleccionar de su lista de correo los donantes a los que debe abordar a los efectos de maximizar los beneficios de la campa√±a de recaudaci√≥n.\nEl Dataset# El dataset que vamos a utilizar, consiste en la base de datos de la organizaci√≥n sin fines de lucro con la lista de correo de los donantes de sus campa√±as anteriores. El mismo, ya lo hemos dividido en un dataset de aprendizaje que se pueden descargar del siguiente enlace; y un dataset que vamos a utilizar para realizar las predicciones, el cual se lo pueden descargar desde este otro enlace. Algunos otros datos a tener en cuenta, son los siguientes:\nEl dataset de aprendizaje contiene 47720 registros y 481 columnas. La primera fila / cabecera del mismo contiene los nombres de cada campo. El dataset de validaci√≥n contiene 47692 registros y 479 columnas. Al igual que en el caso anterior, la primera fila contiene los nombres de cada campo. Los registros del dataset de validaci√≥n son id√©nticos a los registros del dataset de aprendizaje, excepto que los valores para nuestros campos objetivo que necesitamos para el aprendizaje, no existen(es decir, las columnas DONOR_FLAG y DONOR_AMOUNT no est√°n incluidas en el dataset de validaci√≥n). Los espacios en blanco en los campos de tipo texto y los puntos en los campos de tipo num√©rico corresponden a valores faltantes o perdidos. Cada registro tiene un identificador √∫nico de registro o √≠ndice (campo IDX). Para cada registro, hay dos variables objetivo (campos DONOR_FLAG y DONOR_AMOUNT). DONOR_FLAG es una variable binaria que indica si ese registro fue donante o no; mientras que DONOR_AMOUNT contiene el importe de la donaci√≥n para los casos que fueron donantes. Algunos de los valores en el dataset pueden contener errores de formato o de ingreso. Por lo que se deber√≠an corregir o limpiar. Una descripci√≥n detallada del significado de cada columna del dataset, la pueden encontrar en el siguiente enlace. An√°lisis exploratorio y preprocesamiento# El primer paso que deber√≠amos emprender, es realizar un peque√±o an√°lisis exploratorio de nuestro dataset; es decir, valernos de algunos herramientas de la estad√≠stica, junto con algunas visualizaciones para entender un poco m√°s los datos de los que disponemos. Veamos como podemos hacer esto.\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # Importando las librer√≠as que vamos a utilizar import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.preprocessing import LabelEncoder # graficos incrustados %matplotlib inline # parametros esteticos de seaborn sns.set_palette(\u0026#34;deep\u0026#34;, desat=.6) sns.set_context(rc={\u0026#34;figure.figsize\u0026#34;: (8, 4)}) # importando el dataset a un Dataframe de Pandas ONG_data = pd.read_csv(\u0026#39;LEARNING.csv\u0026#39;, header=0) # Examinando las primeras 10 filas y 10 columnas del dataset ONG_data.ix[:10, :10] ODATEDW OSOURCE TCODE STATE ZIP MAILCODE PVASTATE DOB NOEXCH RECINHSE 0 8901 GRI 0 IL 61081 3712 0 1 9401 NWN 0 LA 70611 0 0 2 9401 MSD 1 TN 37127- 3211 0 3 8901 ENQ 0 MN 56475 2603 0 4 9201 HCC 1 LA 70791 0 0 X 5 9301 USB 1 UT 84720 2709 0 6 9401 FRC 1 CA 90056 0 0 7 8801 PCH 2 IL 62376 5201 0 8 8601 AMB 28 FL 32810 B 3601 0 9 9501 L15 1 NC 27850 0 0 10 8701 BBK 2 MN 55125 3601 0 # Controlando la cantidad de registros ONG_data[\u0026#39;DONOR_AMOUNT\u0026#39;].count() 47720 Como podemos ver, utilizando simples expresiones de Python, podemos cargar la base de datos de la ONG en un Dataframe de Pandas; lo que nos va a permitir manipular los datos con suma facilidad. Comenzemos a explorar un poco m√°s en detalle este dataset!\nEn primer lugar, lo que deber√≠amos hacer es controlar si existen valores faltantes o nulos; esto lo podemos realizar utilizando el m√©todo isnull() del siguiente modo:\n# Controlando valores nulos ONG_data.isnull().any().any() True Como podemos ver, el m√©todo nos devuelve el valor \u0026ldquo;True\u0026rdquo;, lo que indica que existen valores nulos en nuestro dataset. Estos valores pueden tener una influencia significativa en nuestro modelo predictivo, por lo que siempre es una decisi√≥n importante determinar la forma en que los vamos a manejar. Las alternativas que tenemos son:\nDejarlos como est√°n, lo que a la larga nos va a traer bastantes dolores de cabeza ya que en general los algoritmos no los suelen procesar correctamente y provocan errores. Eliminarlos, lo que es una alternativa viable aunque, dependiendo la cantidad de valores nulos, puede afectar significativamente el resultado final de nuestro modelo predictivo. Inferir su valor. En este caso, lo que podemos hacer es tratar de inferir el valor faltante y reemplazarlo por el valor inferido. Esta suele ser generalmente la mejor alternativa a seguir. En este ejemplo, yo voy a utilizar la √∫ltima alternativa. Vamos a inferir los valores faltantes utilizando la media aritm√©tica para los datos cuantitativos y la moda para los datos categ√≥ricos.\nComo vamos a utilizar dos m√©todos distintos para reemplazar a los valores faltantes, dependiendo de si son num√©ricos o categ√≥ricos, el primer paso que debemos realizar es tratar de identificar que columnas de nuestro dataset corresponde a cada tipo de datos; para realizar esto vamos a utilizar el atributo dtypes del Dataframe de Pandas.\n# Agrupando columnas por tipo de datos tipos = ONG_data.columns.to_series().groupby(ONG_data.dtypes).groups # Armando lista de columnas categ√≥ricas ctext = tipos[np.dtype(\u0026#39;object\u0026#39;)] len(ctext) # cantidad de columnas con datos categ√≥ricos. 68 # Armando lista de columnas num√©ricas columnas = ONG_data.columns # lista de todas las columnas cnum = list(set(columnas) - set(ctext)) len(cnum) 413 Ahora ya logramos separar a las 481 columnas que tiene nuestro dataset. 68 columnas contienen datos categ√≥ricos y 413 contienen datos cuantitativos. Procedamos a inferir los valores faltantes.\n# Completando valores faltantas datos cuantititavos for c in cnum: mean = ONG_data[c].mean() ONG_data[c] = ONG_data[c].fillna(mean) # Completando valores faltantas datos categ√≥ricos for c in ctext: mode = ONG_data[c].mode()[0] ONG_data[c] = ONG_data[c].fillna(mode) # Controlando que no hayan valores faltantes ONG_data.isnull().any().any() False # Guardando el dataset preprocesado # Save transform datasets ONG_data.to_csv(\u0026#34;LEARNING_procesado.csv\u0026#34;, index=False) Perfecto! Ahora tenemos un dataset limpio de valores faltantes. Ya estamos listos para comenzar a explorar los datos, comencemos por determinar el porcentaje de personas que alguna vez fue donante de la ONG y est√°n incluidos en la base de datos con la que estamos trabajando.\n# Calculando el porcentaje de donantes sobre toda la base de datos porcent_donantes = (ONG_data[ONG_data.DONOR_AMOUNT \u0026gt; 0][\u0026#39;DONOR_AMOUNT\u0026#39;].count() * 1.0 / ONG_data[\u0026#39;DONOR_AMOUNT\u0026#39;].count()) * 100.0 print(\u0026#34;El procentaje de donantes de la base de datos es {0:.2f}%\u0026#34; .format(porcent_donantes)) El procentaje de donantes de la base de datos es 5.08% # Grafico de totas del porcentaje de donantes # Agrupando por DONOR_FLAG donantes = ONG_data.groupby(\u0026#39;DONOR_FLAG\u0026#39;).IDX.count() # Creando las leyendas del grafico. labels = [ \u0026#39;Donante\\n\u0026#39; + str(round(x * 1.0 / donantes.sum() * 100.0, 2)) + \u0026#39;%\u0026#39; for x in donantes ] labels[0] = \u0026#39;No \u0026#39; + labels[0] plt.pie(donantes, labels=labels) plt.title(\u0026#39;Porcion de donantes\u0026#39;) plt.show() # Creando subset con solo los donates ONG_donantes = ONG_data[ONG_data.DONOR_AMOUNT \u0026gt; 0] # cantidad de donantes len(ONG_donantes) 2423 Aqu√≠ podemos ver que el porcentaje de personas que fueron donantes en el pasado es realmente muy bajo, solo un 5 % del total de la base de datos (2423 personas). Este es un dato importante a tener en cuenta ya que al existir tanta diferencia entre las clases a clasificar, esto puede afectar considerablemente a nuestro algoritmo de aprendizaje. Exploremos tambi√©n un poco m√°s en detalle a este grupo peque√±o de personas que fueron donantes; veamos por ejemplo como se dividen de acuerdo a la cantidad de dinero donado.\n# Analizando el importe de donanciones # Creando un segmentos de importes imp_segm = pd.cut(ONG_donantes[\u0026#39;DONOR_AMOUNT\u0026#39;], [0, 10, 20, 30, 40, 50, 60, 100, 200]) # Creando el grafico de barras desde pandas plot = pd.value_counts(imp_segm).plot(kind=\u0026#39;bar\u0026#39;, title=\u0026#39;Importes de donacion\u0026#39;) plot.set_ylabel(\u0026#39;Cant de donantes\u0026#39;) plot.set_xlabel(\u0026#39;Rango de importes\u0026#39;) plt.show() # Agrupaci√≥n por segmento segun importe donado. pd.value_counts(imp_segm) (0, 10] 1026 (10, 20] 921 (20, 30] 358 (30, 40] 53 (40, 50] 43 (60, 100] 15 (50, 60] 4 (100, 200] 3 dtype: int64 # importe de donaci√≥n promedio ONG_donantes[\u0026#39;DONOR_AMOUNT\u0026#39;].mean() 15.598237721832438 # Gr√°fico de cajas del importe de donaci√≥n sns.boxplot(list(ONG_donantes[\u0026#39;DONOR_AMOUNT\u0026#39;])) plt.title(\u0026#39;importe de donaci√≥n\u0026#39;) plt.show() Este an√°lisis nos muestra que la mayor cantidad de donaciones caen en un rango de importes entre 0 y 30, siendo la donaci√≥n promedio 15.60. Tambi√©n podemos ver que donaciones que superen un importe de 50 son casos realmente poco frecuentes, por lo que constituyen valores at√≠picos y ser√≠a prudente eliminar estos casos al entrenar nuestro modelo para que no distorsionen los resultados.\nOtra exploraci√≥n interesante que podr√≠amos realizar sobre nuestro dataset relacionado con los donantes, es ver como se divide este grupo en t√©rminos de g√©nero y edad. Comencemos con el g√©nero!\n# Grafico del g√©nero de los donantes ONG_donantes.groupby(\u0026#39;GENDER\u0026#39;).size().plot(kind=\u0026#39;bar\u0026#39;) plt.title(\u0026#39;Distribuci√≥n por g√©nero\u0026#39;) plt.show() # Donaciones segun el g√©nero ONG_donantes[(ONG_donantes.DONOR_AMOUNT \u0026lt;= 50) \u0026amp; (ONG_donantes.GENDER.isin([\u0026#39;F\u0026#39;, \u0026#39;M\u0026#39;]) )][[\u0026#39;DONOR_AMOUNT\u0026#39;, \u0026#39;GENDER\u0026#39;]].boxplot(by=\u0026#39;GENDER\u0026#39;) plt.title(\u0026#39;Donantes segun sexo\u0026#39;) plt.show() # Media de impote donado por mujeres ONG_donantes[ONG_donantes.GENDER == \u0026#39;F\u0026#39;][[\u0026#39;DONOR_AMOUNT\u0026#39;]].mean() DONOR_AMOUNT 14.610311 dtype: float64 # Media de impote donado por hombres ONG_donantes[ONG_donantes.GENDER == \u0026#39;M\u0026#39;][[\u0026#39;DONOR_AMOUNT\u0026#39;]].mean() DONOR_AMOUNT 16.81989 dtype: float64 Aqu√≠ vemos que las mujeres suelen estar m√°s propensas a donar, aunque donan un importe promedio menor (14.61) al que donan los hombres (16.82). Veamos ahora como se comportan las donaciones respecto a la edad.\n# Distribuci√≥n de la edad de los donantes ONG_donantes[\u0026#39;AGE\u0026#39;].hist().set_title(\u0026#39;Distribuci√≥n de donantes segun edad\u0026#39;) plt.show() # Agrupando la edad por rango de a 10 AGE2 = pd.cut(ONG_donantes[\u0026#39;AGE\u0026#39;], range(0, 100, 10)) ONG_donantes[\u0026#39;AGE2\u0026#39;] = AGE2 # Gr√°fico de barras de donaciones por edad pd.value_counts(AGE2).plot(kind=\u0026#39;bar\u0026#39;, title=\u0026#39;Donaciones por edad\u0026#39;) plt.show() # Importes de donaci√≥n por grango de edad ONG_donantes[ONG_donantes.DONOR_AMOUNT \u0026lt;= 50][[\u0026#39;DONOR_AMOUNT\u0026#39;, \u0026#39;AGE2\u0026#39;]].boxplot(by=\u0026#39;AGE2\u0026#39;) plt.title(\u0026#39;Importe de donaci√≥n por edad\u0026#39;) plt.show() En este √∫ltimo an√°lisis podemos ver que la mayor cantidad de los donantes son personas de entre 60 y 70 a√±os, aunque la media de importe donado m√°s alta la tienen las personas que van desde los 30 a los 60 a√±os.\nCon esto concluyo este an√°lisis; en pr√≥ximos art√≠culos voy a continuar con el ejemplo completando los restantes pasos que incluye un proyecto de Machine Learning hasta concluir el modelo y poder utilizarlo para realizar predicciones (selecci√≥n de atributos - armado de modelo - entrenamiento - evaluaci√≥n - m√©tricas - predicci√≥n). Espero lo hayan disfrutado tanto como yo disfrute al escribirlo!\nSaludos!\nEste post fue escrito utilizando IPython notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2016-04-08","id":23,"permalink":"/blog/2016/04/08/ejemplo-de-machine-learning-con-python-preprocesamiento-y-exploracion/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, Analisis de datos y Python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# En mi art√≠culo Machine Learning con Python, hice una breve introducci√≥n a los principales conceptos que debemos conocer de Machine Learning. En este art√≠culo, la idea es profundizar un poco m√°s en ellos y presentar algunos conceptos nuevos con la ayuda de un ejemplo pr√°ctico.","tags":["python","estadistica","programacion","machine learning","analisis de datos"],"title":"Ejemplo de Machine Learning con Python - Preprocesamiento y exploraci√≥n"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# En mi art√≠culo anterior, An√°lisis de datos categ√≥ricos con Python, mencione la importancia de reconocer los distintos tipos de datos con que nos podemos encontrar al realizar an√°lisis estad√≠sticos y vimos tambi√©n como podemos trabajar con los datos categ√≥ricos. En esta oportunidad, vamos a ver como podemos manipular, interpretar y obtener informaci√≥n de los datos cuantitativos.\nRecordemos que las variables cuantitativas son variables medidas en una escala num√©rica. Altura, peso, tiempo de respuesta, la calificaci√≥n subjetiva del dolor, la temperatura, y la puntuaci√≥n en un examen, son ejemplos de variables cuantitativas. Las variables cuantitativas se distinguen de las variables categ√≥ricas (tambi√©n llamadas cualitativas) como el color favorito, religi√≥n, ciudad de nacimiento, y el deporte favorito; en las que no hay un orden o medida involucrados.\nAnalizando datos cuantitativos con Python# Para los ejemplos de este art√≠culo, vamos a trabajar con el dataset faithful, el cual consiste en una colecci√≥n de observaciones sobre las erupciones del g√©iser Old Faithful en el parque nacional Yellowstone de los Estados Unidos. La informaci√≥n que contiene este dataset es la siguiente:\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # importando modulos necesarios %matplotlib inline import matplotlib.pyplot as plt import numpy as np from scipy import stats import pandas as pd import seaborn as sns from pydataset import data # parametros esteticos de seaborn sns.set_palette(\u0026#34;deep\u0026#34;, desat=.6) sns.set_context(rc={\u0026#34;figure.figsize\u0026#34;: (8, 4)}) faithful = data(\u0026#39;faithful\u0026#39;) faithful.head(10) eruptions waiting 1 3.600 79 2 1.800 54 3 3.333 74 4 2.283 62 5 4.533 85 6 2.883 55 7 4.700 88 8 3.600 85 9 1.950 51 10 4.350 85 Como podemos ver, faithful es un dataset bastante simple que solo contiene observaciones de dos variables; la primera, que se llama eruptions, contiene la informaci√≥n de la duraci√≥n de la erupci√≥n del g√©iser; mientras que la segunda, se llama waiting y contiene la informaci√≥n sobre el tiempo de espera para la siguiente erupci√≥n del g√©iser.\nAl igual de como coment√°bamos cuando analizamos datos categ√≥ricos, lo primero que deber√≠amos intentar hacer es crear una imagen que represente de la mejor manera posible a nuestros datos, ya que nuestro cerebro tiende a procesar mejor la informaci√≥n visual. Para el caso de las variables cuantitativas, un buen candidato para comenzar a hacernos una imagen de lo que nuestros datos representan, son los histogramas.\nHistogramas# Para las variables cuantitativas, a diferencia de lo que pasaba con las variables categ√≥ricas, no existe una forma obvia de agrupar los datos; por tal motivo lo que se suele hacer es, dividir los posibles valores en diferentes contenedores del mismo tama√±o y luego contar el n√∫mero de casos que cae dentro de cada uno de los contenedores. Estos contenedores junto con sus recuentos, nos proporcionan una imagen de la distribuci√≥n de la variable cuantitativa y constituyen la base para poder graficar el histograma. Para construir el gr√°fico, simplemente debemos representar a los recuentos como barras y graficarlas contra los valores de cada uno de los contenedores.\nCon Python podemos representar f√°cilmente el histograma de la variable eruptions utilizando el m√©todo hist del DataFrame de Pandas del siguiente modo:\n# histograma duraci√≥n de erupciones con 8 barras faithful[\u0026#39;eruptions\u0026#39;].hist(bins=8) plt.xlabel(\u0026#34;Duraci√≥n en minutos\u0026#34;) plt.ylabel(\u0026#34;Frecuencia\u0026#34;) plt.show() Como podemos ver con este gr√°fico, la duraci√≥n m√°s frecuente de las erupciones del g√©iser ronda en alrededor de cuatro minutos y medio. Una cosa que debemos hacer notar es que en los histogramas, los contenedores dividen a todos los valores de la variable cuantitativa, por lo que no deber√≠amos encontrar espacios entre las barras (a diferencia de lo que pasaba con los gr√°ficos de barras que vimos en el art√≠culo anterior). Cualquier espacio entre las barras es una brecha en los datos, que nos indica un regi√≥n para la que no existen valores.\nDistribuci√≥n de frecuencia# Un tema √≠ntimamente relacionado con los histogramas son las tablas de distribuci√≥n de frecuencia, en definitiva los histogramas no son m√°s que gr√°ficos de tablas de distribuci√≥n de frecuencia. La distribuci√≥n de frecuencia de una variable cuantitativa consiste en un resumen de la ocurrencia de un dato dentro de una colecci√≥n de categor√≠as que no se superponen. Estas categor√≠as las vamos a poder armar seg√∫n nuestra conveniencia y lo que queramos analizar. Por ejemplo si quisi√©ramos armar la distribuci√≥n de frecuencia de la variable eruptions podr√≠amos realizar las siguiente manipulaciones con Pandas:\n# Distribuci√≥n de frecuencia. # 1ro creamos un rango para las categor√≠as. contenedores = np.arange(1.5, 6., 0.5) # luego cortamos los datos en cada contenedor frec = pd.cut(faithful[\u0026#39;eruptions\u0026#39;], contenedores) # por √∫ltimo hacemos el recuento de los contenedores # para armar la tabla de frecuencia. tabla_frec = pd.value_counts(frec) tabla_frec (4, 4.5] 75 (1.5, 2] 55 (4.5, 5] 54 (2, 2.5] 37 (3.5, 4] 34 (3, 3.5] 9 (2.5, 3] 5 (5, 5.5] 3 dtype: int64 Como nos nuestra esta tabla de distribuci√≥n de frecuencia, la duraci√≥n que m√°s veces ocurre para las erupciones, se encuentran en el rango de 4 a 4.5 minutos.\nDiagrama de tallos y hojas# Los histogramas nos permiten apreciar la distribuci√≥n de los datos de una forma sencilla, pero los mismos no nos muestran los valores del dataset en s√≠ mismos. Para solucionar esto, existe el diagrama de tallos y hojas, el cual es similar al histograma pero nos muestra los valores individuales de nuestro dataset. Para que quede m√°s claro, vemos un ejemplo sencillo. Supongamos que tenemos una muestra sobre el ritmo card√≠aco de 24 mujeres, las observaciones son las siguientes:\npulso = [88, 80, 76, 72, 68, 56, 64, 60, 64, 68, 64, 68, 72, 76, 80, 84, 68, 80, 76, 72, 84, 80, 72, 76 ] Podr√≠amos graficar el histograma de estas observaciones del siguiente modo:\nplt.hist(pulso, bins=7) plt.xlabel(\u0026#34;latidos por minuto\u0026#34;) plt.ylabel(\u0026#34;N¬∞ de mujeres\u0026#34;) plt.show() Este histograma nos muestra la forma en que se distribuyen los datos, pero no nos muestra los datos individuales. Para esto podr√≠amos graficar su diagrama de tallos y hojas de la siguiente manera:\n# Diagrama de tallos y hojas def tallos(d): \u0026#34;Genera un simple diagramas de tallos y hojas\u0026#34; l,t=np.sort(d),10 O=range(int(l[0]-l[0]%t),int(l[-1]+11),t) I=np.searchsorted(l,O) for e,a,f in zip(I,I[1:],O): print(\u0026#39;%3d|\u0026#39;%(f/t),*(l[e:a]-f),sep=\u0026#39;\u0026#39;) tallos(pulso) 5|6 6|04448888 7|22226666 8|0000448 Como vemos, la distribuci√≥n del diagrama de tallos y hojas es similar a la del histograma, pero en este caso si podemos ver los valores de nuestras observaciones. El diagrama se lee as√≠: Por un lado tenemos las decenas de los latidos, las cuales constituyen los tallos de nuestro diagrama (los valores antes del pipe o barra vertical \u0026ldquo;|\u0026rdquo;) y luego vamos agrando hojas a estos tallos , representadas por las unidades de cada latido. De esta forma 5|6, significa que solo aparece el valor 56 una sola vez, en cambio 8|0000, significa que tenemos el valor 80 observado en 4 oportunidades.\nDiagrama de dispersi√≥n# Hasta aqu√≠ venimos graficando √∫nicamente una sola variable cuantitativa pero ¬øqu√© pasa si queremos trabajar con dos variables? Para estos casos existe el diagrama de dispersi√≥n. El diagrama de dispersi√≥n es una de las formas m√°s comunes que existen para visualizar datos y constituye una de las mejores forma de observar relaciones entre dos variables cuantitativas. Veremos que se puede observar un mont√≥n de cosas por el solo hecho de mirar. Este diagrama es una de las mejores formas de visualizar las asociaciones que pueden existir entre nuestros datos.\nEl diagrama de dispersi√≥n empareja los valores de dos variables cuantitativas y luego los representa como puntos geom√©tricos dentro de un diagrama cartesiano. Por ejemplo, volviendo a nuestro dataset faithful, podr√≠amos emparejar a las variables eruptions y waiting en la misma observaci√≥n como coordenadas (x, y) y luego graficarlas en el eje cartesiano. Con la ayuda de Python podr√≠amos generar el diagrama de dispersi√≥n del siguiente modo:\n# diagrama de dispersi√≥n disp= faithful.plot(kind=\u0026#39;scatter\u0026#39;, x=\u0026#39;eruptions\u0026#39;, y=\u0026#39;waiting\u0026#39;) Como podemos ver con solo observar la dispersi√≥n de los datos parece existir una relaci√≥n lineal entre los datos de este dataset.\nMedidas de tendencia central# Una vez que ya nos dimos una buena idea visual de como se distribuyen los datos y de las relaciones que pueden existir entre los mismos, podemos pasar a calcular medidas num√©ricas propias de la estad√≠stica descriptiva. En general, suele ser interesante conocer cual puede ser el promedio o valor central al que tiende la distribuci√≥n de nuestros datos, para esto se utilizan las medidas de tendencia central, entre las que podemos encontrar a:\nLa media aritm√©tica La media ponderada La media geom√©trica La media arm√≥nica La mediana La media truncada La moda Veamos como podemos calcularlas con Python:\nMedia aritm√©tica# La media aritm√©tica es el valor obtenido al sumar todos los datos y dividir el resultado entre el n√∫mero total elementos. La calculamos con el m√©todo mean.\n# media de variable eruptions faithful[\u0026#39;eruptions\u0026#39;].mean() 3.4877830882352936 Media ponderada# La media ponderada es apropiada cuando en un dataset cada dato tiene una importancia relativa (o peso) respecto de los dem√°s. Como esta media no aplica para nuestro dataset no la vamos a calcular.\nMedia geom√©trica# La media geom√©trica es √∫til cuando queremos comparar cosas con propiedades muy diferentes; tambi√©n es es recomendada para datos de progresi√≥n geom√©trica, para promediar razones, inter√©s compuesto y n√∫meros √≠ndices. Se calcula tomando la ra√≠z n-√©sima del producto de todos los datos. La calculamos con la funci√≥n gmean de SciPy.\n# media geometrica stats.gmean(faithful[\u0026#39;eruptions\u0026#39;]) 3.2713131325361786 Media arm√≥nica# La media arm√≥nica promedia el n√∫mero de elementos y los divide por la suma de sus inversos. La media arm√≥nica es siempre la media m√°s baja y es recomendada para promediar velocidades. La calculamos con la funci√≥n hmean de SciPy.\n# media arm√≥nica stats.hmean(faithful[\u0026#39;eruptions\u0026#39;]) 3.0389330499472611 Mediana# La mediana representa el valor de posici√≥n central en un conjunto de datos ordenados. La podemos calcular utilizando el m√©todo median de Pandas\n# mediana faithful[\u0026#39;eruptions\u0026#39;].median() 4.0 Media truncada# La media truncada es una mezcla entre la media aritm√©tica y la mediana. Para calcular el promedio previamente se descartan porciones en el extremo inferior y superior de la distribuci√≥n de los datos. En Python podemos utilizar la funci√≥n trim_mean de SciPy.\n# media truncada, recortando el 10 superior e inferior stats.trim_mean(faithful[\u0026#39;eruptions\u0026#39;], .10) 3.5298073394495413 Moda# Por √∫ltimo, la moda es el valor que tiene mayor frecuencia absoluta. Son los picos que vemos en el histograma. Dependiendo de la la distribuci√≥n de los datos puede existir m√°s de una, como en el caso de la variable eruptions. La calculamos con el m√©todo mode.\n# moda faithful[\u0026#39;eruptions\u0026#39;].mode() 0 1.867 1 4.500 dtype: float64 Medidas de dispersi√≥n# Las medidas de tendencia central no son las √∫nicas medidas de resumen estad√≠stico que podemos calcular; otras medidas tambi√©n de gran importancia son las medidas de dispersi√≥n. Las medidas de dispersi√≥n, tambi√©n llamadas medidas de variabilidad, muestran la variabilidad de una distribuci√≥n, indicando por medio de un n√∫mero si las diferentes puntuaciones de una variable est√°n muy alejadas de la media. Cuanto mayor sea ese valor, mayor ser√° la variabilidad, y cuanto menor sea, m√°s homog√©nea ser√° a la media. As√≠ se sabe si todos los casos son parecidos o var√≠an mucho entre ellos. Las principales medidas de dispersi√≥n son:\nLa varianza El desv√≠o est√°ndar Los cuartiles La covarianza El coeficiente de correlaci√≥n Analicemos cada uno de ellos:\nVarianza# La varianza intenta describir la dispersi√≥n de los datos. Se define como la esperanza del cuadrado de la desviaci√≥n de dicha variable respecto a su media. Una varianza peque√±a indica que los puntos de datos tienden a estar muy cerca de la media y por lo tanto el uno al otro, mientras que una varianza alta indica que los puntos de datos est√°n muy distribuidos alrededor de la media y la una de la otra. La podemos calcular con el m√©todo var.\n# varianza faithful[\u0026#39;eruptions\u0026#39;].var() 1.3027283328494705 Desv√≠o est√°ndar# El desv√≠o est√°ndar o desviaci√≥n t√≠pica es una medida que se utiliza para cuantificar la cantidad de variaci√≥n o dispersi√≥n de un conjunto de valores de datos. Un desv√≠o est√°ndar cerca de 0 indica que los puntos de datos tienden a estar muy cerca de la media del conjunto, mientras que un alto desv√≠o est√°ndar indica que los puntos de datos se extienden a lo largo de un rango amplio de valores. Se calcula como la ra√≠z cuadrada de la varianza y con Pandas lo podemos obtener con el m√©todo std.\n# desvio est√°ndar faithful[\u0026#39;eruptions\u0026#39;].std() 1.1413712511052092 Cuartiles# Los cuartiles son los tres puntos que dividen el conjunto de datos en cuatro grupos iguales, cada grupo comprende un cuarto de los datos.El (Q1) se define como el n√∫mero medio entre el n√∫mero m√°s peque√±o y la mediana del conjunto de datos. El segundo cuartil (Q2) es la mediana de los datos. El tercer cuartil (Q3) es el valor medio entre la mediana y el valor m√°s alto del conjunto de datos. Para dividir nuestro dataset en sus cuartiles utilizamos el m√©todo quantile.\n# cuartiles faithful[\u0026#39;eruptions\u0026#39;].quantile([.25, .5, .75]) 0.25 2.16275 0.50 4.00000 0.75 4.45425 dtype: float64 Un gr√°fico relacionado a los cuartiles y describe varias caracter√≠sticas importantes al mismo tiempo, tales como la dispersi√≥n y simetr√≠a es el diagrama de caja. Para su realizaci√≥n se representan los tres cuartiles y los valores m√≠nimo y m√°ximo de los datos, sobre un rect√°ngulo, alineado horizontal o verticalmente. Podemos utilizar la funci√≥n boxplot de Seaborn para generarlo.\n# diagrama de cajas cajas=sns.boxplot(list(faithful[\u0026#39;eruptions\u0026#39;])) Hasta aqu√≠ hemos calculado medidas de dispersi√≥n para una sola variable, pero nuestro dataset tiene dos variables cuantitativas; veamos como podemos calcular medidas combinadas para la dos variables.\nCovarianza# La covarianza es el equivalente de la varianza aplicado a una variable bidimensional. Es la media aritm√©tica de los productos de las desviaciones de cada una de las variables respecto a sus medias.La covarianza indica el sentido de la correlaci√≥n entre las variables; Si es mayor que cero la correlaci√≥n es directa, en caso de ser menor, la correlaci√≥n es inversa. La podemos calcular utilizando el m√©todo cov.\n# covarianza faithful.cov() eruptions waiting eruptions 1.302728 13.977808 waiting 13.977808 184.823312 Correlaci√≥n# Por √∫ltimo, el coeficiente de correlaci√≥n es una medida del grado de dependencia lineal entre dos variables. El coeficiente de correlaci√≥n oscila entre -1 y 1. Un valor de 1 significa que una ecuaci√≥n lineal describe la relaci√≥n entre las dos variables a la perfecci√≥n, con todos los puntos de datos cayendo sobre una l√≠nea recta de pendiente positiva. Un valor de -1 implica que todos los puntos de datos se encuentran en una l√≠nea con pendiente negativa. Un valor de 0 implica que no existe una correlaci√≥n lineal entre las variables. Lo podemos calcular con el m√©todo corr.\n# coeficiente de correlaci√≥n faithful.corr() eruptions waiting eruptions 1.000000 0.900811 waiting 0.900811 1.000000 Como podemos ver las dos variables tienen una correlaci√≥n bastante alta, lo que sugiere que est√°n √≠ntimamente relacionadas; a la misma conclusi√≥n hab√≠amos llegado al observar el diagrama de dispersi√≥n.\nResumen estad√≠stico# Hasta aqu√≠ hemos calculado tanto las medidas de tendencia central como las medidas de dispersi√≥n una por una, pero ¬øno ser√≠a m√°s conveniente que con un simple comando podemos obtener un resumen estad√≠stico con las principales medidas? Es por esto que Pandas nos ofrece el m√©todo describe, un comando para gobernarlos a todos!, el cual nos ofrece un resumen con las principales medidas estad√≠sticas.\n# resumen estad√≠stico faithful[\u0026#39;eruptions\u0026#39;].describe() count 272.000000 mean 3.487783 std 1.141371 min 1.600000 25% 2.162750 50% 4.000000 75% 4.454250 max 5.100000 Name: eruptions, dtype: float64 Siguiendo la misma l√≠nea; Seaborn nos ofrece la funci√≥n pairplot que nos proporciona un resumen gr√°fico con histogramas y diagramas de dispersi√≥n de las variables de nuestro dataset.\npar= sns.pairplot(faithful) Con esto concluye este art√≠culo; ahora ya deber√≠an estar en condiciones de poder analizar tanto variables cuantitativas como variables categ√≥ricas. A practicar!\nSaludos!\nEste post fue escrito utilizando Jupyter notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2016-03-13","id":24,"permalink":"/blog/2016/03/13/analisis-de-datos-cuantitativos-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# En mi art√≠culo anterior, An√°lisis de datos categ√≥ricos con Python, mencione la importancia de reconocer los distintos tipos de datos con que nos podemos encontrar al realizar an√°lisis estad√≠sticos y vimos tambi√©n como podemos trabajar con los datos categ√≥ricos. En esta oportunidad, vamos a ver como podemos manipular, interpretar y obtener informaci√≥n de los datos cuantitativos.","tags":["python","estadistica","programacion","machine learning","analisis de datos"],"title":"An√°lisis de datos cuantitativos con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Cuando trabajamos con estad√≠sticas, es importante reconocer los diferentes tipos de datos: num√©ricos (discretos y continuos), categ√≥ricos y ordinales. Los datos no son m√°s que observaciones del mundo en que vivimos, por tanto, los mismos pueden venir en diferentes formas, no solo num√©rica. Por ejemplo, si le pregunt√°ramos a nuestros amigos ¬øcu√°ntas mascotas tienen? nos podr√≠an responder: 0, 1, 2, 4, 3, 8; esta informaci√≥n por s√≠ misma puede ser √∫til, pero para nuestro an√°lisis de mascotas, nos podr√≠a servir tambi√©n otro tipo de informaci√≥n, como por ejemplo el g√©nero de cada uno de nuestros amigos; de esta forma obtendr√≠amos la siguiente informaci√≥n: hombre, mujer, mujer, mujer, hombre, mujer. Como vemos, podemos incluir a los datos dentro de tres categor√≠as fundamentales: datos cuantitativos o num√©ricos, datos cualitativos o categ√≥ricos y datos ordinales.\nDatos cuantitativos# Los datos cuantitativos son representados por n√∫meros; estos n√∫meros van a ser significativos si representan la medida o la cantidad observada de cierta caracter√≠stica. Dentro de esta categor√≠a podemos encontrar por ejemplo: cantidades de d√≥lares, cuentas, tama√±os, n√∫mero de empleados, y kil√≥metros por hora. Con los datos cuantitativos, se puede hacer todo tipo de tareas de procesamiento de datos num√©ricos, tales como sumarlos, calcular promedios, o medir su variabilidad. Asimismo, vamos a poder dividir a los datos cuantitativos en discretos y continuos, dependiendo de los valores potencialmente observables.\nLos datos discretos solo van a poder asumir un valor de una lista de n√∫meros espec√≠ficos. Representan √≠tems que pueden ser contados; todos sus posibles valores pueden ser listados. Suele ser relativamente f√°cil trabajar con este tipo de dato.\nLos datos continuos representan mediciones; sus posibles valores no pueden ser contados y s√≥lo pueden ser descritos usando intervalos en la recta de los n√∫meros reales. Por ejemplo, la cantidad de kil√≥metros recorridos no puede ser medida con exactitud, puede ser que hayamos recorrido 1.7 km o 1.6987 km; en cualquier medida que tomemos del mundo real, siempre pueden haber peque√±as o grandes variaciones. Generalmente, los datos continuos se suelen redondear a un n√∫mero fijo de decimales para facilitar su manipulaci√≥n.\nDatos cualitativos# Si los datos nos dicen en cual de determinadas categor√≠as no num√©ricas nuestros √≠tems van a caer, entonces estamos hablando de datos cualitativos o categ√≥ricos; ya que los mismos van a representar determinada cualidad que los √≠tems poseen. Dentro de esta categor√≠a vamos a encontrar datos como: el sexo de una persona, el estado civil, la ciudad natal, o los tipos de pel√≠culas que le gustan. Los datos categ√≥ricos pueden tomar valores num√©ricos (por ejemplo, \u0026ldquo;1\u0026rdquo; para indicar \u0026ldquo;masculino\u0026rdquo; y \u0026ldquo;2\u0026rdquo; para indicar \u0026ldquo;femenino\u0026rdquo;), pero esos n√∫meros no tienen un sentido matem√°tico.\nDatos ordinales# Una categor√≠a intermedia entre los dos tipos de datos anteriores, son los datos ordinales. En este tipo de datos, va a existir un orden significativo, vamos a poder clasificar un primero, segundo, tercero, etc. es decir, que podemos establecer un ranking para estos datos, el cual posiblemente luego tenga un rol importante en la etapa de an√°lisis. Los datos se dividen en categor√≠as, pero los n√∫meros colocados en cada categor√≠a tienen un significado. Por ejemplo, la calificaci√≥n de un restaurante en una escala de 0 (bajo) a 5 (m√°s alta) estrellas representa datos ordinales. Los datos ordinales son a menudo tratados como datos categ√≥ricos, en el sentido que se suelen agrupar y ordenar. Sin embargo, a diferencia de los datos categ√≥ricos, los n√∫meros s√≠ tienen un significado matem√°tico.\nEn este art√≠culo me voy a centrar en el segundo grupo, los datos categ√≥ricos; veremos como podemos manipular f√°cilmente con la ayuda de Python estos datos para poder encontrar patrones, relaciones, tendencias y excepciones.\nAn√°lisis de datos categ√≥ricos con Python# Para ejemplificar el an√°lisis, vamos a utilizar nuestras habituales librer√≠as cient√≠ficas NumPy, Pandas, Matplotlib y Seaborn. Tambi√©n vamos a utilizar la librer√≠a pydataset, la cual nos facilita cargar los diferentes dataset para analizar.\nLa idea es realizar un an√°lisis estad√≠stico sobre los datos de los sobrevivientes a la tragedia del Titanic.\nLa tragedia del Titanic# El hundimiento del Titanic es uno de los naufragios m√°s infames de la historia. El 15 de abril de 1912, durante su viaje inaugural, el Titanic se hundi√≥ despu√©s de chocar con un iceberg, matando a miles de personas. Esta tragedia sensacional conmocion√≥ a la comunidad internacional y condujo a mejores normas de seguridad aplicables a los buques. Una de las razones por las que el naufragio dio lugar a semejante cantidad de muertes fue que no hab√≠a suficientes botes salvavidas para los pasajeros y la tripulaci√≥n. Aunque hubo alg√∫n elemento de suerte involucrada en sobrevivir al hundimiento, algunos grupos de personas ten√≠an m√°s probabilidades de sobrevivir que otros, como las mujeres, los ni√±os y la clase alta.\nEl siguiente dataset proporciona informaci√≥n sobre el destino de los pasajeros en el viaje fatal del trasatl√°ntico Titanic, que se resume de acuerdo con el nivel econ√≥mico (clase), el sexo, la edad y la supervivencia.\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # importando modulos necesarios %matplotlib inline import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from pydataset import data # parametros esteticos de seaborn sns.set_palette(\u0026#34;deep\u0026#34;, desat=.6) sns.set_context(rc={\u0026#34;figure.figsize\u0026#34;: (8, 4)}) # importando dataset titanic = data(\u0026#39;titanic\u0026#39;) # ver primeros 10 registros titanic.head(10) class age sex survived 1 1st class adults man yes 2 1st class adults man yes 3 1st class adults man yes 4 1st class adults man yes 5 1st class adults man yes 6 1st class adults man yes 7 1st class adults man yes 8 1st class adults man yes 9 1st class adults man yes 10 1st class adults man yes El problema con datos como estos, y en general con la mayor√≠a de las tablas de datos, es que nos presentan mucha informaci√≥n y no nos permiten ver que es lo que realmente sucede o sucedi√≥. Por tanto, deber√≠amos procesarla de alguna manera para hacernos una imagen de lo que los datos realmente representan y nos quieren decir; y que mejor manera para hacernos una imagen de algo que utilizar visualizaciones. Una buena visualizaci√≥n de los datos puede revelar cosas que es probable que no podamos ver en una tabla de n√∫meros y nos ayudar√° a pensar con claridad acerca de los patrones y relaciones que pueden estar escondidos en los datos. Tambi√©n nos va a ayudar a encontrar las caracter√≠sticas y patrones m√°s importantes o los casos que son realmente excepcionales y no deber√≠amos de encontrar.\nTablas de frecuencia# Para hacernos una imagen de los datos, lo primero que tenemos que hacer es agruparlos. Al armar diferentes grupos nos vamos acercando a la comprensi√≥n de los datos. La idea es ir amontonamos las cosas que parecen ir juntas, para poder ver como se distribuyen a trav√©s de las diferentes categor√≠as. Para los datos categ√≥ricos, agrupar es f√°cil; simplemente debemos contar el n√∫mero de √≠tems que corresponden a cada categor√≠a y apilarlos. Una forma en la que podemos agrupar nuestro dataset del Titanic es contando las diferentes clases de pasajeros. Podemos organizar estos conteos en una tabla de frecuencia, que registra los totales y los nombres de las categor√≠as utilizando la funci√≥n value_counts que nos proporciona Pandas del siguiente modo:\n# tabla de frecuencia de clases de pasajeros pd.value_counts(titanic[\u0026#39;class\u0026#39;]) 3rd class 706 1st class 325 2nd class 285 dtype: int64 Contar las cantidad de apariciones de cada categor√≠a puede ser √∫til, pero a veces puede resultar m√°s √∫til saber la fracci√≥n o proporci√≥n de los datos de cada categor√≠a, as√≠ que podr√≠amos entonces dividir los recuentos por el total de casos para obtener los porcentajes que representa cada categor√≠a.\nUna tabla de frecuencia relativa muestra los porcentajes, en lugar de los recuentos de los valores en cada categor√≠a. Ambos tipos de tablas muestran c√≥mo los casos se distribuyen a trav√©s de las categor√≠as. De esta manera, ellas describen la distribuci√≥n de una variable categ√≥rica, ya que enumeran las posibles categor√≠as y nos dicen con qu√© frecuencia se produce cada una de ellas.\n# tabla de frecuencia relativa de pasajeros 100 * titanic[\u0026#39;class\u0026#39;].value_counts() / len(titanic[\u0026#39;class\u0026#39;]) 3rd class 53.647416 1st class 24.696049 2nd class 21.656535 dtype: float64 Gr√°ficos de tartas y barras# Ahora que ya conocemos a las tablas de frecuencia ya estamos en condiciones de crear visualizaciones que realmente nos den una imagen de los datos, sus propiedades y sus relaciones. En este punto, debemos ser sumamente cuidadosos, ya que una mala visualizaci√≥n puede llegar a distorsionar nuestra comprensi√≥n, en lugar de ayudarnos. Las mejores visualizaciones de datos siguen un principio fundamental llamado el principio del √°rea. Este principio nos dice que el √°rea ocupada por cada parte del gr√°fico se debe corresponder con la magnitud del valor que representa. Violaciones del principio de √°rea son una forma com√∫n de mentir con estad√≠sticas. Dos gr√°ficos √∫tiles que podemos utilizar para representar nuestros datos y que cumplen con este principio son el gr√°fico de barras y el gr√°fico de tarta.\nGr√°fico de barras# El gr√°fico de barras nos ayuda a darnos una impresi√≥n visual m√°s precisa de la distribuci√≥n de nuestros datos. La altura de cada barra muestra el recuento de su categor√≠a. Los barras tienen el mismo ancho, por lo que sus alturas determinan sus √°reas, y estas √°reas son proporcionales a los recuentos en cada categor√≠a. De esta forma, podemos ver f√°cilmente que hab√≠a m√°s del doble de pasajeros de tercera clase, que de primera o segunda clase. Los gr√°ficos de barras hacen que este tipo de comparaciones sean f√°ciles y naturales. Veamos como podemos crearlos de forma sencilla utilizando el m√©todo plot dentro de un DataFrame de Pandas.\n# Gr√°fico de barras de pasajeros del Titanic plot = titanic[\u0026#39;class\u0026#39;].value_counts().plot(kind=\u0026#39;bar\u0026#39;, title=\u0026#39;Pasajeros del Titanic\u0026#39;) Si quisi√©ramos enfocarnos en la proporci√≥n relativa de los pasajeros de cada una de las clases, simplemente podemos sustituir a los recuentos con porcentajes y utilizar un gr√°fico de barras de frecuencias relativas.\n# gr√°fico de barras de frecuencias relativas. plot = (100 * titanic[\u0026#39;class\u0026#39;].value_counts() / len(titanic[\u0026#39;class\u0026#39;])).plot( kind=\u0026#39;bar\u0026#39;, title=\u0026#39;Pasajeros del Titanic %\u0026#39;) Gr√°fico de tartas# El gr√°fico de tarta muestra el total de casos como un c√≠rculo y luego corta este c√≠rculo en piezas cuyos tama√±os son proporcionales a la fracci√≥n que cada categor√≠a representa sobre el total de casos. Los gr√°fico de tarta dan una impresi√≥n r√°pida de c√≥mo todo un grupo se divide en grupos m√°s peque√±os. Lo podr√≠amos graficar del siguiente modo, tambi√©n utilizando el m√©todo plot:\n# Gr√°fico de tarta de pasajeros del Titanic plot = titanic[\u0026#39;class\u0026#39;].value_counts().plot(kind=\u0026#39;pie\u0026#39;, autopct=\u0026#39;%.2f\u0026#39;, figsize=(6, 6), title=\u0026#39;Pasajeros del Titanic\u0026#39;) Como se puede apreciar, con el gr√°fico de tarta no es tan f√°cil determinar que los pasajeros de tercera clase son m√°s que el doble que los de primera clase; tampoco es f√°cil determinar si hay m√°s pasajeros de primera o de segunda clase. Para este tipo de comparaciones, son mucho m√°s √∫tiles los gr√°ficos de barras.\nRelacionando variables categ√≥ricas# Al analizar la tragedia del Titanic, una de las preguntas que podr√≠amos hacer es ¬øexiste alguna relaci√≥n entre la clase de pasajeros y la posibilidad de alcanzar un bote salvavidas y sobrevivir a la tragedia? Para poder responder a esta pregunta, vamos a necesitar analizar a las variables class y survived de nuestro dataset en forma conjunta. Una buena forma de analizar dos variables categ√≥ricas en forma conjunta, es agrupar los recuentos en una tabla de doble entrada; este tipo de tablas se conocen en estad√≠stica con el nombre de tabla de contingencia. Veamos como podemos crear esta tabla utilizando la funci√≥n crosstab de Pandas.\n# Tabla de contingencia class / survived pd.crosstab(index=titanic[\u0026#39;survived\u0026#39;], columns=titanic[\u0026#39;class\u0026#39;], margins=True) class 1st class 2nd class 3rd class All survived no 122 167 528 817 yes 203 118 178 499 All 325 285 706 1316 Los m√°rgenes de la tabla, tanto en la derecha y en la parte inferior, nos muestran los totales. La l√≠nea inferior de la tabla representa la distribuci√≥n de frecuencia de la clase de pasajeros. La columna derecha de la tabla es la distribuci√≥n de frecuencia de la variable supervivencia. Cuando se presenta la informaci√≥n de este modo, cada celda de cada uno de los m√°rgenes de la tabla representa la distribuci√≥n marginal de esa variable en particular. Cada celda nos va a mostrar el recuento para la combinaci√≥n de los valores de nuestras dos variables categ√≥ricas, en este caso class y survived.\nAl igual de como hab√≠amos visto con las tablas de frecuencia, tambi√©n nos podr√≠a ser √∫til representar a las tablas de contingencia con porcentajes relativos; esto lo podr√≠amos realizar utilizando el m√©todo apply del siguiente modo:\n# tabla de contingencia en porcentajes relativos total pd.crosstab(index=titanic[\u0026#39;survived\u0026#39;], columns=titanic[\u0026#39;class\u0026#39;], margins=True).apply(lambda r: r/len(titanic) *100, axis=1) class 1st class 2nd class 3rd class All survived no 9.270517 12.689970 40.121581 62.082067 yes 15.425532 8.966565 13.525836 37.917933 All 24.696049 21.656535 53.647416 100.000000 Con esta tabla podemos ver f√°cilmente que solo el 37.91% de los pasajeros sobrevivi√≥ a la tragedia y que este 37% se compone de la siguiente forma: del total de pasajeros sobrevivi√≥ un 15.42% de pasajeros que eran de primera clase, un 8.97% que eran de segunda clase y un 13.52% que eran pasajeros de tercera clase.\nVolviendo a nuestra pregunta inicial sobre la posibilidad de sobrevivir seg√∫n la clase de pasajero, podr√≠a ser m√°s √∫til armar la tabla de porcentajes como un porcentaje relativo sobre el total de cada fila, es decir calcular el porcentaje relativo que cada clase tiene sobre haber sobrevivido o no. Esto lo podemos realizar del siguiente modo:\n# tabla de contingencia en porcentajes relativos segun sobreviviente pd.crosstab(index=titanic[\u0026#39;survived\u0026#39;], columns=titanic[\u0026#39;class\u0026#39;] ).apply(lambda r: r/r.sum() *100, axis=1) class 1st class 2nd class 3rd class survived no 14.932681 20.440636 64.626683 yes 40.681363 23.647295 35.671343 Aqu√≠ podemos ver que de los pasajeros que sobrevivieron a la tragedia, el 40.68% correspond√≠an a primera clase, el 35.67% a tercera clase y el 23.65% a segunda clase. Por tanto podr√≠amos inferir que los pasajeros de primera clase ten√≠an m√°s posibilidades de sobrevivir.\nEs m√°s, tambi√©n podr√≠amos armar la tabla de porcentaje relativos en relaci√≥n al total de cada clase de pasajero y as√≠ podr√≠amos ver que de los pasajeros de primera clase, logr√≥ sobrevivir un 62.46%.\n# tabla de contingencia en porcentajes relativos segun clase pd.crosstab(index=titanic[\u0026#39;survived\u0026#39;], columns=titanic[\u0026#39;class\u0026#39;] ).apply(lambda r: r/r.sum() *100, axis=0) class 1st class 2nd class 3rd class survived no 37.538462 58.596491 74.787535 yes 62.461538 41.403509 25.212465 Este √∫ltimo resultado lo podr√≠amos representar visualmente con simples gr√°fico de barras del siguiente modo:\n# Gr√°fico de barras de sobreviviviente segun clase plot = pd.crosstab(index=titanic[\u0026#39;class\u0026#39;], columns=titanic[\u0026#39;survived\u0026#39;]).apply(lambda r: r/r.sum() *100, axis=1).plot(kind=\u0026#39;bar\u0026#39;) # Gr√°fico de barras de sobreviviviente segun clase plot = pd.crosstab(index=titanic[\u0026#39;survived\u0026#39;], columns=titanic[\u0026#39;class\u0026#39;] ).apply(lambda r: r/r.sum() *100, axis=0).plot(kind=\u0026#39;bar\u0026#39;, stacked=True) Estas mismas manipulaciones las podemos realizar para otro tipo de combinaci√≥n de variables categ√≥ricas, como podr√≠a ser el sexo o la edad de los pasajeros, pero eso ya se los dejo a ustedes para que se entretengan y practiquen un rato.\nCon este termina esta art√≠culo, si les gust√≥ y est√°n interesados en la estad√≠sticas, no duden en visitar mi anterior art√≠culo Probabilidad y Estad√≠stica con Python y seguir la novedades del blog!\nSaludos!\nEste post fue escrito utilizando Jupyter notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2016-02-29","id":25,"permalink":"/blog/2016/02/29/analisis-de-datos-categoricos-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Cuando trabajamos con estad√≠sticas, es importante reconocer los diferentes tipos de datos: num√©ricos (discretos y continuos), categ√≥ricos y ordinales. Los datos no son m√°s que observaciones del mundo en que vivimos, por tanto, los mismos pueden venir en diferentes formas, no solo num√©rica.","tags":["python","estadistica","programacion","machine learning","analisis de datos"],"title":"An√°lisis de datos categ√≥ricos con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# El √Ålgebra lineal constituye la base de gran parte de las matem√°ticas modernas, ya sea en su fase te√≥rica, aplicada, o computacional. Es un √°rea activa que tiene conexiones con muchas √°reas dentro y fuera de las matem√°ticas, como ser: el an√°lisis funcional, las ecuaciones diferenciales, la investigaci√≥n operativa, la econometr√≠a y la ingenier√≠a. Es por esto, que se vuelve sumamente importante conocer sus m√©todos en profundidad.\nLa idea de este art√≠culo, es profundizar alguno de los temas que ya vimos en mi art√≠culo anterior (√Ålgebra lineal con Python), presentar algunos nuevos, e ilustrar la utilidad de esta rama de la matem√°ticas con alguna de sus aplicaciones.\nCampos# Un Campo, \\(F\\), es una estructura algebraica en la cual las operaciones de adici√≥n y multiplicaci√≥n se pueden realizar y cumplen con las siguientes propiedades:\nLa propiedad conmutativa tanto para la adici√≥n como para la multiplicaci√≥n; es decir: \\(a + b = b + a\\); y \\(a \\cdot b = b \\cdot a\\); para todo \\(a, b \\in F\\)\nLa propiedad asociativa, tanto para la adici√≥n como para la multiplicaci√≥n; es decir: \\((a + b) + c = a + (b + c)\\); y \\((a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)\\); para todo \\(a, b, c \\in F\\)\nLa propiedad distributiva de la multiplicaci√≥n sobre la adici√≥n; es decir: \\(a \\cdot (b + c) = a \\cdot b + a \\cdot c\\); para todo \\(a, b, c \\in F\\)\nLa existencia de un elemento neutro tanto para la adici√≥n como para la multiplicaci√≥n; es decir: \\(a + 0 = a\\); y \\(a \\cdot 1 = a\\); para todo \\(a \\in F\\).\nLa existencia de un elemento inverso tanto para la adici√≥n como para la multiplicaci√≥n; es decir: \\(a + (-a) = 0\\); y \\(a \\cdot a^{-1} = 1\\); para todo \\(a \\in F\\) y \\(a \\ne 0\\).\nDos de los Campos m√°s comunes con los que nos vamos a encontrar al trabajar en problemas de √Ålgebra lineal, van a ser el conjunto de los n√∫meros reales, \\(\\mathbb{R}\\); y el conjunto de los n√∫meros complejos, \\(\\mathbb{C}\\).\nVectores# Muchas nociones f√≠sicas, tales como las fuerzas, velocidades y aceleraciones, involucran una magnitud (el valor de la fuerza, velocidad o aceleraci√≥n) y una direcci√≥n. Cualquier entidad que involucre magnitud y direcci√≥n se llama vector. Los vectores se representan por flechas en las que la longitud de ellas define la magnitud; y la direcci√≥n de la flecha representa la direcci√≥n del vector. Podemos pensar en los vectores como una serie de n√∫meros. √âstos n√∫meros tienen una orden preestablecido, y podemos identificar cada n√∫mero individual por su √≠ndice en ese orden. Los vectores identifican puntos en el espacio, en donde cada elemento representa una coordenada del eje en el espacio. La t√≠pica forma de representarlos es la siguiente:\n$$v = \\left[ \\begin{array}{c} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{array} \\right]$$ Geom√©tricamente podemos representarlos del siguiente modo en el plano de 2 dimensiones:\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # importando modulos necesarios %matplotlib inline import matplotlib.pyplot as plt import numpy as np import scipy.sparse as sp import scipy.sparse.linalg import scipy.linalg as la import sympy # imprimir con notaci√≥n matem√°tica. sympy.init_printing(use_latex=\u0026#39;mathjax\u0026#39;) Ver C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # graficando vector en R^2 [2, 4] def move_spines(): \u0026#34;\u0026#34;\u0026#34;Crea la figura de pyplot y los ejes. Mueve las lineas de la izquierda y de abajo para que se intersecten con el origen. Elimina las lineas de la derecha y la de arriba. Devuelve los ejes.\u0026#34;\u0026#34;\u0026#34; fix, ax = plt.subplots() for spine in [\u0026#34;left\u0026#34;, \u0026#34;bottom\u0026#34;]: ax.spines[spine].set_position(\u0026#34;zero\u0026#34;) for spine in [\u0026#34;right\u0026#34;, \u0026#34;top\u0026#34;]: ax.spines[spine].set_color(\u0026#34;none\u0026#34;) return ax def vect_fig(vector, color): \u0026#34;\u0026#34;\u0026#34;Genera el grafico de los vectores en el plano\u0026#34;\u0026#34;\u0026#34; v = vector ax.annotate(\u0026#34; \u0026#34;, xy=v, xytext=[0, 0], color=color, arrowprops=dict(facecolor=color, shrink=0, alpha=0.7, width=0.5)) ax.text(1.1 * v[0], 1.1 * v[1], v) ax = move_spines() ax.set_xlim(-5, 5) ax.set_ylim(-5, 5) ax.grid() vect_fig([2, 4], \u0026#34;blue\u0026#34;) Combinaciones lineales# Cuando trabajamos con vectores, nos vamos a encontrar con dos operaciones fundamentales, la suma o adici√≥n; y la multiplicaci√≥n por escalares. Cuando sumamos dos vectores \\(v\\) y \\(w\\), sumamos elemento por elemento, del siguiente modo:\n$$v + w = \\left[ \\begin{array}{c} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{array} \\right] + \\left[ \\begin{array}{c} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{array} \\right] = \\left[ \\begin{array}{c} v_1 + w_1 \\\\ v_2 + w_2 \\\\ \\vdots \\\\ v_n + w_n \\end{array} \\right]$$ Geom√©tricamente lo podemos ver representado del siguiente modo:\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # graficando suma de vectores en R^2 # [2, 4] + [2, -2] ax = move_spines() ax.set_xlim(-5, 5) ax.set_ylim(-5, 5) ax.grid() vecs = [[2, 4], [2, -2]] # lista de vectores for v in vecs: vect_fig(v, \u0026#34;blue\u0026#34;) v = np.array([2, 4]) + np.array([2, -2]) vect_fig(v, \u0026#34;red\u0026#34;) ax.plot([2, 4], [-2, 2], linestyle=\u0026#39;--\u0026#39;) a =ax.plot([2, 4], [4, 2], linestyle=\u0026#39;--\u0026#39; ) Cuando multiplicamos vectores por escalares, lo que hacemos es tomar un n√∫mero \\(\\alpha\\) y un vector \\(v\\); y creamos un nuevo vector \\(w\\) en el cada elemento de \\(v\\) es multiplicado por \\(\\alpha\\) del siguiente modo:\n$$\\begin{split}\\alpha v = \\left[ \\begin{array}{c} \\alpha v_1 \\\\ \\alpha v_2 \\\\ \\vdots \\\\ \\alpha v_n \\end{array} \\right]\\end{split}$$ Geom√©tricamente podemos representar a esta operaci√≥n en el plano de 2 dimensiones del siguiente modo:\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # graficando multiplicaci√≥n por escalares en R^2 # [2, 3] * 2 ax = move_spines() ax.set_xlim(-6, 6) ax.set_ylim(-6, 6) ax.grid() v = np.array([2, 3]) vect_fig(v, \u0026#34;blue\u0026#34;) v = v * 2 vect_fig(v, \u0026#34;red\u0026#34;) Cuando combinamos estas dos operaciones, formamos lo que se conoce en √Ålgebra lineal como combinaciones lineales. Es decir que una combinaci√≥n lineal va a ser una expresi√≥n matem√°tica construida sobre un conjunto de vectores, en el que cada vector es multiplicado por un escalar y los resultados son luego sumados. Matem√°ticamente lo podemos expresar de la siguiente forma:\n$$w = \\alpha_1 v_1 + \\alpha_2 v_2 + \\dots + \\alpha_n v_n = \\sum_{i=1}^n \\alpha_i v_i $$ en donde, \\(v_n\\) son vectores y \\(\\alpha_n\\) son escalares.\nMatrices, combinaciones lineales y Ax = b# Una matriz es un arreglo bidimensional de n√∫meros ordenados en filas y columnas, donde una fila es cada una de las l√≠neas horizontales de la matriz y una columna es cada una de las l√≠neas verticales. En una matriz cada elemento puede ser identificado utilizando dos √≠ndices, uno para la fila y otro para la columna en que se encuentra. Las podemos representar de la siguiente manera:\n$$A=\\begin{bmatrix}a_{11} \u0026 a_{12} \u0026 \\dots \u0026 a_{1n}\\\\a_{21} \u0026 a_{22} \u0026 \\dots \u0026 a_{2n} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ a_{n1} \u0026 a_{n2} \u0026 \\dots \u0026 a_{nn}\\end{bmatrix}$$ Las matrices se utilizan para m√∫ltiples aplicaciones y sirven, en particular, para representar los coeficientes de los sistemas de ecuaciones lineales o para representar combinaciones lineales.\nSupongamos que tenemos los siguientes 3 vectores:\n$$x_1 = \\left[ \\begin{array}{c} 1 \\\\ -1 \\\\ 0 \\end{array} \\right] \\ x_2 = \\left[ \\begin{array}{c} 0 \\\\ 1 \\\\ -1 \\end{array} \\right] \\ x_3 = \\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ 1 \\end{array} \\right]$$ su combinaci√≥n lineal en el espacio de 3 dimensiones va a ser igual a \\(\\alpha_1 x_1 + \\alpha_2 x_2 + \\alpha_3 x_3\\); lo que es lo mismo que decir:\n$$\\alpha_1 \\left[ \\begin{array}{c} 1 \\\\ -1 \\\\ 0 \\end{array} \\right] + \\alpha_2 \\left[ \\begin{array}{c} 0 \\\\ 1 \\\\ -1 \\end{array} \\right] + \\alpha_3 \\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ 1 \\end{array} \\right] = \\left[ \\begin{array}{c} \\alpha_1 \\\\ \\alpha_2 - \\alpha_1 \\\\ \\alpha_3 - \\alpha_2 \\end{array} \\right]$$ Ahora esta combinaci√≥n lineal la podr√≠amos reescribir en forma matricial. Los vectores \\(x_1, x_2\\) y \\(x_3\\), pasar√≠an a formar las columnas de la matriz \\(A\\) y los escalares \\(\\alpha_1, \\alpha_2\\) y \\(\\alpha_3\\) pasar√≠an a ser los componentes del vector \\(x\\) del siguiente modo:\n$$\\begin{bmatrix}1 \u0026 0 \u0026 0\\\\-1 \u0026 1 \u0026 0 \\\\ 0 \u0026 -1 \u0026 1\\end{bmatrix}\\begin{bmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3\\end{bmatrix}= \\begin{bmatrix}\\alpha_1 \\\\ \\alpha_2 - \\alpha_1 \\\\ \\alpha_3 - \\alpha_2 \\end{bmatrix}$$ De esta forma la matriz \\(A\\) multiplicada por el vector \\(x\\), nos da como resultado la misma combinaci√≥n lineal \\(b\\). De esta forma, arribamos a una de las ecuaciones m√°s fundamentales del √Ålgebra lineal:\n$$Ax = b$$ Esta ecuaci√≥n no solo nos va a servir para expresar combinaciones lineales, sino que tambi√©n se vuelve de suma importancia a la hora de resolver sistemas de ecuaciones lineales, en d√≥nde \\(b\\) va a ser conocido y la inc√≥gnita pasa a ser \\(x\\). Por ejemplo, supongamos que queremos resolver el siguiente sistemas de ecuaciones de 3 inc√≥gnitas:\n$$ 2x_1 + 3x_2 + 5x_3 = 52 \\\\ 3x_1 + 6x_2 + 2x_3 = 61 \\\\ 8x_1 + 3x_2 + 6x_3 = 75 $$ Podemos ayudarnos de SymPy para expresar a la matriz \\(A\\) y \\(b\\) para luego arribar a la soluci√≥n del vector \\(x\\).\n# Resolviendo sistema de ecuaciones con SymPy A = sympy.Matrix(( (2, 3, 5), (3, 6, 2), (8, 3, 6) )) A $$\\left[\\begin{matrix}2 \u0026 3 \u0026 5\\\\3 \u0026 6 \u0026 2\\\\8 \u0026 3 \u0026 6\\end{matrix}\\right]$$ b = sympy.Matrix(3,1,(52,61,75)) b $$\\left[\\begin{matrix}52\\\\61\\\\75\\end{matrix}\\right]$$ # Resolviendo Ax = b x = A.LUsolve(b) x $$\\left[\\begin{matrix}3\\\\7\\\\5\\end{matrix}\\right]$$ # Comprobando la soluci√≥n A*x $$\\left[\\begin{matrix}52\\\\61\\\\75\\end{matrix}\\right]$$ La matriz identidad , la matriz transpuesta y la matriz invertible# Tres matrices de suma importancia en problemas de √Ålgebra lineal. Son la matriz identidad, la matriz transpuesta y la matriz invertible.\nLa matriz identidad es el elemento neutro en la multiplicaci√≥n de matrices, es el equivalente al n√∫mero 1. Cualquier matriz multiplicada por la matriz identidad nos da como resultado la misma matriz. La matriz identidad es una matriz cuadrada (tiene siempre el mismo n√∫mero de filas que de columnas); y su diagonal principal se compone de todos elementos 1 y el resto de los elementos se completan con 0. Suele representase con la letra \\(I\\).\nPor ejemplo la matriz identidad de 3x3 ser√≠a la siguiente:\n$$I=\\begin{bmatrix}1 \u0026 0 \u0026 0 \u0026 \\\\0 \u0026 1 \u0026 0\\\\ 0 \u0026 0 \u0026 1\\end{bmatrix}$$ La matriz transpuesta de una matriz \\(A\\) de \\(m \\times n\\) va a ser igual a la matriz \\(n \\times m\\) \\(A^T\\), la cual se obtiene al transformar las filas en columnas y las columnas en filas, del siguiente modo:\n$$\\begin{bmatrix}a \u0026 b \u0026 \\\\c \u0026 d \u0026 \\\\ e \u0026 f \u0026 \\end{bmatrix}^T= \\begin{bmatrix}a \u0026 c \u0026 e \u0026\\\\b \u0026 d \u0026 f \u0026 \\end{bmatrix}$$ Una matriz cuadrada va a ser sim√©trica si \\(A^T = A\\), es decir si \\(A\\) es igual a su propia matriz transpuesta.\nAlgunas de las propiedades de las matrices transpuestas son:\na. \\((A^T)^T = A\\)\nb. \\((A + B)^T = A^T + B^T\\)\nc. \\(k(A)^T = k(A^T)\\)\nd. \\((AB)^T = B^T A^T\\)\ne. \\((A^r)^T = (A^T)^r\\) para todos los \\(r\\) no negativos.\nf. Si \\(A\\) es una matriz cuadrada, entonces \\(A + A^T\\) es una matriz sim√©trica.\ng. Para cualquier matriz \\(A\\), \\(A A^T\\) y \\(A^T A\\) son matrices sim√©tricas.\nVeamos algunos ejemplos en Python\n# Matriz transpuesta A = sympy.Matrix( [[ 2,-3,-8, 7], [-2,-1, 2,-7], [ 1, 0,-3, 6]] ) A $$\\left[\\begin{matrix}2 \u0026 -3 \u0026 -8 \u0026 7\\\\-2 \u0026 -1 \u0026 2 \u0026 -7\\\\1 \u0026 0 \u0026 -3 \u0026 6\\end{matrix}\\right]$$ A.transpose() $$\\left[\\begin{matrix}2 \u0026 -2 \u0026 1\\\\-3 \u0026 -1 \u0026 0\\\\-8 \u0026 2 \u0026 -3\\\\7 \u0026 -7 \u0026 6\\end{matrix}\\right]$$ # transpuesta de transpuesta vuelve a A. A.transpose().transpose() $$\\left[\\begin{matrix}2 \u0026 -3 \u0026 -8 \u0026 7\\\\-2 \u0026 -1 \u0026 2 \u0026 -7\\\\1 \u0026 0 \u0026 -3 \u0026 6\\end{matrix}\\right]$$ # creando matriz simetrica As = A*A.transpose() As $$\\left[\\begin{matrix}126 \u0026 -66 \u0026 68\\\\-66 \u0026 58 \u0026 -50\\\\68 \u0026 -50 \u0026 46\\end{matrix}\\right]$$ # comprobando simetria. As.transpose() $$\\left[\\begin{matrix}126 \u0026 -66 \u0026 68\\\\-66 \u0026 58 \u0026 -50\\\\68 \u0026 -50 \u0026 46\\end{matrix}\\right]$$ La matriz invertible es muy importante, ya que esta relacionada con la ecuaci√≥n \\(Ax = b\\). Si tenemos una matriz cuadrada \\(A\\) de \\(n \\times n\\), entonces la matriz inversa de \\(A\\) es una matriz \\(A\u0026rsquo;\\) o \\(A^{-1}\\) de \\(n \\times n\\) que hace que la multiplicaci√≥n $A A^{-1}$ sea igual a la matriz identidad \\(I\\). Es decir que es la matriz rec√≠proca de \\(A\\).\n\\(A A^{-1} = I\\) o \\(A^{-1} A = I\\)\nEn caso de que estas condiciones se cumplan, decimos que la matriz es invertible.\nQue una matriz sea invertible tiene importantes implicaciones, como ser:\na. Si \\(A\\) es una matriz invertible, entonces su matriz inversa es √∫nica.\nb. Si \\(A\\) es una matriz invertible de \\(n \\times n\\), entonces el sistemas de ecuaciones lineales dado por \\(Ax = b\\) tiene una √∫nica soluci√≥n \\(x = A^{-1}b\\) para cualquier \\(b\\) en \\(\\mathbb{R}^n\\).\nc. Una matriz va a ser invertible si y solo si su determinante es distinto de cero. En el caso de que el determinante sea cero se dice que la matriz es singular.\nd. Si \\(A\\) es una matriz invertible, entonces el sistema \\(Ax = 0\\) solo tiene una soluci√≥n trivial. Es decir, en las que todas las inc√≥gnitas son ceros.\ne. Si \\(A\\) es una matriz invertible, entonces su forma escalonada va a ser igual a la matriz identidad.\nf. Si \\(A\\) es una matriz invertible, entonces \\(A^{-1}\\) es invertible y:\n$$(A^{-1})^{-1} = A$$ g. Si \\(A\\) es una matriz invertible y \\(\\alpha\\) es un escalar distinto de cero, entonces \\(\\alpha A\\) es invertible y:\n$$(\\alpha A)^{-1} = \\frac{1}{\\alpha}A^{-1}$$ .\nh. Si \\(A\\) y \\(B\\) son matrices invertibles del mismo tama√±o, entonces \\(AB\\) es invertible y:\n$$(AB)^{-1} = B^{-1} A^{-1}$$ .\ni. Si \\(A\\) es una matriz invertible, entonces \\(A^T\\) es invertible y:\n$$(A^T)^{-1} = (A^{-1})^T$$ .\nCon SymPy podemos trabajar con las matrices invertibles del siguiente modo:\n# Matriz invertible A = sympy.Matrix( [[1,2], [3,9]] ) A $$\\left[\\begin{matrix}1 \u0026 2\\\\3 \u0026 9\\end{matrix}\\right]$$ A_inv = A.inv() A_inv $$\\left[\\begin{matrix}3 \u0026 - \\frac{2}{3}\\\\-1 \u0026 \\frac{1}{3}\\end{matrix}\\right]$$ # A * A_inv = I A*A_inv $$\\left[\\begin{matrix}1 \u0026 0\\\\0 \u0026 1\\end{matrix}\\right]$$ # forma escalonada igual a indentidad. A.rref() $$\\left ( \\left[\\begin{matrix}1 \u0026 0\\\\0 \u0026 1\\end{matrix}\\right], \\quad \\left ( 0, \\quad 1\\right )\\right )$$ # la inversa de A_inv es A A_inv.inv() $$\\left[\\begin{matrix}1 \u0026 2\\\\3 \u0026 9\\end{matrix}\\right]$$ Espacios vectoriales# Las Matem√°ticas derivan su poder en gran medida de su capacidad para encontrar las caracter√≠sticas comunes de los diversos problemas y estudiarlos de manera abstracta. Existen muchos problemas que implican los conceptos relacionados de adici√≥n, multiplicaci√≥n por escalares, y la linealidad. Para estudiar estas propiedades de manera abstracta, debemos introducir la noci√≥n de espacio vectorial.\nPara alcanzar la definici√≥n de un espacio vectorial, debemos combinar los conceptos que venimos viendo hasta ahora de Campo, vector y las operaciones de adici√≥n; y multiplicaci√≥n por escalares. De esta forma un espacio vectorial, \\(V\\), sobre un Campo, \\(F\\), va a ser un conjunto en el que est√°n definidas las operaciones de adici√≥n y multiplicaci√≥n por escalares, tal que para cualquier par de elementos \\(x\\) e \\(y\\) en \\(V\\), existe un elemento √∫nico \\(x + y\\) en \\(V\\), y para cada elemento \\(\\alpha\\) en \\(F\\) y cada elemento \\(x\\) en \\(V\\), exista un √∫nico elemento \\(\\alpha x\\) en \\(V\\), de manera que se cumplan las siguientes condiciones:\nPara todo \\(x, y\\) en \\(V\\), \\(x + y = y + x\\) (conmutatividad de la adici√≥n).\nPara todo \\(x, y, z\\) en \\(V\\), \\((x + y) + z = x + (y + z)\\). (asociatividad de la adici√≥n).\nExiste un elemento en \\(V\\) llamado \\(0\\) tal que \\(x + 0 = x\\) para todo \\(x\\) en \\(V\\).\nPara cada elemento \\(x\\) en \\(V\\), existe un elemento \\(y\\) en \\(V\\) tal que \\(x + y = 0\\).\nPara cada elemento \\(x\\) en \\(V\\), \\(1 x = x\\).\nPara cada par, \\(\\alpha, \\beta\\) en \\(F\\) y cada elemento \\(x\\) en \\(V\\), \\((\\alpha \\beta) x = \\alpha (\\beta x)\\).\nPara cada elemento \\(\\alpha\\) en \\(F\\) y cada para de elementos \\(x, y\\) en \\(V\\), \\(\\alpha(x + y) = \\alpha x + \\alpha y\\).\nPara cada par de elementos \\(\\alpha, \\beta\\) en \\(F\\) y cada elemento \\(x\\) en \\(V\\), \\((\\alpha + \\beta)x = \\alpha x + \\beta x\\).\nLos espacios vectoriales m√°s comunes son \\(\\mathbb{R}^2\\), el cual representa el plano de 2 dimensiones y consiste de todos los pares ordenados de los n√∫meros reales:\n$$\\mathbb{R}^2 = \\{(x, y): x, y \\in \\mathbb{R}\\}$$ y \\(\\mathbb{R}^3\\), que representa el espacio ordinario de 3 dimensiones y consiste en todos los tr√≠os ordenados de los n√∫meros reales:\n$$\\mathbb{R}^3 = \\{(x, y, z): x, y, z \\in \\mathbb{R}\\}$$ Una de las grandes bellezas del √Ålgebra lineal es que podemos f√°cilmente pasar a trabajar sobre espacios de \\(n\\) dimensiones, \\(\\mathbb{R}^n\\)!\nTampoco tenemos porque quedarnos con solo los n√∫meros reales, ya que la definici√≥n que dimos de un espacio vectorial reside sobre un Campo; y los campos pueden estar representados por n√∫meros complejos. Por tanto tambi√©n podemos tener espacios vectoriales \\(\\mathbb{C}^2, \\mathbb{C}^3, \\dots, \\mathbb{C}^n\\).\nSubespacios# Normalmente, en el estudio de cualquier estructura algebraica es interesante examinar subconjuntos que tengan la misma estructura que el conjunto que esta siendo considerado. As√≠, dentro de los espacios vectoriales, podemos tener subespacios vectoriales, los cuales son un subconjunto que cumplen con las mismas propiedades que el espacio vectorial que los contiene. De esta forma, \\(\\mathbb{R}^3\\) representa un subespacio del espacio vectorial \\(\\mathbb{R}^n\\).\nIndependencia lineal# La independencia lineal es un concepto aparentemente simple con consecuencias que se extienden profundamente en muchos aspectos del an√°lisis. Si deseamos entender cuando una matriz puede ser invertible, o cuando un sistema de ecuaciones lineales tiene una √∫nica soluci√≥n, o cuando una estimaci√≥n por m√≠nimos cuadrados se define de forma √∫nica, la idea fundamental m√°s importante es la de independencia lineal de vectores.\nDado un conjunto finito de vectores \\(x_1, x_2, \\dots, x_n\\) se dice que los mismos son linealmente independientes, si y solo si, los √∫nicos escalares \\(\\alpha_1, \\alpha_2, \\dots, \\alpha_n\\) que satisfacen la ecuaci√≥n:\n$$\\alpha_1 x_1 + \\alpha_2 x_2 + \\dots + \\alpha_n x_n = 0$$ son todos ceros, \\(\\alpha_1 = \\alpha_2 = \\dots = \\alpha_n = 0\\).\nEn caso de que esto no se cumpla, es decir, que existe una soluci√≥n a la ecuaci√≥n de arriba en que no todos los escalares son ceros, a esta soluci√≥n se la llama no trivial y se dice que los vectores son linealmente dependientes.\nPara ilustrar la definici√≥n y que quede m√°s clara, veamos algunos ejemplos. Supongamos que queremos determinar si los siguientes vectores son linealmente independientes:\n$$\\begin{split}x_1 = \\left[ \\begin{array}{c} 1.2 \\\\ 1.1 \\\\ \\end{array} \\right] \\ \\ \\ x_2 = \\left[ \\begin{array}{c} -2.2 \\\\ 1.4 \\\\ \\end{array} \\right]\\end{split}$$ Para lograr esto, deber√≠amos resolver el siguiente sistema de ecuaciones y verificar si la √∫nica soluci√≥n es aquella en que los escalares sean ceros.\n$$\\begin{split}\\alpha_1 \\left[ \\begin{array}{c} 1.2 \\\\ 1.1 \\\\ \\end{array} \\right] + \\alpha_2 \\left[ \\begin{array}{c} -2.2 \\\\ 1.4 \\\\ \\end{array} \\right]\\end{split} = 0 $$ Para resolver este sistema de ecuaciones, podemos recurrir a la ayuda de Python.\n# Resolviendo el sistema de ecuaciones. A = np.array([[1.2, -2.2], [1.1, 1.4]]) b = np.array([0., 0.]) x = np.linalg.solve(A, b) x array([0., 0.]) Ver C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # Soluci√≥n gr√°fica. x_vals = np.linspace(-5, 5, 50) # crea 50 valores entre 0 y 5 ax = move_spines() ax.set_xlim(-5, 5) ax.set_ylim(-5, 5) ax.grid() ax.plot(x_vals, (1.2 * x_vals) / -2.2) # grafica 1.2x_1 - 2.2x_2 = 0 a = ax.plot(x_vals, (1.1 * x_vals) / 1.4) # grafica 1.1x + 1.4x_2 = 0 Como podemos ver, tanto por la soluci√≥n num√©rica como por la soluci√≥n gr√°fica, estos vectores son linealmente independientes, ya que la √∫nica soluci√≥n a la ecuaci√≥n \\(\\alpha_1 x_1 + \\alpha_2 x_2 + \\dots + \\alpha_n x_n = 0\\), es aquella en que los escalares son cero.\nDeterminemos ahora si por ejemplo, los siguientes vectores en \\(\\mathbb{R}^4\\) son linealmente independientes: \\({(3, 2, 2, 3), (3, 2, 1, 2), (3, 2, 0, 1)}\\). Aqu√≠, ahora deber√≠amos resolver la siguiente ecuaci√≥n:\n$$\\alpha_1 (3, 2, 2, 3) +\\alpha_2 (3, 2, 1, 2) + \\alpha_3 (3, 2, 0, 1) = (0, 0, 0, 0)$$ Para resolver este sistema de ecuaciones que no es cuadrado (tiene 4 ecuaciones y solo 3 inc√≥gnitas); podemos utilizar SymPy.\n# Sympy para resolver el sistema de ecuaciones lineales a1, a2, a3 = sympy.symbols(\u0026#39;a1, a2, a3\u0026#39;) A = sympy.Matrix(( (3, 3, 3, 0), (2, 2, 2, 0), (2, 1, 0, 0), (3, 2, 1, 0) )) A $$\\left[\\begin{matrix}3 \u0026 3 \u0026 3 \u0026 0\\\\2 \u0026 2 \u0026 2 \u0026 0\\\\2 \u0026 1 \u0026 0 \u0026 0\\\\3 \u0026 2 \u0026 1 \u0026 0\\end{matrix}\\right]$$ sympy.solve_linear_system(A, a1, a2, a3) $$\\left \\{ a_{1} : a_{3}, \\quad a_{2} : - 2 a_{3}\\right \\}$$ Como vemos, esta soluci√≥n es no trivial, ya que por ejemplo existe la soluci√≥n $\\alpha_1 = 1, \\ \\alpha_2 = -2 , \\ \\alpha_3 = 1$ en la que los escalares no son ceros. Por lo tanto este sistema es linealmente dependiente.\nPor √∫ltimo, podr√≠amos considerar si los siguientes polinomios son linealmente independientes: \\(1 -2x -x^2\\), \\(1 + x\\), \\(1 + x + 2x^2\\). En este caso, deber√≠amos resolver la siguiente ecuaci√≥n:\n$$\\alpha_1 (1 ‚àí 2x ‚àí x^2) + \\alpha_2 (1 + x) + \\alpha_3 (1 + x + 2x^2) = 0$$ y esta ecuaci√≥n es equivalente a la siguiente:\n$$(\\alpha_1 + \\alpha_2 + \\alpha_3 ) + (‚àí2 \\alpha_1 + \\alpha_2 + \\alpha_3 )x + (‚àí\\alpha_1 + 2 \\alpha_2 )x^2 = 0$$ Por lo tanto, podemos armar el siguiente sistema de ecuaciones:\n$$\\alpha_1 + \\alpha_2 + \\alpha_3 = 0, \\\\ -2 \\alpha_1 + \\alpha_2 + \\alpha_3 = 0, \\\\ -\\alpha_1 + 2 \\alpha_2 = 0. $$ El cual podemos nuevamente resolver con la ayuda de SymPy.\nA = sympy.Matrix(( (1, 1, 1, 0), (-2, 1, 1, 0), (-1, 2, 0, 0) )) A $$\\left[\\begin{matrix}1 \u0026 1 \u0026 1 \u0026 0\\\\-2 \u0026 1 \u0026 1 \u0026 0\\\\-1 \u0026 2 \u0026 0 \u0026 0\\end{matrix}\\right]$$ sympy.solve_linear_system(A, a1, a2, a3) $$\\left \\{ a_{1} : 0, \\quad a_{2} : 0, \\quad a_{3} : 0\\right \\}$$ Como vemos, todos los escalares son ceros, por lo tanto estos polinomios son linealmente independientes.\nEspacio nulo, espacio columna y espacio fila# Un termino particularmente relacionado con la independencia lineal es el de espacio nulo o n√∫cleo. El espacio nulo de una matriz \\(A\\), el cual lo vamos a expresar como \\(N(A)\\), va a consistir de todas las soluciones a la ecuaci√≥n fundamental \\(Ax = 0\\). Por supuesto, una soluci√≥n inmediata a esta ecuaci√≥n es el caso de \\(x = 0\\), que ya vimos que establece la independencia lineal. Esta soluci√≥n solo va a ser la √∫nica que exista para los casos de matrices invertibles. Pero en el caso de las matrices singulares (aquellas que no son invertibles, que tienen determinante igual a cero), van a existir soluciones que no son cero para la ecuaci√≥n \\(Ax = 0\\). El conjunto de todas estas soluciones, va a representar el espacio nulo.\nPara encontrar el espacio nulo tambi√©n nos podemos ayudar de SymPy.\n# Espacio nulo de un matriz A = sympy.Matrix(((1, 5, 7), (0, 0, 9))) A $$\\left[\\begin{matrix}1 \u0026 5 \u0026 7\\\\0 \u0026 0 \u0026 9\\end{matrix}\\right]$$ # Calculando el espacio nulo x = A.nullspace() x $$\\left [ \\left[\\begin{matrix}-5\\\\1\\\\0\\end{matrix}\\right]\\right ]$$ # Comprobando la soluci√≥n A_aum = sympy.Matrix(((1, 5, 7, 0), (0, 0, 9, 0))) sympy.solve_linear_system(A_aum, a1, a2, a3) $$\\left \\{ a_{1} : - 5 a_{2}, \\quad a_{3} : 0\\right \\}$$ # Comprobaci√≥n con numpy A = np.array([[1, 5, 7], [0, 0, 9]]) x = np.array([[-5], [1], [0]]) A.dot(x) array([[0], [0]]) Otro espacio de suma importancia es el espacio columna. El espacio columna, \\(C(A)\\), consiste en todas las combinaciones lineales de las columnas de una matriz \\(A\\). Estas combinaciones son los posibles vectores \\(Ax\\). Este espacio es fundamental para resolver la ecuaci√≥n \\(Ax = b\\); ya que para resolver esta ecuaci√≥n debemos expresar a \\(b\\) como una combinaci√≥n de columnas. El sistema \\(Ax = b\\), va a tener soluci√≥n solamente si \\(b\\) esta en el espacio columna de \\(A\\). Como las matrices tienen la forma \\(m \\times n\\), sus columnas tienen \\(m\\) componentes (\\(n\\) son las filas). Por lo tanto el espacio columna es un subespacio de \\(\\mathbb{R}^m\\) y no \\(\\mathbb{R}^n\\).\nPor √∫ltimo, el otro espacio que conforma los espacios fundamentales de una matriz, es el espacio fila, el cual esta constituido por las combinaciones lineales de las filas de una matriz.\nPara obtener estos espacios, nuevamente podemos recurrir a SymPy. Para poder obtener estos espacios, primero vamos a tener que obtener la forma escalonada de la matriz, la cual es la forma a la que arribamos luego del proceso de eliminaci√≥n.\n# A.rref() forma escalonada. A = sympy.Matrix( [[2,-3,-8, 7], [-2,-1,2,-7], [1 ,0,-3, 6]]) A.rref() # [0, 1, 2] es la ubicaci√≥n de las pivot. $$\\left ( \\left[\\begin{matrix}1 \u0026 0 \u0026 0 \u0026 0\\\\0 \u0026 1 \u0026 0 \u0026 3\\\\0 \u0026 0 \u0026 1 \u0026 -2\\end{matrix}\\right], \\quad \\left ( 0, \\quad 1, \\quad 2\\right )\\right )$$ # Espacio columna [ A[:,c] for c in A.rref()[1] ] $$\\left [ \\left[\\begin{matrix}2\\\\-2\\\\1\\end{matrix}\\right], \\quad \\left[\\begin{matrix}-3\\\\-1\\\\0\\end{matrix}\\right], \\quad \\left[\\begin{matrix}-8\\\\2\\\\-3\\end{matrix}\\right]\\right ]$$ # Espacio fila [ A.rref()[0][r,:] for r in A.rref()[1] ] $$\\left [ \\left[\\begin{matrix}1 \u0026 0 \u0026 0 \u0026 0\\end{matrix}\\right], \\quad \\left[\\begin{matrix}0 \u0026 1 \u0026 0 \u0026 3\\end{matrix}\\right], \\quad \\left[\\begin{matrix}0 \u0026 0 \u0026 1 \u0026 -2\\end{matrix}\\right]\\right ]$$ Rango# Otro concepto que tambi√©n esta ligado a la independencia lineal es el de rango. Los n√∫meros de columnas \\(m\\) y filas \\(n\\) pueden darnos el tama√±o de una matriz, pero esto no necesariamente representa el verdadero tama√±o del sistema lineal, ya que por ejemplo si existen dos filas iguales en una matriz \\(A\\), la segunda fila desaparec√≠a en el proceso de eliminaci√≥n. El verdadero tama√±o de \\(A\\) va a estar dado por su rango. El rango de una matriz es el n√∫mero m√°ximo de columnas (filas respectivamente) que son linealmente independientes. Por ejemplo si tenemos la siguiente matriz de 3 x 4:\n$$A = \\begin{bmatrix}1 \u0026 1 \u0026 2 \u0026 4\\\\1 \u0026 2 \u0026 2 \u0026 5 \\\\ 1 \u0026 3 \u0026 2 \u0026 6\\end{bmatrix}$$ Podemos ver que la tercer columna \\((2, 2, 2)\\) es un m√∫ltiplo de la primera y que la cuarta columna \\((4, 5, 6)\\) es la suma de las primeras 3 columnas. Por tanto el rango de \\(A\\) va a ser igual a 2; ya que la tercer y cuarta columna pueden ser eliminadas.\nObviamente, el rango tambi√©n lo podemos calcular con la ayuda de Python.\n# Calculando el rango con SymPy A = sympy.Matrix([[1, 1, 2, 4], [1, 2, 2, 5], [1, 3, 2, 6]]) A $$\\left[\\begin{matrix}1 \u0026 1 \u0026 2 \u0026 4\\\\1 \u0026 2 \u0026 2 \u0026 5\\\\1 \u0026 3 \u0026 2 \u0026 6\\end{matrix}\\right]$$ # Rango con SymPy A.rank() $$2$$ # Rango con numpy A = np.array([[1, 1, 2, 4], [1, 2, 2, 5], [1, 3, 2, 6]]) np.linalg.matrix_rank(A) 2 Una √∫til aplicaci√≥n de calcular el rango de una matriz es la de determinar el n√∫mero de soluciones al sistema de ecuaciones lineales, de acuerdo al enunciado del Teorema de Rouch√©‚ÄìFrobenius. El sistema tiene por lo menos una soluci√≥n si el rango de la matriz de coeficientes equivale al rango de la matriz aumentada. En ese caso, √©sta tiene exactamente una soluci√≥n si el rango equivale al n√∫mero de inc√≥gnitas.\nLa norma y la Ortogonalidad# Si quisi√©ramos saber cual es el largo del un vector, lo √∫nico que necesitamos es el famoso teorema de Pit√°goras. En el plano \\(\\mathbb{R}^2\\), el largo de un vector \\(v=\\begin{bmatrix}a \\ b \\end{bmatrix}\\) va a ser igual a la distancia desde el origen \\((0, 0)\\) hasta el punto \\((a, b)\\). Esta distancia puede ser f√°cilmente calculada gracias al teorema de Pit√°goras y va ser igual a \\(\\sqrt{a^2 + b^2}\\), como se puede ver en la siguiente figura:\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # Calculando largo de un vector # forma un tri√°ngulo rect√°ngulo ax = move_spines() ax.set_xlim(-6, 6) ax.set_ylim(-6, 6) ax.grid() v = np.array([4, 6]) vect_fig(v, \u0026#34;blue\u0026#34;) a = ax.vlines(x=v[0], ymin=0, ymax = 6, linestyle=\u0026#39;--\u0026#39;, color=\u0026#39;g\u0026#39;) En esta definici√≥n podemos observar que \\(a^2 + b^2 = v \\cdot v\\), por lo que ya estamos en condiciones de poder definir lo que en √Ålgebra lineal se conoce como norma.\nEl largo o norma de un vector \\(v = \\begin{bmatrix} v_1 \\ v_2 \\ \\vdots \\ v_n \\end{bmatrix}\\), en \\(\\mathbb{R}^n\\) va a ser igual a un n√∫mero no negativo \\(||v||\\) definido por:\n$$||v|| = \\sqrt{v \\cdot v} = \\sqrt{v_1^2 + v_2^2 + \\dots + v_n^2}$$ Es decir que la norma de un vector va a ser igual a la ra√≠z cuadrada de la suma de los cuadrados de sus componentes.\nOrtogonalidad# El concepto de perpendicularidad es fundamental en geometr√≠a. Este concepto llevado a los vectores en \\(\\mathbb{R}^n\\) se llama ortogonalidad.\nDos vectores \\(v\\) y \\(w\\) en \\(\\mathbb{R}^n\\) van a ser ortogonales el uno al otro si su producto interior es igual a cero. Es decir, \\(v \\cdot w = 0\\).\nGeom√©tricamente lo podemos ver de la siguiente manera:\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # Vectores ortogonales ax = move_spines() ax.set_xlim(-6, 6) ax.set_ylim(-6, 6) ax.grid() vecs = [np.array([4, 6]), np.array([-3, 2])] for v in vecs: vect_fig(v, \u0026#34;blue\u0026#34;) a = ax.plot([-3, 4], [2, 6], linestyle=\u0026#39;--\u0026#39;, color=\u0026#39;g\u0026#39;) # comprobando su producto interior. v = np.array([4, 6]) w = np.array([-3, 2]) v.dot(w) 0 Un conjunto de vectores en \\(\\mathbb{R}^n\\) va a ser ortogonal si todo los pares de los distintos vectores en el conjunto son ortogonales entre s√≠. O sea:\n\\(v_i \\cdot v_j = 0\\) para todo \\(i, j = 1, 2, \\dots, k\\) y donde \\(i \\ne j\\).\nPor ejemplo, si tenemos el siguiente conjunto de vectores en \\(\\mathbb{R}^3\\):\n$$v1 = \\begin{bmatrix} 2 \\\\ 1 \\\\ -1\\end{bmatrix} \\ v2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1\\end{bmatrix} \\ v3 = \\begin{bmatrix} 1 \\\\ -1 \\\\ 1\\end{bmatrix}$$ En este caso, deber√≠amos combrobar que:\n$$v1 \\cdot v2 = 0 \\\\ v2 \\cdot v3 = 0 \\\\ v1 \\cdot v3 = 0 $$ # comprobando ortogonalidad del conjunto v1 = np.array([2, 1, -1]) v2 = np.array([0, 1, 1]) v3 = np.array([1, -1, 1]) v1.dot(v2), v2.dot(v3), v1.dot(v3) (0, 0, 0) Como vemos, este conjunto es ortogonal. Una de las principales ventajas de trabajar con conjuntos de vectores ortogonales es que los mismos son necesariamente linealmente independientes.\nEl concepto de ortogonalidad es uno de los m√°s importantes y √∫tiles en √Ålgebra lineal y surge en muchas situaciones pr√°cticas, sobre todo cuando queremos calcular distancias.\nDeterminante# El determinante es un n√∫mero especial que puede calcularse sobre las matrices cuadradas. Este n√∫mero nos va a decir muchas cosas sobre la matriz. Por ejemplo, nos va decir si la matriz es invertible o no. Si el determinante es igual a cero, la matriz no es invertible. Cuando la matriz es invertible, el determinante de \\(A^{-1}= 1/(\\det \\ A)\\). El determinante tambi√©n puede ser √∫til para calcular √°reas.\nPara obtener el determinante de una matriz debemos calcular la suma de los productos de las diagonales de la matriz en una direcci√≥n menos la suma de los productos de las diagonales en la otra direcci√≥n. Se represente con el s√≠mbolo \\(|A|\\) o \\(\\det A\\).\nAlgunas de sus propiedades que debemos tener en cuenta son:\na. El determinante de la matriz identidad es igual a 1. \\(\\det I = 1\\).\nb. Una matriz \\(A\\) es singular (no tiene inversa) si su determinante es igual a cero.\nc. El determinante cambia de signo cuando dos columnas(o filas) son intercambiadas.\nd. Si dos filas de una matriz \\(A\\) son iguales, entonces el determinante es cero.\ne. Si alguna fila de la matriz \\(A\\) son todos ceros, entonces el determinante es cero.\nf. La matriz transpuesta \\(A^T\\), tiene el mismo determinante que \\(A\\).\ng. El determinante de \\(AB\\) es igual al determinante de \\(A\\) multiplicado por el determinante de \\(B\\). \\(\\det (AB) = \\det A \\cdot \\det B\\).\nh. El determinante es una funci√≥n lineal de cada una de las filas en forma separada. Si multiplicamos solo una fila por \\(\\alpha\\), entonces el determinante tambi√©n es multiplicado por \\(\\alpha\\).\nVeamos como podemos obtener el determinante con la ayuda de Python\n# Determinante con sympy A = sympy.Matrix( [[1, 2, 3], [2,-2, 4], [2, 2, 5]] ) A.det() $$2$$ # Determinante con numpy A = np.array([[1, 2, 3], [2,-2, 4], [2, 2, 5]] ) np.linalg.det(A) $$1.9999999999999998$$ # Determinante como funcion lineal de fila A[0] = A[0:1]*5 np.linalg.det(A) $$9.999999999999998$$ # cambio de signo de determinante A = sympy.Matrix( [[2,-2, 4], [1, 2, 3], [2, 2, 5]] ) A.det() $$-2$$ Eigenvalores y Eigenvectores# Cuando estamos resolviendo ecuaciones lineales del tipo \\(Ax = b\\), estamos trabajando con problemas est√°ticos. ¬øPero qu√© pasa si quisi√©ramos trabajar con problemas din√°micos?. Es en este tipo de situaciones donde los Eigenvalores y Eigenvectores tienen su mayor importancia.\nSupongamos que tenemos una matriz cuadrada \\(A\\) de \\(n \\times n\\). Una pregunta natural que nos podr√≠amos hacer sobre \\(A\\) es: ¬øExiste alg√∫n vector \\(x\\) distinto de cero para el cual \\(Ax\\) es un escalar m√∫ltiplo de \\(x\\)?. Si llevamos esta pregunta al lenguaje matem√°tico nos vamos a encontrar con la siguiente ecuaci√≥n:\n$$Ax = \\lambda x$$ Cuando esta ecuaci√≥n es v√°lida y \\(x\\) no es cero, decimos que \\(\\lambda\\) es el Eigenvalor o valor propio de \\(A\\) y \\(x\\) es su correspondiente Eigenvector o vector propio.\nMuchos problemas en ciencia derivan en problemas de Eigenvalores, en los cuales la principal pregunta es: ¬øCu√°les son los Eigenvalores de una matriz dada, y cu√°les son sus correspondientes Eigenvectores. Un √°rea donde nos va a ser de mucha utilidad esta teor√≠a, es en problemas con sistemas de ecuaciones diferenciales lineales.\nCalculando Eigenvalores# Hasta aqu√≠ todo muy bien, pero dada una matriz cuadrada \\(A\\) de \\(n \\times n\\), ¬øc√≥mo podemos obtener sus Eigenvalores?.\nPodemos comenzar por observar que la ecuaci√≥n \\(Ax = \\lambda x\\) es equivalente a \\((A - \\lambda I)x = 0\\). Dado que estamos interesados en soluciones a esta ecuaci√≥n que sean distintas de cero, la matriz \\(A - \\lambda I\\) debe ser singular, no invertible, por lo tanto su determinante debe ser cero, \\(\\det (A - \\lambda I) = 0\\). De esta forma, podemos utilizar esta ecuaci√≥n para encontrar los Eigenvalores de \\(A\\). Particularmente, podr√≠amos formar el polinomio caracter√≠stico de la matriz \\(A\\), el cual va a tener grado \\(n\\) y por lo tanto va a tener \\(n\\) soluciones, es decir que vamos a encontrar \\(n\\) Eigenvalores. Algo que debemos tener en cuenta es, que a pesar de que la matriz \\(A\\) sea real, debemos estar preparados para encontrar Eigenvalores que sean complejos.\nPara que quede m√°s claro, veamos un ejemplo de como podemos calcular los Eigenvalores. Supongamos que tenemos la siguiente matriz:\n$$A = \\begin{bmatrix} 3 \u0026 2 \\\\ 7 \u0026 -2 \\end{bmatrix}$$ Su polinomio caracter√≠stico va a ser igual a:\n$$p(\\lambda) = \\det (A - \\lambda I) = \\det \\begin{bmatrix}3 - \\lambda \u0026 2 \\\\ 7 \u0026 -2-\\lambda\\end{bmatrix} = (3 - \\lambda)(-2-\\lambda) - 14 \\\\ =\\lambda^2 - \\lambda - 20 = (\\lambda - 5) (\\lambda + 4)$$ Por lo tanto los Eigenvalores de \\(A\\) van a ser \\(5\\) y \\(-4\\).\nObviamente, tambi√©n los podemos obtener mucho m√°s f√°cilmente con la ayuda de Python.\n# Eigenvalores con numpy A = np.array([[3, 2], [7, -2]]) x, v = np.linalg.eig(A) # x Eigenvalor, v Eigenvector x, v (array([ 5., -4.]), array([[ 0.70710678, -0.27472113], [ 0.70710678, 0.96152395]])) # Eigenvalores con SymPy A = sympy.Matrix([[3, 2], [7, -2]]) # Eigenvalor A.eigenvals() $$\\left \\{ -4 : 1, \\quad 5 : 1\\right \\}$$ # Eigenvector A.eigenvects() $$\\left [ \\left ( -4, \\quad 1, \\quad \\left [ \\left[\\begin{matrix}- \\frac{2}{7}\\\\1\\end{matrix}\\right]\\right ]\\right ), \\quad \\left ( 5, \\quad 1, \\quad \\left [ \\left[\\begin{matrix}1\\\\1\\end{matrix}\\right]\\right ]\\right )\\right ]$$ # comprobando la soluci√≥n Ax = Œªx # x eigenvector, v eigenvalue x = A.eigenvects()[0][2][0] v = A.eigenvects()[0][0] # Ax == vx A*x, v*x $$\\left ( \\left[\\begin{matrix}\\frac{8}{7}\\\\-4\\end{matrix}\\right], \\quad \\left[\\begin{matrix}\\frac{8}{7}\\\\-4\\end{matrix}\\right]\\right )$$ Con esto termino con este recorrido por los principales conceptos del √Ålgebra lineal, muchos de los cuales veremos en pr√≥ximos art√≠culos que tienen muchas aplicaciones interesantes. Espero que les sea de utilidad y les sirva de referencia.\nSaludos!\nEste post fue escrito utilizando Jupyter notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2016-02-10","id":26,"permalink":"/blog/2016/02/10/mas-algebra-lineal-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# El √Ålgebra lineal constituye la base de gran parte de las matem√°ticas modernas, ya sea en su fase te√≥rica, aplicada, o computacional. Es un √°rea activa que tiene conexiones con muchas √°reas dentro y fuera de las matem√°ticas, como ser: el an√°lisis funcional, las ecuaciones diferenciales, la investigaci√≥n operativa, la econometr√≠a y la ingenier√≠a.","tags":["python","algebra","programacion","machine learning","redes neuronales","matematica","calculo","matrices","vectores","ecuaciones diferenciales"],"title":"M√°s √Ålgebra lineal con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Nuestra comprensi√≥n de los procesos fundamentales de la naturaleza se basa en gran medida en Ecuaciones en derivadas parciales. Ejemplos de ello son las vibraciones de los s√≥lidos, la din√°mica de los fluidos, la difusi√≥n de los productos qu√≠micos, la propagaci√≥n del calor, la estructura de las mol√©culas, las interacciones entre fotones y electrones, y la radiaci√≥n de ondas electromagn√©ticas. Las Ecuaciones en derivadas parciales tambi√©n juegan un papel central en las matem√°ticas modernas, especialmente en la geometr√≠a y el an√°lisis; lo que las convierte en una herramienta de suma utilidad que debemos conocer.\nNota: Este art√≠culo corresponde a la tercer entrega de mi serie de art√≠culos sobre C√°lculo con Python; los anteriores fueron: Introducci√≥n al C√°lculo y Ecuaciones Diferenciales con Python, los cuales es recomendable haber le√≠do previamente.\n¬øQu√© es una ecuaci√≥n en derivadas parciales?# Una Ecuaci√≥n en derivadas parciales es una ecuaci√≥n que, como su nombre lo indica, contiene derivadas parciales. A diferencia de lo que hab√≠amos visto con las ecuaciones diferenciales ordinarias, en donde la funci√≥n inc√≥gnita depende solo de una variable; en las Ecuaciones en derivadas parciales, o EDP para abreviar, la funci√≥n inc√≥gnita va a depender de dos o m√°s variables independientes \\(x, y, \\dots\\). Generalmente a la funci√≥n inc√≥gnita la vamos a expresar como \\(u(x, y, \\dots)\\) y a sus derivadas parciales como \\(\\partial u / \\partial x = u_x\\) o \\(\\partial u / \\partial y = u_y\\) dependiendo de sobre que variable estemos derivando. Entonces una Ecuaci√≥n en derivadas parciales va a ser la identidad que relaciona a las variables independientes (\\(x, y, \\dots\\)), con la variable dependiente \\(u\\) (nuestra funci√≥n inc√≥gnita), y las derivadas parciales de \\(u\\). Lo podr√≠amos expresar de la siguiente forma:\n$$F(x, y, u(x, y), u_x(x, y), u_y(x, y)) = F(x, y, u, u_x, u_y) = 0$$ Al igual de como pasaba con las ecuaciones diferenciales ordinarias, el orden de una Ecuaci√≥n en derivadas parciales va a estar dado por la mayor derivada presente. Por lo tanto, el caso que expresamos m√°s arriba corresponde a una EDP, con dos variables independientes, de primer orden. Si quisi√©ramos expresar una EDP de segundo orden, podr√≠amos hacerlo de la siguiente manera:\n$$F(x, y, u, u_x, u_y, u_{xx}, u_{xy}, u_{yy})=0.$$ Clasificaci√≥n de ecuaciones en derivadas parciales# La clasificaci√≥n de las Ecuaciones en derivadas parciales va a ser algo fundamental, ya que la teor√≠a y los m√©todos para poder solucionarlas van a depender de la clase de ecuaci√≥n con la que estemos tratando. Las clasificaciones m√°s importantes que debemos tener en cuenta, son:\n1- El orden de la EDP: Como ya mencionamos, el mismo va a estar dado por el orden de la mayor derivada presente.\n2- N√∫mero de variables: Esta clasificaci√≥n va a estar dada por la cantidad de variables independientes que contenga la EDP.\n3- Linealidad: Esta es una de las clasificaciones m√°s importantes, vamos a poder clasificar a las Ecuaciones en derivadas parciales en lineales o no lineales. En las lineales, la variable dependiente \\(u\\) y todas sus derivadas, van a aparecer en una forma lineal, es decir, que van a tener grado uno (no van a estar elevadas al cuadrado, o multiplicadas entre s√≠). M√°s precisamente, una Ecuaci√≥n en derivadas parciales de segundo orden con dos variables, va a tomar la siguiente forma:\n$$Au_{xx} + Bu_{xy} + Cu_{yy} + Du_x + Eu_y + Fu = G$$ en donde \\(A, B, C, D, E, F,\\) y \\(G\\) pueden ser constantes o una funci√≥n dada de \\(x\\) e \\(y\\). Por ejemplo:\n\\(u_{tt} = e^tu_{xx} + \\sin t\\), ser√≠a una ecuaci√≥n lineal.\n\\(uu_{xx} + u_y = 0\\), ser√≠a una ecuaci√≥n no lineal.\n\\(u_{xx} + yu_{yy} + u_x = 0\\), ser√≠a una ecuaci√≥n lineal.\n\\(xu_x + yu_y + u^2 = 0\\), ser√≠a una ecuaci√≥n no lineal.\nTipos de ecuaciones lineales: Asimismo, a las Ecuaciones en derivadas parciales lineales de segundo orden las vamos a poder subdividir en las siguientes categor√≠as:\nEcuaciones parab√≥licas: Las cuales van a describir el flujo del calor y el proceso de difusi√≥n. √âstas ecuaciones van a satisfacer la condici√≥n \\(B^2 -4AC = 0\\).\nEcuaciones hiperb√≥licas: Las cuales describen los sistemas de vibraci√≥n y los movimientos de ondas. Satisfacen la condici√≥n \\(B^2 -4AC \u0026gt; 0\\).\nEcuaciones El√≠pticas: Las cuales describen los fen√≥menos de estados estacionarios y satisfacen la condici√≥n \\(B^2 -4AC \u0026lt; 0\\).\n4- Homogeneidad: Otra clasificaci√≥n que podemos utilizar, es la de homogeneidad. Una ecuaci√≥n va a ser homog√©nea, si el lado derecho de la ecuaci√≥n, \\(G(x, y)\\), es id√©nticamente cero para todo \\(x\\) e \\(y\\). En caso contrario, se la llama no homog√©nea.\n5- Tipos de coeficientes: Por √∫ltimo, podemos clasificar a las EDP de acuerdo a sus coeficientes \\(A, B, C, D, E, \\) y \\(F\\), si los mismos son constantes, se dice que la ecuaci√≥n es de coeficientes constantes, en caso contrario ser√° de coeficientes variables.\n¬øC√≥mo resolver ecuaciones en derivadas parciales?# Existen varios m√©todos que podemos utilizar para intentar resolver las Ecuaciones en derivadas parciales, la principal idea detr√°s la mayor√≠a de estos m√©todos es la transformar a estas ecuaciones en ecuaciones diferenciales ordinarias (EDO), o en alguna ecuaci√≥n algebraica; las cuales son m√°s sencillas de resolver. Algunos los m√©todos que podemos utilizar son:\n1- El m√©todo de separaci√≥n de variables: Este m√©todo es uno de los m√°s importantes y m√°s productivos a la hora de resolver a las Ecuaciones en derivadas parciales. La idea es reducir a la EDP de \\(n\\) variables, en \\(n\\) EDOs.\n2- El m√©todo de la transformada integral: Este m√©todo es similar al que ya vimos al resolver EDOs. La idea es aplicar una transformada integral para reducir una EDP de \\(n\\) variables, en otra de \\(n - 1\\) variables. De esta forma una EDP de 2 variables, puede ser transformada en una EDO.\n3- El m√©todo del cambio de coordenadas: Este m√©todo intenta cambiar a la EDP original en una EDO o en otra EDP m√°s sencilla de resolver por medio del cambio de coordenadas del problema.\n4- El m√©todo de perturbaci√≥n: Este m√©todo aplica la teor√≠a perturbacional para intentar cambiar un problema de EDP no lineal en una serie de problemas lineales que se aproximan al no lineal original.\n5- El m√©todo de expansi√≥n de autofunciones: Este m√©todo intentan encontrar la soluci√≥n de una EDP como una suma infinita de autofunciones. Estas autofunciones son halladas por medio de la resoluci√≥n del problema del valor propio que corresponde al problema original.\n6- El m√©todo de las ecuaciones integrales: Este m√©todo convierte a la EDP en una ecuaci√≥n integral; la cual luego es resuelta aplicando ciertas t√©cnicas particulares que se aplican a ese tipo ecuaciones.\n7- M√©todos num√©ricos: La mayor√≠a de las t√©cnicas para resolver num√©ricamente a las EDP se basan en la idea de discretizar el problema en cada variable independiente que se produce en la EDP, y de esta forma, reformular el problema en una forma algebraica. Esto usualmente resulta en problemas de √°lgebra lineal de gran escala. Dos de las t√©cnicas principales para reformular las EDP a una forma algebraica son, los m√©todos de diferencias finitas (MDF), donde las derivadas del problema son aproximadas por medio de la f√≥rmula de diferencias finitas; y los m√©todos de los elementos finitos (MEF), en donde la funci√≥n inc√≥gnita se escribe como combinaci√≥n lineal de funciones de base simple que pueden ser derivadas e integradas f√°cilmente. En muchos casos, estos m√©todos num√©ricos van a ser las √∫nicas herramientas que podamos utilizar para resolver a las EDP.\nSoluci√≥n de ecuaciones en derivadas parciales b√°sicas# Como resolver este tipo de ecuaciones es una tarea realmente complicada, vamos a empezar por resolver anal√≠ticamente las m√°s f√°ciles para ganar confianza y as√≠ luego poder pasar a ecuaciones m√°s complicadas.\nLa m√°s simple de las Ecuaciones en derivadas parciales que nos podemos encontrar es:\n\\(u_x = 0\\), en donde \\(u = u(x, y)\\).\nEsta ecuaci√≥n nos dice que la derivada parcial de \\(u\\) con respecto a \\(x\\) es cero, lo que significa que \\(u\\) no depende de \\(x\\). Por lo tanto, la soluci√≥n de esta ecuaci√≥n va a ser \\(u=f(y)\\), en donde \\(f\\) es una funci√≥n arbitraria de una variable. Por ejemplo, \\(u = y^2 - y\\) podr√≠a ser una posible soluci√≥n.\nSubiendo un poco m√°s la complejidad, podemos pasar a una EDP de segundo orden, como la siguiente:\n\\(u_{xx} = 0\\)\nEn este caso, podemos integrar una vez \\(u_{xx}\\) para obtener \\(u_x(x, y) = f(y)\\). Si volvemos a integrar este resultado, podemos arribar a la soluci√≥n final \\(u(x, y) = f(y)x + g(y)\\), en donde \\(f\\) y \\(f\\) son dos funciones arbitrarias.\nPor √∫ltimo, si quisi√©ramos resolver la siguiente EDP:\n\\(u_{xy} = 0\\)\nPrimero integramos con respecto a \\(x\\) tomando a \\(y\\) como fija, de esta forma obtenemos \\(u_y(x, y) = f(y)\\). Luego podemos integrar con respecto a \\(y\\) tomando a \\(x\\) como fija, y llegamos a la soluci√≥n:\n\\(u(x, y) = F(y) + g(x)\\), en donde \\(F\u0026rsquo; = f\\)\nComo podemos ver de estos ejemplos, las soluciones anal√≠ticas de las EDP dependen de funciones arbitrarias (en lugar de constantes arbitrarias como era el caso de las EDO). Por lo tanto vamos a necesitar condiciones auxiliares para poder determinar una √∫nica soluci√≥n.\nLa condici√≥n inicial y la condici√≥n de frontera# Al igual que nos pasaba cuando vimos las ecuaciones diferenciales ordinarias; las EDP pueden tener muchas soluciones, pero a nosotros nos va a interesar encontrar la soluci√≥n para un caso particular; para lograr esto, debemos imponer unas condiciones auxiliares al problema original. Estas condiciones van a estar motivadas por la F√≠sica del problema que estemos analizando y pueden llevar a ser de dos tipos diferentes: condiciones iniciales y condiciones de frontera.\nLa condici√≥n inicial va a establecer el estado del problema al momento de tiempo cero, \\(t_0\\). Por ejemplo para el problema de difusi√≥n, la condici√≥n inicial va a ser:\n$$u(x, t_0) = \\phi(x)$$ donde \\(\\phi(x)= \\phi(x, y, z)\\) es una funci√≥n que puede representar el estado de concentraci√≥n inicial. Para el problema del flujo del calor, \\(\\phi(x)\\) va a representar la temperatura inicial.\nLa condici√≥n de frontera nos va a delimitar el dominio en el que nuestra EDP es v√°lida. As√≠ por ejemplo, volviendo al problema de difusi√≥n, el dominio en el que nuestra EDP es v√°lida, puede estar delimitado por la superficie del objeto que contiene al l√≠quido. Existen varios tipos de condiciones de frontera, de las cuales las m√°s importantes son:\nLa condici√≥n de frontera de Dirichlet, en d√≥nde los valores v√°lidos de la funci√≥n inc√≥gnita \\(u\\) son especificados.\nLa condici√≥n de frontera de Neumann, en donde los valores v√°lidos especificados son dados para alguna de las derivadas de \\(u\\).\nLa condici√≥n de frontera de Robin, en donde los valores v√°lidos son especificados por una combinaci√≥n lineal de una funci√≥n y las derivadas de \\(u\\).\nInterpretaci√≥n geom√©trica de EDP de primer orden# Las EDP de primer orden poseen una interpretaci√≥n geom√©trica la cual nos puede facilitar alcanzar una soluci√≥n general para ellas.\nCoeficientes constantes# Tomemos la siguiente ecuaci√≥n de coeficientes constantes:\n\\(au_x + bu_y = 0\\), en donde \\(a\\) y \\(b\\) son constantes y ambas no pueden ser cero.\nEn esta ecuaci√≥n la cantidad \\(au_x + bu_y\\) es la derivada direccional de \\(u\\) en la direcci√≥n del vector \\(V = (a, b) = ai + bj\\). Como esta cantidad tiene que ser cero, esto significa que \\(u (x, y)\\) debe ser constante en la direcci√≥n de \\(V\\). El vector \\((b, -a)\\) es ortogonal a \\(V\\), por lo tanto, las l√≠neas paralelas a \\(V\\) (ver el gr√°fico m√°s abajo) tienen las ecuaciones \\(bx - ay\\) constantes. (Se las llama las l√≠neas caracter√≠sticas.) Entonces, la soluci√≥n es constante en cada una de esas l√≠neas. Por lo tanto, \\(u (x, y)\\) depende de \\(bx - ay\\) solamente. De esta forma podemos llegar a la soluci√≥n general para este tipo de ecuaciones, que va a ser:\n\\(u(x, y) = f(bx - ay)\\), en donde \\(f\\) es una funci√≥n arbitraria de una variable.\nEntonces, si por ejemplo, quisi√©ramos resolver la EDP, \\(4u_x - 3u_y= 0\\), con la condici√≥n de frontera que $u(0, y) = y^3$. Podemos aplicar la soluci√≥n general que obtuvimos arriba y llegar al resultado \\(u(x, y) = f(-3x - 4y)\\). Ahora, solo nos faltar√≠a aplicar la condici√≥n para poder determinar cual es la funci√≥n arbitraria \\(f\\). Si sustituimos $x=0$ en nuestra soluci√≥n, obtenemos \\(y^3 = f(-4y)\\). Si decimos que \\(z = -4y\\), entonces nuestra funci√≥n es \\(f(z) = -z^3 / 64\\). Por lo tanto la soluci√≥n de nuestra EDP que satisface la condici√≥n de frontera es \\(u(x, y) = (3x + 4y)^3 / 64\\).\nCoeficientes variables# Consideremos ahora la siguiente EDP de coeficientes variables:\n\\(u_x + yu_y = 0\\)\nEsta ecuaci√≥n es similar a la que vimos anteriormente, con la diferencia de que ahora tenemos al coeficiente variable \\(y\\). Utilizando la misma intuici√≥n geom√©trica que usamos antes, podemos ver que aqu√≠ tambi√©n la derivada direccional de \\(u\\) en el vector \\(v=(1, y)\\) es constante, pero esta vez el vector no es constante, sino que es variable con \\(y\\). Las curvas que tienen a \\(v\\) como su vector tangente, tienen pendiente \\(y/1\\), es decir:\n\\(\\frac{dy}{dx} = \\frac{y}{1}\\).\nPodemos resolver esta ecuaci√≥n como una EDO y as√≠ obtener:\n\\(y = Ce^x\\), o lo que es lo mismo \\(e^{-x}y = C\\).\nLa soluci√≥n de nuestra EDP entonces va a ser constante en estas curvas caracter√≠sticas (ver gr√°fico); y van a responder a la siguiente soluci√≥n general:\n\\(u(x, y) = f(e^{-x}y)\\), en donde \\(f\\) es una funci√≥n arbitraria.\nResolviendo ecuaciones en derivadas parciales con Python# Es tiempo de nuevamente recurrir a nuestros queridos paquetes cient√≠ficos de Python, NumPy, Matplotlib, SymPy y SciPy para ayudarnos a resolver las Ecuaciones en derivadas parciales. As√≠ como en el caso de las Ecuaciones diferenciales ordinarias vimos que exist√≠a dentro del paquete SymPy, el solucionador gen√©rico sympy.dsolve; para el caso de las EDP, vamos a tener al solucionador sympy.pdsolve. Aunque en este caso, es mucho m√°s limitado que su versi√≥n para EDO; ya que solo vamos a poder resolver EDP de primer orden. Veamos como funciona:\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # importando modulos necesarios %matplotlib inline import matplotlib.pyplot as plt import matplotlib as mpl import numpy as np import sympy # imprimir con notaci√≥n matem√°tica. sympy.init_printing(use_latex=\u0026#39;mathjax\u0026#39;) # Defino las variables x, y, u, z = sympy.symbols(\u0026#39;x y u z\u0026#39;) f = sympy.Function(\u0026#39;f\u0026#39;) # Defino la EDP 4u_x - 3u_y = 0 u = f(x, y) u_x = u.diff(x) u_y = u.diff(y) eq = sympy.Eq(4*u_x - 3*u_y) eq $$4 \\frac{\\partial}{\\partial x} f{\\left (x,y \\right )} - 3 \\frac{\\partial}{\\partial y} f{\\left (x,y \\right )} = 0$$ # Resuelvo la ecuaci√≥n sympy.pdsolve(eq) $$f{\\left (x,y \\right )} = F{\\left (- 3 x - 4 y \\right )}$$ # Defino la EDP u_x + yu_y = 0 u = f(x, y) u_x = u.diff(x) u_y = u.diff(y) eq2 = sympy.Eq(u_x + y*u_y) eq2 $$y \\frac{\\partial}{\\partial y} f{\\left (x,y \\right )} + \\frac{\\partial}{\\partial x} f{\\left (x,y \\right )} = 0$$ # Resuelvo la ecuaci√≥n sympy.pdsolve(eq2) $$f{\\left (x,y \\right )} = F{\\left (y e^{- x} \\right )}$$ # Calsificaci√≥n de EDP. sympy.classify_pde(eq2) ('1st_linear_variable_coeff',) Como podemos comprobar, obtuvimos los mismos resultados utilizando sympy.pdsolve que en nuestro an√°lisis manual con la interpretaci√≥n geom√©trica. Otra limitaci√≥n que vamos a tener al trabajar con sympy.pdsolve es que no podemos aplicar nuestras condiciones auxiliares al problema, por lo que para despejar la funci√≥n arbitraria, deber√≠amos hacer un trabajo manual. Asimismo, SymPy tambi√©n nos ofrece la funci√≥n classify_pde la cual nos ayuda a saber con que tipo de EDP estamos tratando. (Recordemos que pdsolve solo puede resolver EDPs de primer orden).\nSeparaci√≥n de variables# Otra forma en que nos podemos ayudar de SymPy para resolver EDPs, es utilizando el m√©todo de separaci√≥n de variables. La caracter√≠stica esencial de esta t√©cnica es transformar la EDP en un conjunto de EDOs (las cuales podemos solucionar con la ayuda de SymPy). De esta forma, la soluci√≥n requerida de la EDP se expresa como un producto \\(u (x, y) = X (x) Y (y) \\ne 0\\), o como una suma \\(u (x, y) = X (x) + Y (y)\\), donde \\(X (x)\\) e \\(Y (y)\\) son funciones de \\(x\\) e \\(y\\), respectivamente. Muchos de los problemas significativos en ecuaciones en derivadas parciales pueden ser resueltos por este m√©todo. Para ilustrar como funciona esta t√©cnica, veamos un ejemplo. Vamos a resolver la siguiente EDP.\n\\(y^2u_x^2 + x^2u_y^2 = (xyu)^2\\), que cumple con la condici√≥n \\(u(x, 0) = 3e^{x^2/4}\\).\nPodemos entonces asumir que \\(u(x, y) = f(x) g(y) \\ne 0\\) es una soluci√≥n separable de ella; por tanto lo reemplazamos en la ecuaci√≥n para obtener:\n$$y^2(f'(x) g(y))^2 + x^2(f(x) g'(y))^2 = x^2 y^2 (f(x) g(y))^2$$ lo que es equivalente a decir;\n$$\\frac{1}{x^2}\\left(\\frac{f'(x)}{f(x)}\\right)^2 + \\frac{1}{y^2}\\left(\\frac{g'(y)}{g(y)}\\right)^2 = 1$$ Luego, ayud√°ndonos de la constante de separaci√≥n \\(\\lambda^2\\), podemos separar a esta ecuaci√≥n en dos EDOs, del siguiente modo; primero igualamos la ecuaci√≥n anterior a \\(\\lambda^2\\):\n$$\\frac{1}{x^2}\\left(\\frac{f'(x)}{f(x)}\\right)^2 = 1 - \\frac{1}{y^2}\\left(\\frac{g'(y)}{g(y)}\\right)^2 = \\lambda^2$$ y luego separamos ambas ecuaciones para obtener:\n$$\\frac{1}{x}\\frac{f'(x)}{f(x)} = \\lambda \\\\ \\frac{g'(x)}{yg(y)}= \\sqrt{1 - \\lambda^2}$$ Ahora podemos utilizar sympy.dsolve para resolver ambas EDOs:\n# EDO n¬∞ 1 edo1 = sympy.Eq((1 / x) * (f(x).diff(x)/f(x)) - z) edo1 $$- z + \\frac{\\frac{d}{d x} f{\\left (x \\right )}}{x f{\\left (x \\right )}} = 0$$ # Resolviendo EDO n¬∞ 1 sympy.dsolve(edo1) $$f{\\left (x \\right )} = C_{1} e^{\\frac{x^{2} z}{2}}$$ # EDO n¬∞ 2 edo2 = sympy.Eq((f(y).diff(y)) / (y*f(y)) - sympy.sqrt(1 - z**2)) edo2 $$- \\sqrt{- z^{2} + 1} + \\frac{\\frac{d}{d y} f{\\left (y \\right )}}{y f{\\left (y \\right )}} = 0$$ # Resolviendo EDO n¬∞ 2 sympy.dsolve(edo2) $$\\left [ f{\\left (y \\right )} = C_{1} e^{- \\frac{1}{2} \\sqrt{y^{4} \\left(- z^{2} + 1\\right)}}, \\quad f{\\left (y \\right )} = C_{1} e^{\\frac{1}{2} \\sqrt{y^{4} \\left(- z^{2} + 1\\right)}}\\right ]$$ Entonces ahora podemos utilizar estos resultados para armar la soluci√≥n final a nuestra EDP original, el cual va a ser:\n$$u(x, y) = C e^{\\frac{\\lambda}{2}x^2 + \\frac{1}{2}y^2\\sqrt{1 - \\lambda^2}}$$ En donde \\(C = C_1 C_2\\) es una constante arbitraria.\nPor √∫ltimo, utilizando la condici√≥n \\(u(x, 0) = 3e^{x^2/4}\\), podemos despejar tanto a \\(C\\) (\\(C=3\\)) como a \\(\\lambda\\) (\\(\\lambda = 1/2\\)) y arribar a la soluci√≥n final:\n$$u(x, y) = 3 e^{\\frac{1}{4}\\left(x^2 + y^2\\sqrt{3}\\right)}$$ Si bien debemos realizar un trabajo manual previo, aun as√≠ SymPy sigue siendo de gran ayuda para facilitarnos llegar a la soluci√≥n final.\nM√©todos num√©ricos - M√©todo de Elementos Finitos# Por √∫ltimo, para cerrar este art√≠culo, veamos como podemos aplicar el m√©todos de los elementos finitos (MEF) con Python. Para esto nos vamos ayudar de la librer√≠a FEniCS, la cual es un framework para resolver num√©ricamente problemas generales de EDP utilizando el m√©todos de los elementos finitos. Para instalar esta librer√≠a en Ubuntu, pueden utilizar los siguientes comandos:\nsudo add-apt-repository ppa:fenics-packages/fenics sudo apt-get update sudo apt-get install fenics Deben tener en cuenta que por ahora solo funciona con Python 2. La interfaz principal que vamos a utilizar para trabajar con este framework nos la proporcionan las librer√≠as dolfin y mshr; las cuales debemos importar para poder trabajar con el. Una vez importadas, podemos configurar algunos de sus par√°metros para lograr el comportamiento deseado.\n# importando modulos de fenics import dolfin import mshr dolfin.parameters[\u0026#34;reorder_dofs_serial\u0026#34;] = False dolfin.parameters[\u0026#34;allow_extrapolation\u0026#34;] = True El problema que vamos a resolver con la ayuda de FEniCS, va a ser la siguiente EDP:\n$$u_{xx} + u_{yy} = 0$$ Con las siguientes condiciones de frontera:\n$$u(x=0) = 3 ; \\ u(x=1)=-1 ; \\ u(y=0) = -5 ; \\ u(y=1) = 5$$ El primer paso en la soluci√≥n de una EDP utilizando el m√©todos de los elementos finitos, es definir una malla que describa la discretizaci√≥n del dominio del problema. Para este caso, vamos a utilizar la funci√≥n RectangleMesh que nos ofrece FEniCS.\n# Discretizando el problema N1 = N2 = 75 mesh = dolfin.RectangleMesh(dolfin.Point(0, 0), dolfin.Point(1, 1), N1, N2) El siguiente paso es definir una representaci√≥n del espacio funcional para las funciones de ensayo y prueba. Para esto vamos a utilizar la clase FunctionSpace. El constructor de esta clase tiene al menos tres argumentos: un objeto de malla, el nombre del tipo de funci√≥n base, y el grado de la funci√≥n base. En este caso, vamos a utilizar la funci√≥n de Lagrange.\n# Funciones bases V = dolfin.FunctionSpace(mesh, \u0026#39;Lagrange\u0026#39;, 1) u = dolfin.TrialFunction(V) v = dolfin.TestFunction(V) DEBUG:FFC:Reusing form from cache. Ahora debemos definir a nuestra EDP en su formulaci√≥n d√©bil equivalente para poder tratarla como un problema de √°lgebra lineal que podamos resolver con el MEF.\n# Formulaci√≥n debil de la EDP a = dolfin.inner(dolfin.nabla_grad(u), dolfin.nabla_grad(v)) * dolfin.dx f = dolfin.Constant(0.0) L = f * v * dolfin.dx Por √∫ltimo, solo nos falta definir las condiciones de frontera.\n# Defino condiciones de frontera def u0_top_boundary(x, on_boundary): return on_boundary and abs(x[1]-1) \u0026lt; 1e-8 def u0_bottom_boundary(x, on_boundary): return on_boundary and abs(x[1]) \u0026lt; 1e-8 def u0_left_boundary(x, on_boundary): return on_boundary and abs(x[0]) \u0026lt; 1e-8 def u0_right_boundary(x, on_boundary): return on_boundary and abs(x[0]-1) \u0026lt; 1e-8 # Definiendo condiciones de frontera de Dirichlet bc_t = dolfin.DirichletBC(V, dolfin.Constant(5), u0_top_boundary) bc_b = dolfin.DirichletBC(V, dolfin.Constant(-5), u0_bottom_boundary) bc_l = dolfin.DirichletBC(V, dolfin.Constant(3), u0_left_boundary) bc_r = dolfin.DirichletBC(V, dolfin.Constant(-1), u0_right_boundary) # Lista de condiciones de frontera bcs = [bc_t, bc_b, bc_r, bc_l] Con esta especificaci√≥n de las condiciones de frontera, ya estamos listos para resolver nuestra EDP utilizando la funci√≥n dolfin.solve. El vector resultante, luego lo podemos convertir a una matriz de NumPy y utilizarla para graficar la soluci√≥n con Matplotlib.\n# Resolviendo la EDP u_sol = dolfin.Function(V) dolfin.solve(a == L, u_sol, bcs) DEBUG:FFC:Reusing form from cache. DEBUG:FFC:Reusing form from cache. # graficando la soluci√≥n u_mat = u_sol.vector().array().reshape(N1+1, N2+1) x = np.linspace(0, 1, N1+2) y = np.linspace(0, 1, N1+2) X, Y = np.meshgrid(x, y) fig, ax = plt.subplots(1, 1, figsize=(8, 6)) c = ax.pcolor(X, Y, u_mat, vmin=-5, vmax=5, cmap=mpl.cm.get_cmap(\u0026#39;RdBu_r\u0026#39;)) cb = plt.colorbar(c, ax=ax) ax.set_xlabel(r\u0026#34;$x_1$\u0026#34;, fontsize=18) ax.set_ylabel(r\u0026#34;$x_2$\u0026#34;, fontsize=18) cb.set_label(r\u0026#34;$u(x_1, x_2)$\u0026#34;, fontsize=18) fig.tight_layout() Para profundizar en como utilizar el framework FEniCS, les recomiendo que visiten la documentaci√≥n del sitio, que tienen varios ejemplos.\nCon esto concluyo este art√≠culo. Obviamente, no es m√°s que una introducci√≥n al fascinante y complejo mundo de las Ecuaciones en derivadas parciales, cada clase de EDP es un mundo en s√≠ mismo y quedaron muchos temas sin tratar; los cuales tal vez profundice en alg√∫n otro art√≠culo. Espero que les pueda servir como referencia y lo hayan encontrado instructivo.\nSaludos!\nEste post fue escrito utilizando Jupyter notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2016-01-27","id":27,"permalink":"/blog/2016/01/27/ecuaciones-en-derivadas-parciales-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Nuestra comprensi√≥n de los procesos fundamentales de la naturaleza se basa en gran medida en Ecuaciones en derivadas parciales. Ejemplos de ello son las vibraciones de los s√≥lidos, la din√°mica de los fluidos, la difusi√≥n de los productos qu√≠micos, la propagaci√≥n del calor, la estructura de las mol√©culas, las interacciones entre fotones y electrones, y la radiaci√≥n de ondas electromagn√©ticas.","tags":["python","calculo","derivada","ecuaciones diferenciales","derivadas parciales","matematica","integral"],"title":"Ecuaciones en derivadas parciales con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Vivimos en un mundo en constante cambio. La posici√≥n de la Tierra cambia con el tiempo; la velocidad de un objeto en ca√≠da libre cambia con la distancia; el √°rea de un c√≠rculo cambia seg√∫n el tama√±o de su radio; la trayectoria de un proyectil cambia seg√∫n la velocidad y el √°ngulo de disparo. Al intentar modelar matem√°ticamente cualquiera de estos fen√≥menos, veremos que generalmente adoptan la forma de una o m√°s Ecuaciones diferenciales.\nNota: Antes de continuar leyendo, si no tienen frescos sus conocimientos b√°sicos de C√°lculo integral y C√°lculo diferencial; les recomiendo que visiten mi anterior art√≠culo de Introducci√≥n al C√°lculo.\n¬øQu√© es una ecuaci√≥n diferencial?# Una Ecuaci√≥n diferencial es una ecuaci√≥n que involucra una variable dependiente y sus derivadas con respecto a una o m√°s variables independientes. Muchas de las leyes de la naturaleza, en F√≠sica, Qu√≠mica, Biolog√≠a, y Astronom√≠a; encuentran la forma m√°s natural de ser expresadas en el lenguaje de las Ecuaciones diferenciales. Estas ecuaciones no s√≥lo tienen aplicaciones en la ciencias f√≠sicas, sino que tambi√©n abundan sus aplicaciones en las ciencias aplicadas como ser Ingenier√≠a, Finanzas y Econom√≠a.\nEs f√°cil entender la raz√≥n detr√°s de esta amplia utilidad de las Ecuaciones diferenciales. Si recordamos que \\(y = f(x)\\) es una funci√≥n, entonces su derivada \\(dy / dx\\) puede ser interpretada como el ritmo de cambio de \\(y\\) con respecto a \\(x\\). En muchos procesos naturales, las variables involucradas y su ritmo de cambio est√°n conectados entre s√≠ por medio de los principios cient√≠ficos b√°sicos que rigen el proceso. Cuando esta conexi√≥n es expresada matem√°ticamente, el resultado generalmente es una Ecuaci√≥n diferencial.\nPara ilustrar el caso, veamos un ejemplo. Seg√∫n la segunda ley de la din√°mica de Newton, la aceleraci√≥n \\(a\\) de un cuerpo de masa \\(m\\) es proporcional a la fuerza total \\(F\\) que act√∫a sobre √©l. Si expresamos esto matem√°ticamente en la forma de una ecuaci√≥n, entonces tememos que:\n$$F = ma$$ A primera vista, esta ecuaci√≥n no parece ser una Ecuaci√≥n diferencial, pero supongamos que un objeto de masa \\(m\\) esta en ca√≠da libre bajo la influencia de la fuerza de gravedad. En este caso, la √∫nica fuerza actuando sobre el objeto es \\(mg\\), donde \\(g\\) es la aceleraci√≥n debido a la gravedad. Si consideramos a \\(u\\), como la posici√≥n del objeto desde una altura determinada; entonces la velocidad de ca√≠da del objeto va a ser igual al ritmo de cambio de la posici√≥n \\(u\\) con respecto al tiempo \\(t\\); es decir que la velocidad es igual a \\(v = du / dt\\), o sea, la derivada de la posici√≥n del objeto con respecto al tiempo; y como la aceleraci√≥n no es otra cosa que el ritmo de cambio de la velocidad, entonces podemos definir a la aceleraci√≥n como una segunda derivada de la posici√≥n del objeto con respecto al tiempo, lo que es igual a decir que \\(g = d^2u / dt^2\\). De esta forma, podemos reescribir la ecuaci√≥n inicial de la segunda ley de la din√°mica de Newton como la siguiente Ecuaci√≥n diferencial.\n$$F = m\\frac{d^2u}{dt^2}$$ De esta manera, estamos expresando a la segunda ley de la din√°mica de Newton en funci√≥n de la posici√≥n del objeto.\nEcuaciones diferenciales ordinarias y parciales# El caso precedente, es el caso t√≠pico de una Ecuaci√≥n diferencial ordinaria, ya que todas las derivadas involucradas son tomadas con respecto a una √∫nica y misma variable independiente. Por otro lado, si en la ecuaci√≥n tenemos derivadas de m√°s de una variable independiente, debemos hablar de Ecuaciones difenciales parciales. Por ejemplo, si \\(w = f(x, y, z, t)\\) es una funci√≥n de tiempo y 3 coordenadas de un punto en el espacio, entonces las siguientes son sus Ecuaciones diferenciales parciales:\n$$\\frac{\\partial^2 w}{\\partial x^2} + \\frac{\\partial^2 w}{\\partial y^2} + \\frac{\\partial^2 w}{\\partial z^2} = 0; \\\\ \\\\ a^2\\left(\\frac{\\partial^2 w}{\\partial x^2} + \\frac{\\partial^2 w}{\\partial y^2} + \\frac{\\partial^2 w}{\\partial z^2}\\right) = \\frac{\\partial w}{\\partial t}; \\\\ \\\\ a^2\\left(\\frac{\\partial^2 w}{\\partial x^2} + \\frac{\\partial^2 w}{\\partial y^2} + \\frac{\\partial^2 w}{\\partial z^2}\\right) = \\frac{\\partial^2 w}{\\partial t^2}. $$ En esta caso, el s√≠mbolo \\(\\partial\\), es la notaci√≥n matem√°tica para expresar que estamos derivando parcialmente, as√≠ por ejemplo, si queremos expresar que vamos a derivar z con respecto a x, escribimos \\(\\partial z / \\partial x\\).\nEstos ejemplos, tienen una gran aplicaci√≥n en F√≠sica y se conocen como la ecuaci√≥n de Laplace, la ecuaci√≥n del calor y la ecuaci√≥n de onda respectivamente. En general, las Ecuaciones diferenciales parciales surgen en problemas de campos el√©ctricos, mec√°nica de fluidos, difusi√≥n y movimientos de ondas. La teor√≠a de estas ecuaciones es bastante diferente con respecto a las ecuaciones diferenciales ordinarias y suelen ser tambi√©n m√°s complicadas de resolver. En este art√≠culo me voy a enfocar principalmente en las ecuaciones diferenciales ordinarias, o EDOs para abreviar.\nOrden de las Ecuaciones diferenciales# El orden de una Ecuaci√≥n diferencial va a ser igual al orden de la mayor derivada presente. As√≠, en nuestro primer ejemplo, la Ecuaci√≥n diferencial de la segunda ley de la din√°mica de Newton es de segundo orden, ya que nos encontramos ante la segunda derivada de la posici√≥n del objeto con respecto al tiempo. La ecuaci√≥n general de las ecuaciones diferenciales ordinarias de grado \\(n\\) es la siguiente:\n$$F\\left(x, y, \\frac{dy}{dx}, \\frac{d^2y}{dx^2}, \\dots , \\frac{d^ny}{dx^n}\\right) = 0 $$ o utilizando la notaci√≥n prima para las derivadas,\n$$F(x, y, y', y'', \\dots, y^{(n)}) = 0$$ La m√°s simple de todas las ecuaciones diferenciales ordinarias es la siguiente ecuaci√≥n de primer orden:\n$$ \\frac{dy}{dx} = f(x)$$ y para resolverla simplemente debemos calcular su integral indefinida:\n$$y = \\int f(x) dx + c$$ .\nEcuaciones diferenciales separables# Una ecuaci√≥n separable es una ecuaci√≥n diferencial de primer orden en la que la expresi√≥n para \\(dx / dy\\) se puede factorizar como una funci√≥n de x multiplicada por una funci√≥n de y. En otras palabras, puede ser escrita en la forma:\n$$\\frac{dy}{dx} = f(x)g(y)$$ El nombre separable viene del hecho de que la expresi√≥n en el lado derecho se puede \u0026ldquo;separar\u0026rdquo; en una funci√≥n de \\(x\\) y una funci√≥n de \\(y\\).\nPara resolver este tipo de ecuaciones, podemos reescribirlas en la forma diferencial:\n$$\\frac{dy}{g(y)} = f(x)dx$$ y luego podemos resolver la ecuaci√≥n original integrando:\n$$\\int \\frac{dy}{g(y)} = \\int f(x) dx + c$$ √âstas suelen ser las Ecuaciones diferenciales m√°s f√°ciles de resolver, ya que el problema de resolverlas puede ser reducido a un problema de integraci√≥n; a pesar de que igualmente muchas veces estas integrales pueden ser dif√≠ciles de calcular.\nEcuaciones diferenciales lineales# Uno de los tipos m√°s importantes de Ecuaciones diferenciales son las Ecuaciones diferenciales lineales. Este tipo de ecuaciones son muy comunes en varias ciencias y tienen la ventaja de que pueden llegar a ser resueltas en forma anal√≠tica ya que su ecuaci√≥n diferencial de primer orden adopta la forma:\n$$\\frac{dy}{dx} + P(x)y = Q(x) $$ donde, \\(P\\) y \\(Q\\) son funciones continuas de \\(x\\) en un determinado intervalo. Para resolver este tipo de ecuaciones de la forma \\( y\u0026rsquo; + P(x)y = Q(x)\\), debemos multiplicar los dos lados de la ecuaci√≥n por el factor de integraci√≥n \\(e^{\\int P(x) dx}\\) y luego integrar ambos lados. As√≠, si por ejemplo quisi√©ramos resolver la siguiente Ecuaci√≥n diferencial:\n$$\\frac{dy}{dx} + 3x^2y = 6x^2$$ Primero calculamos el factor de integraci√≥n, \\(I(x) = e^{\\int 3x^2 \\ dx}\\), lo que es igual a \\(e^{x^3}\\).\nLuego multiplicamos ambos lados de la ecuaci√≥n por el reci√©n calculado factor de integraci√≥n.\n$$e^{x^3}\\frac{dy}{dx} + 3x^2e^{x^3}y = 6x^2e^{x^3}$$ simplificando, obtenemos:\n$$\\frac{d}{dx}(e^{x^3}y) = 6x^2e^{x^3}$$ Por √∫ltimo, integramos ambos lados de la ecuaci√≥n:\n$$e^{x^3}y = \\int 6x^2e^{x^3} \\ dx = 2e^{x^3} + C$$ y podemos obtener la soluci√≥n final:\n$$y = Ce^{-x^3} +2 $$ Condici√≥n inicial# Cuando utilizamos Ecuaciones diferenciales, generalmente no estamos interesados en encontrar una familia de soluciones (la soluci√≥n general), como la que hayamos en el caso precedente, sino que vamos a estar m√°s interesados en una soluci√≥n que satisface un requerimiento particular.En muchos problemas f√≠sicos debemos de encontrar una soluci√≥n particular que satisface una condici√≥n de la forma \\(y(t_o) = y_0\\). Esto se conoce como la condici√≥n inicial, y el problema de encontrar una soluci√≥n de la Ecuaci√≥n diferencial que satisface la condici√≥n inicial se conoce como el problema de valor inicial.\nSeries de potencias# Cuando comenzamos a lidiar con las Ecuaciones diferenciales, veremos que existen un gran n√∫mero de ellas que no pueden ser resueltas en forma anal√≠tica utilizando los principios del C√°lculo integral y el C√°lculo diferencial; pero sin embargo, tal vez podamos encontrar soluciones aproximadas para este tipo de ecuaciones en t√©rminos de Series de potencias.\n¬øQu√© es una serie de potencias?# Una Serie de potencias es una serie, generalmente infinita, que posee la siguiente forma:\n$$\\sum_{n=0}^{\\infty} C_nX^n = C_0 + C_1X + C_2X^2 + C_3X^3 + \\dots $$ En d√≥nde \\(x\\) es una variable y las $C_n$ son constantes o los coeficientes de la serie. Una Serie de potencias puede converger para algunos valores de \\(x\\) y divergir para otros valores de \\(x\\). La suma de la serie es una funci√≥n.\n$$f(x) = C_0 + C_1X + C_2X^2 + C_3X^3 + \\dots + C_nX^n + \\dots$$ El dominio de esta funci√≥n va a estar dado por el conjunto de todos los \\(x\\) para los que la serie converge.\nSeries de Taylor# Las Series de Taylor son un caso especial de Serie de potencias cuyos t√©rminos adoptan la forma \\((x - a)^n\\). Las Series de Taylor nos van a permitir aproximar funciones continuas que no pueden resolverse en forma anal√≠tica y se van a calcular a partir de las derivadas de estas funciones. Su definici√≥n matem√°tica es la siguiente:\n$$f(x) = \\sum_{n=0}^{\\infty}\\frac{f^{(n)}(a)}{n!}(x - a)^n$$ Lo que es equivalente a decir:\n$$f(x) = f(a) + \\frac{f'(a)}{1!}(x -a) + \\frac{f''(a)}{2!}(x -a)^2 + \\frac{f'''(a)}{3!}(x -a)^3 + \\dots$$ Una de las razones de que las Series de Taylor sean importantes es que nos permiten integrar funciones que de otra forma no pod√≠amos manejar. De hecho, a menudo Newton integraba funciones por medio de, en primer lugar, expresarlas como Series de potencias y luego integrando la serie t√©rmino a t√©rmino. Por ejemplo, la funci√≥n $f(x) = e^{-x^2}$, no puede ser integrada por los m√©todos normales del C√°lculo integral, por lo que debemos recurrir a las Series de Taylor para aproximar la soluci√≥n de su integral. Podr√≠amos construir la Serie de Taylor de esta funci√≥n, del siguiente modo:\n$$e^{-x^2} = \\sum_{n=0}^{\\infty}\\frac{(-x^2)^n}{n!} = \\sum_{n=0}^{\\infty}(-1)^n\\frac{x^{2n}}{n!} = 1 - \\frac{x^2}{1!} + \\frac{x^4}{2!} -\\frac{x^6}{3!} + \\dots$$ Resolviendo Ecuaciones diferenciales con Python# Mientras que algunos problemas de Ecuaciones diferenciales ordinarias se pueden resolver con m√©todos anal√≠ticos, como hemos mencionado anteriormente, son mucho m√°s comunes los problemas que no se pueden resolver anal√≠ticamente. Por lo tanto, en estos casos debemos recurrir a los m√©todos num√©ricos. Es aqu√≠, d√≥nde el poder de las computadoras y en especial, de los paquetes cient√≠ficos de Python como NumPy, Matplotlib, SymPy y SciPy, se vuelven sumamente √∫tiles. Veamos como podemos utilizar la fuerza computacional para resolver Ecuaciones diferenciales.\nSoluciones anal√≠ticas con Python# SymPy nos proporciona un solucionador gen√©rico de Ecuaciones diferenciales ordinarias, sympy.dsolve, el cual es capaz de encontrar soluciones anal√≠ticas a muchas EDOs elementales. Mientras sympy.dsolve se puede utilizar para resolver muchas EDOs sencillas simb√≥licamente, como veremos a continuaci√≥n, debemos tener en cuenta que la mayor√≠a de las EDOs no se pueden resolver anal√≠ticamente. Por ejemplo, retomando el ejemplo que resolvimos anal√≠ticamente m√°s arriba, veamos si llegamos al mismo resultado utilizando SymPy para solucionar la siguiente Ecuaci√≥n diferencial ordinaria:\n$$\\frac{dy}{dx} = -3x^2y + 6x^2$$ Ver C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # importando modulos necesarios %matplotlib inline import matplotlib.pyplot as plt import numpy as np import sympy from scipy import integrate # imprimir con notaci√≥n matem√°tica. sympy.init_printing(use_latex=\u0026#39;mathjax\u0026#39;) # Resolviendo ecuaci√≥n diferencial # defino las incognitas x = sympy.Symbol(\u0026#39;x\u0026#39;) y = sympy.Function(\u0026#39;y\u0026#39;) # expreso la ecuacion f = 6*x**2 - 3*x**2*(y(x)) sympy.Eq(y(x).diff(x), f) $$\\frac{d}{d x} y{\\left (x \\right )} = - 3 x^{2} y{\\left (x \\right )} + 6 x^{2}$$ Aqu√≠ primero definimos la inc√≥gnitas \\(x\\) utilizando el objeto Symbol e \\(y\\), que al ser una funci√≥n, la definimos con el objeto Function, luego expresamos en Python la ecuaci√≥n que define a la funci√≥n. Ahora solo nos restar√≠a aplicar la funci√≥n dsolve para resolver nuestra EDO.\n# Resolviendo la ecuaci√≥n sympy.dsolve(y(x).diff(x) - f) $$y{\\left (x \\right )} = C_{1} e^{- x^{3}} + 2$$ Como podemos ver, arribamos exactamente al mismo resultado. Siguiendo el mismo procedimiento, podemos resolver otras EDOs, por ejemplo si quisi√©ramos resolver la siguiente Ecuaci√≥n diferencial:\n$$\\frac{dy}{dx} = \\frac{1}{2}(y^2 -1) $$ que cumpla con la condici√≥n inicial \\(y(0) = 2\\), debemos realizar el siguiente procedimiento:\n# definiendo la ecuaci√≥n eq = 1.0/2 * (y(x)**2 - 1) # Condici√≥n inicial ics = {y(0): 2} # Resolviendo la ecuaci√≥n edo_sol = sympy.dsolve(y(x).diff(x) - eq) edo_sol $$y{\\left (x \\right )} = \\frac{C_{1} + e^{x}}{C_{1} - e^{x}}$$ Aqu√≠ encontramos la soluci√≥n general de nuestra Ecuaci√≥n diferencial, ahora reemplazamos los valores de la condici√≥n inicial en nuestra ecuaci√≥n.\nC_eq = sympy.Eq(edo_sol.lhs.subs(x, 0).subs(ics), edo_sol.rhs.subs(x, 0)) C_eq $$2 = \\frac{C_{1} + 1}{C_{1} - 1}$$ y por √∫ltimo despejamos el valor de la constante de integraci√≥n resolviendo la ecuaci√≥n.\nsympy.solve(C_eq) $$\\left [ 3\\right ]$$ Como vemos, el valor de la constante de integraci√≥n es 3, por lo tanto nuestra soluci√≥n para el problema del valore inicial es:\n$$y{\\left (x \\right )} = \\frac{3 + e^{x}}{3 - e^{x}}$$ Para los casos en que no exista una soluci√≥n anal√≠tica, pero s√≠ una soluci√≥n aproximada por una Serie de potencias, sympy.dsolve nos va a devolver la serie como soluci√≥n. Veamos el caso de la siguiente Ecuaci√≥n diferencial:\n$$\\frac{dy}{dx} = x^2 + y^2 -1$$ # Soluci√≥n con serie de potencias f = y(x)**2 + x**2 -1 sympy.dsolve(y(x).diff(x) - f) $$y{\\left (x \\right )} = \\frac{x^{3}}{3} \\left(C_{1} \\left(3 C_{1} - 1\\right) + 1\\right) + \\frac{x^{5}}{60} \\left(C_{1} \\left(4 C_{1} - 7\\right) + 10 C_{1} - 6\\right) + C_{1} + C_{1} x + C_{1} x^{2} + \\frac{C_{1} x^{4}}{12} + \\mathcal{O}\\left(x^{6}\\right)$$ Campos de direcciones# Los Campos de direcciones es una t√©cnica sencilla pero √∫til para visualizar posibles soluciones a las ecuaciones diferenciales de primer orden. Se compone de l√≠neas cortas que muestran la pendiente de la funci√≥n inc√≥gnita en el plano x-y. Este gr√°fico se puede producir f√°cilmente debido a que la pendiente de \\(y(x)\\) en los puntos arbitrarios del plano x-y est√° dada por la definici√≥n misma de la Ecuaci√≥n diferencial ordinaria:\n$$\\frac{dy}{dx} = f(x, y(x))$$ Es decir, que s√≥lo tenemos que iterar sobre los valores \\(x\\) e \\(y\\) en la grilla de coordenadas de inter√©s y evaluar \\(f(x, y(x))\\) para saber la pendiente de \\(y(x)\\) en ese punto. Cuantos m√°s segmentos de l√≠neas trazamos en un Campo de direcci√≥n, m√°s clara ser√° la imagen. La raz√≥n por la cual el gr√°fico de Campos de direcciones es √∫til, es que las curvas suaves y continuos que son tangentes a las l√≠neas de pendiente en cada punto del gr√°fico, son las posibles soluciones a la Ecuaci√≥n diferencial ordinaria.\nPor supuesto que calcular las pendientes y dibujar los segmentos de l√≠nea para un gran n√∫mero de puntos a mano ser√≠a algo realmente tedioso, pero para eso existen las computadoras y Python!. Veamos un ejemplo, supongamos que tenemos la siguiente Ecuaci√≥n diferencial ordinaria, la cual seg√∫n lo que vimos m√°s arriba, sabemos que no tiene una soluci√≥n anal√≠tica:\n$$\\frac{dy}{dx} = x^2 + y^2 -1$$ entonces, con la ayuda de Python, podemos graficar su Campo de direcci√≥n del siguiente modo:\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; def plot_direction_field(x, y_x, f_xy, x_lim=(-5, 5), y_lim=(-5, 5), ax=None): \u0026#34;\u0026#34;\u0026#34;Esta funci√≥n dibuja el campo de direcci√≥n de una EDO\u0026#34;\u0026#34;\u0026#34; f_np = sympy.lambdify((x, y_x), f_xy, modules=\u0026#39;numpy\u0026#39;) x_vec = np.linspace(x_lim[0], x_lim[1], 20) y_vec = np.linspace(y_lim[0], y_lim[1], 20) if ax is None: _, ax = plt.subplots(figsize=(4, 4)) dx = x_vec[1] - x_vec[0] dy = y_vec[1] - y_vec[0] for m, xx in enumerate(x_vec): for n, yy in enumerate(y_vec): Dy = f_np(xx, yy) * dx Dx = 0.8 * dx**2 / np.sqrt(dx**2 + Dy**2) Dy = 0.8 * Dy*dy / np.sqrt(dx**2 + Dy**2) ax.plot([xx - Dx/2, xx + Dx/2], [yy - Dy/2, yy + Dy/2], \u0026#39;b\u0026#39;, lw=0.5) ax.axis(\u0026#39;tight\u0026#39;) ax.set_title(r\u0026#34;$%s$\u0026#34; % (sympy.latex(sympy.Eq(y(x).diff(x), f_xy))), fontsize=18) return ax # Defino incognitas x = sympy.symbols(\u0026#39;x\u0026#39;) y = sympy.Function(\u0026#39;y\u0026#39;) # Defino la funci√≥n f = y(x)**2 + x**2 -1 # grafico de campo de direcci√≥n fig, axes = plt.subplots(1, 1, figsize=(8, 6)) campo_dir = plot_direction_field(x, y(x), f, ax=axes) Las l√≠neas de direcci√≥n en el gr√°fico de arriba sugieren c√≥mo las curvas que son soluciones a la Ecuaci√≥n diferencial ordinaria se van a comportar. Por lo tanto, los Campos de direcciones son una herramienta muy √∫til para visualizar posibles soluciones para Ecuaciones diferenciales ordinarias que no se pueden resolver anal√≠ticamente. Este gr√°fico, tambi√©n nos puede ayudar a determinar el rango de validez de la soluci√≥n aproximada por la Serie de potencias. Por ejemplo si resolvemos nuestra EDO para la condici√≥n inicial \\(y(0) = 0\\), veamos a que conclusiones arribamos.\n# Condici√≥n inicial ics = {y(0): 0} # Resolviendo la ecuaci√≥n diferencial edo_sol = sympy.dsolve(y(x).diff(x) - f, ics=ics) edo_sol $$y{\\left (x \\right )} = - x + \\frac{2 x^{3}}{3} - \\frac{4 x^{5}}{15} + \\mathcal{O}\\left(x^{6}\\right)$$ fig, axes = plt.subplots(1, 2, figsize=(10, 5)) # panel izquierdo - soluci√≥n aproximada por Serie de potencias plot_direction_field(x, y(x), f, ax=axes[0]) x_vec = np.linspace(-3, 3, 100) axes[0].plot(x_vec, sympy.lambdify(x, edo_sol.rhs.removeO())(x_vec), \u0026#39;b\u0026#39;, lw=2) # panel derecho - Soluci√≥n por m√©todo iterativo plot_direction_field(x, y(x), f, ax=axes[1]) x_vec = np.linspace(-1, 1, 100) axes[1].plot(x_vec, sympy.lambdify(x, edo_sol.rhs.removeO())(x_vec), \u0026#39;b\u0026#39;, lw=2) # Resolviendo la EDO en forma iterativa edo_sol_m = edo_sol_p = edo_sol dx = 0.125 # x positivos for x0 in np.arange(1, 2., dx): x_vec = np.linspace(x0, x0 + dx, 100) ics = {y(x0): edo_sol_p.rhs.removeO().subs(x, x0)} edo_sol_p = sympy.dsolve(y(x).diff(x) - f, ics=ics, n=6) axes[1].plot(x_vec, sympy.lambdify(x, edo_sol_p.rhs.removeO())(x_vec), \u0026#39;r\u0026#39;, lw=2) # x negativos for x0 in np.arange(1, 5, dx): x_vec = np.linspace(-x0-dx, -x0, 100) ics = {y(-x0): edo_sol_m.rhs.removeO().subs(x, -x0)} edo_sol_m = sympy.dsolve(y(x).diff(x) - f, ics=ics, n=6) axes[1].plot(x_vec, sympy.lambdify(x, edo_sol_m.rhs.removeO())(x_vec), \u0026#39;r\u0026#39;, lw=2) En el panel de la izquierda podemos ver el gr√°fico de la soluci√≥n aproximada por la Serie de potencias; en el panel de la derecha vemos el gr√°fico al que podemos arribar resolviendo la EDO para valores incrementales de \\(x\\) para la condici√≥n inicial en forma iterativa. Como podemos ver, las soluci√≥n aproximada se alinea bien con el campo de direcciones para los valores de \\(x\\) entre $-1.5$ y $1.5$, luego comienza a desviarse, lo que nos indica que la soluci√≥n aproximada ya no ser√≠a v√°lida. En el panel de la derecha podemos ver una soluci√≥n que se adapta mucho mejor con el campo de direcciones.\nTransformada de Laplace# Un m√©todo alternativo que podemos utilizar para resolver en forma anal√≠tica Ecuaciones diferenciales ordinarias complejas, es utilizar la Transformada de Laplace, que es un tipo particular de transformada integral. La idea es que podemos utilizar esta t√©cnica para transformar nuestra Ecuaci√≥n diferencial en algo m√°s simple, resolver esta ecuaci√≥n m√°s simple y, a continuaci√≥n, invertir la transformaci√≥n para recuperar la soluci√≥n a la Ecuaci√≥n diferencial original.\n¬øQu√© es una Transformada de Laplace?# Para poder comprender la Transformada de Laplace, primero debemos revisar la definici√≥n general de la transformada integral, la cu√°l adapta la siguiente forma:\n$$T(f(t)) = \\int_{\\alpha}^{\\beta} K (s, t) \\ f(t) \\ dt = F(s) $$ En este caso, \\(f(t)\\) es la funci√≥n que queremos transformar, y \\(F(s)\\) es la funci√≥n transformada. Los l√≠mites de la integraci√≥n, \\(\\alpha\\) y \\(\\beta\\), pueden ser cualquier valor entre \\(-\\infty\\) y \\(+\\infty\\) y \\(K(s, t)\\) es lo que se conoce como el n√∫cleo o kernel de la transformada, y podemos elegir el kernel que nos plazca. La idea es poder elegir un kernel que nos d√© la oportunidad de simplificar la Ecuaci√≥n diferencial con mayor facilidad.\nSi nos restringimos a Ecuaciones diferenciales con coeficientes constantes, entonces un kernel que resulta realmente √∫til es \\(e^{-st}\\), ya que al diferenciar este kernel con respecto de \\(t\\), terminamos obteniendo potencias de \\(s\\), que podemos equiparar a los coeficientes constantes. De esta forma, podemos arribar a la definici√≥n de la Transformada de Laplace:\n$$\\mathcal{L}\\{f(t)\\}=\\int_0^{\\infty} e^{-st} \\ f(t) \\ dt$$ Tengan en cuenta, que adem√°s de usar el kernel \\(e^{-st}\\), los l√≠mites de la integraci√≥n van desde \\(0\\) a \\(\\infty\\), ya que valores negativos de \\(t\\) har√≠an divergir la integral.\nTabla de transformadas de Laplace# Calcular la Transformada de Laplace a veces puede resultar en una tarea complicada. Por suerte, podemos recurrir a la siguiente tabla para resolver la Transformada de Laplace para las funciones m√°s comunes:\nFunci√≥n original Transformada de Laplace Restricciones $$1$$ $$\\frac{1}{s}$$ $$s\u003e0$$ $$e^{at}$$ $$\\frac{1}{s -a}$$ $$s\u003ea$$ $$t^n$$ $$\\frac{n!}{s^{n+1}}$$ $$s\u003e0$$, $$n$$ un entero $$\u003e 0$$ $$\\cos at$$ $$\\frac{s}{s^2 + a^2}$$ $$s\u003e0$$ $$\\sin at$$ $$\\frac{a}{s^2 + a^2}$$ $$s\u003e0$$ $$\\cosh at$$ $$\\frac{s}{s^2 - a^2}$$ $$s\u003e|a|$$ $$\\sinh at$$ $$\\frac{a}{s^2 - a^2}$$ $$s\u003e|a|$$ $$e^{at}\\cos bt$$ $$\\frac{s- a}{(s^2 - a^2) + b^2}$$ $$s\u003ea$$ $$e^{at}\\sin bt$$ $$\\frac{b}{(s^2 - a^2) + b^2}$$ $$s\u003ea$$ $$t^n e^{at}$$ $$\\frac{n!}{(s-a)^{n+1}}$$ $$s\u003ea$$, $$n$$ un entero $$\u003e 0$$ $$f(ct)$$ $$\\frac{1}{c}\\mathcal{L}\\{f(s/c)\\}$$ $$c\u003e0$$ Propiedades de las transformadas de Laplace# Las Transformadas de Laplace poseen algunas propiedades que tambi√©n nos van a facilitar el trabajo de resolverlas, algunas de ellas son:\nLa Transformada de Laplace es un operador lineal: Esta propiedad nos dice la Transformada de Laplace de una suma, es igual a la suma de las Transformadas de Laplace de cada uno de los t√©rminos. Es decir: $$\\mathcal{L}\\{c_1f_1(t) + c_2f_2(t)\\}=c_1\\mathcal{L}\\{f_1(t)\\} + c_2\\mathcal{L}\\{f_2(t)\\}$$ La Transformada de Laplace de la primer derivada: Esta propiedad nos dice que si \\(f(t)\\) es continua y $f'(t)$ es continua en el intervalo $0 \\le t \\le \\alpha$. Entonces la Transformada de Laplace de la primer derivada es: $$\\mathcal{L}\\{f'(t)\\} = s\\mathcal{L}\\{f(t)\\} - f(0)$$ La Transformada de Laplace de derivadas de orden superior: Esta propiedad es la generalizaci√≥n de la propiedad anterior para derivadas de orden n. Su formula es: $$\\mathcal{L}\\{f^{(n)}(t)\\} = s^n\\mathcal{L}\\{f(t)\\} - s^{n-1}f(0) - \\dots -f^{(n-1)}(0) \\\\ = s^n\\mathcal{L}\\{f(t)\\} - \\sum_{i=1}^n s^{n-i}f^{(i-1)}(0)$$ Resolviendo ecuaciones diferenciales con la transformada de Laplace# La principal ventaja de utilizar Transformadas de Laplace es que cambia la Ecuaci√≥n diferencial en una ecuaci√≥n algebraica, lo que simplifica el proceso para calcular su soluci√≥n. La √∫nica parte complicada es encontrar las transformaciones y las inversas de las transformaciones de los varios t√©rminos de la Ecuaci√≥n diferencial que queramos resolver. Veamos entonces como Python y SymPy nos ayudan a resolver Ecuaciones diferenciales utilizando las Transformadas de Laplace. Vamos a intentar resolver la siguiente ecuaci√≥n:\n$$y'' + 3y' + 2y = 0$$ con las siguientes condiciones iniciales: \\(y(0) = 2\\) y \\(y\u0026rsquo;(0) = -3\\)\n# Ejemplo de transformada de Laplace # Defino las incognitas t = sympy.symbols(\u0026#34;t\u0026#34;, positive=True) y = sympy.Function(\u0026#34;y\u0026#34;) # Defino la ecuaci√≥n edo = y(t).diff(t, t) + 3*y(t).diff(t) + 2*y(t) sympy.Eq(edo) $$2 y{\\left (t \\right )} + 3 \\frac{d}{d t} y{\\left (t \\right )} + \\frac{d^{2}}{d t^{2}} y{\\left (t \\right )} = 0$$ # simbolos adicionales. s, Y = sympy.symbols(\u0026#34;s, Y\u0026#34;, real=True) # Calculo la transformada de Laplace L_edo = sympy.laplace_transform(edo, t, s, noconds=True) sympy.Eq(L_edo) $$2 \\mathcal{L}_{t}\\left[y{\\left (t \\right )}\\right]\\left(s\\right) + 3 \\mathcal{L}_{t}\\left[\\frac{d}{d t} y{\\left (t \\right )}\\right]\\left(s\\right) + \\mathcal{L}_{t}\\left[\\frac{d^{2}}{d t^{2}} y{\\left (t \\right )}\\right]\\left(s\\right) = 0$$ Como podemos ver en este resultado, al aplicar la funci√≥n sympy.laplace_transform sobre la derivada de \\(y(t)\\), SymPy nos devuelve un termino con la forma \\(\\mathcal{L}_{t}\\left[\\frac{d}{d t} y{\\left (t \\right )}\\right]\\left(s\\right)\\). Esto es una complicaci√≥n si queremos arribar a una ecuaci√≥n algebraica. Por tanto antes de proceder, deber√≠amos utilizar la siguiente funci√≥n para resolver este inconveniente.\ndef laplace_transform_derivatives(e): \u0026#34;\u0026#34;\u0026#34; Evalua las transformadas de Laplace de derivadas de funciones sin evaluar. \u0026#34;\u0026#34;\u0026#34; if isinstance(e, sympy.LaplaceTransform): if isinstance(e.args[0], sympy.Derivative): d, t, s = e.args n = len(d.args) - 1 return ((s**n) * sympy.LaplaceTransform(d.args[0], t, s) - sum([s**(n-i) * sympy.diff(d.args[0], t, i-1).subs(t, 0) for i in range(1, n+1)])) if isinstance(e, (sympy.Add, sympy.Mul)): t = type(e) return t(*[laplace_transform_derivatives(arg) for arg in e.args]) return e # Aplicamos la nueva funcion para evaluar las transformadas de Laplace # de derivadas L_edo_2 = laplace_transform_derivatives(L_edo) sympy.Eq(L_edo_2) $$s^{2} \\mathcal{L}_{t}\\left[y{\\left (t \\right )}\\right]\\left(s\\right) + 3 s \\mathcal{L}_{t}\\left[y{\\left (t \\right )}\\right]\\left(s\\right) - s y{\\left (0 \\right )} + 2 \\mathcal{L}_{t}\\left[y{\\left (t \\right )}\\right]\\left(s\\right) - 3 y{\\left (0 \\right )} - \\left. \\frac{d}{d t} y{\\left (t \\right )} \\right|_{\\substack{ t=0 }} = 0$$ # reemplazamos la transfomada de Laplace de y(t) por la incognita Y # para facilitar la lectura de la ecuaci√≥n. L_edo_3 = L_edo_2.subs(sympy.laplace_transform(y(t), t, s), Y) sympy.Eq(L_edo_3) $$Y s^{2} + 3 Y s + 2 Y - s y{\\left (0 \\right )} - 3 y{\\left (0 \\right )} - \\left. \\frac{d}{d t} y{\\left (t \\right )} \\right|_{\\substack{ t=0 }} = 0$$ # Definimos las condiciones iniciales ics = {y(0): 2, y(t).diff(t).subs(t, 0): -3} ics $$\\left \\{ y{\\left (0 \\right )} : 2, \\quad \\left. \\frac{d}{d t} y{\\left (t \\right )} \\right|_{\\substack{ t=0 }} : -3\\right \\}$$ # Aplicamos las condiciones iniciales L_edo_4 = L_edo_3.subs(ics) L_edo_4 $$Y s^{2} + 3 Y s + 2 Y - 2 s - 3$$ # Resolvemos la ecuaci√≥n y arribamos a la Transformada de Laplace # que es equivalente a nuestra ecuaci√≥n diferencial Y_sol = sympy.solve(L_edo_4, Y) Y_sol $$\\left [ \\frac{2 s + 3}{s^{2} + 3 s + 2}\\right ]$$ # Por √∫ltimo, calculamos al inversa de la Transformada de Laplace que # obtuvimos arriba, para obtener la soluci√≥n de nuestra ecuaci√≥n diferencial. y_sol = sympy.inverse_laplace_transform(Y_sol[0], s, t) y_sol $$\\frac{e^{t} + 1}{e^{2 t}}$$ # Comprobamos la soluci√≥n. y_sol.subs(t, 0), sympy.diff(y_sol).subs(t, 0) $$\\left ( 2, \\quad -3\\right )$$ Como podemos ver Transformadas de Laplace, pueden ser una buena alternativa para resolver Ecuaciones diferenciales en forma anal√≠tica. Pero a√∫n as√≠, siguen existiendo ecuaciones que se resisten a ser resueltas por medios anal√≠ticos, para estos casos, debemos recurrir a los m√©todos num√©ricos.\nM√©todos num√©ricos# Cuando todo lo dem√°s falla, llegan los m√©todos num√©ricos al rescate; siempre vamos a poder calcular una aproximaci√≥n num√©rica a una soluci√≥n de una Ecuaci√≥n diferencial. De √©stos m√©todos ya no vamos a obtener las formulas elegantes y acabadas que ven√≠amos viendo, sino que vamos a obtener n√∫meros. Existen muchos enfoques para resolver ecuaciones diferenciales ordinarias en forma num√©rica, y generalmente est√°n dise√±ados para resolver problemas que est√°n formulados como un sistema de ecuaciones diferenciales de primer orden de la forma est√°ndar:\n$$ \\frac{dy}{dx} = f(x, y(x))$$ donde \\(y(x)\\) es un vector de la funciones inc√≥gnitas de \\(x\\).\nLa idea b√°sica de muchos de los m√©todos num√©ricos para resolver ecuaciones diferenciales ordinarias es capturada por el M√©todo de Euler, veamos de que se trata este m√©todo.\nEl M√©todo de Euler# Para entender este m√©todo, revisemos nuevamente la siguiente Ecuaci√≥n diferencial:\n$$ \\frac{dy}{dx} = f(x, y(x))$$ El M√©todo de Euler se√±ala que puede que no tengamos la funci√≥n real que representa la soluci√≥n a la Ecuaci√≥n diferencial precedente. Sin embargo, si poseemos la pendiente de la curva en cualquier punto. Es decir, el ritmo de cambio de la curva, que no es otra cosa que su derivada, la cual podemos utilizar para iterar sobre soluciones en distintos puntos.\nSupongamos entonces que tenemos el punto \\((x_0, y_0)\\), que se encuentra en la curva soluci√≥n. Dada la definici√≥n de la ecuaci√≥n que estamos analizando, sabemos que la pendiente de la curva soluci√≥n en ese punto es \\(f(x_0, y_0)\\). ¬øQu√© pasar√≠a entonces si quisi√©ramos encontrar la soluci√≥n num√©rica en el punto \\((x, y)\\) que se encuentra a una corta distancia \\(h\\)?. En este caso, podr√≠amos definir a \\(y\\) como \\(y = y_0 + \\Delta y\\), es decir, \\(y\\) va ser igual al valor de \\(y\\) en el punto inicial, m√°s el cambio que ocurri√≥ en \\(y\\) al movernos a la distancia \\(h\\). Y como el cambio en \\(y\\) va a estar dado por la pendiente de la curva, que en este caso sabemos que es igual a \\(f(x_0, y_0)\\), podemos definir la siguiente relaci√≥n de recurrencia que es la base para encontrar las soluciones num√©ricas con el M√©todo de Euler:\n$$y = y_0 + f(x_0, y_0)h$$ La cual podemos generalizar para todo \\(n\\) del siguiente forma:\n$$y_n = y_{n -1} + f(x_{n - 1}, y_{n - 1})h$$ .\nEste m√©todo esta √≠ntimamente relacionado con los Campos de direcciones que analizamos m√°s arriba. En general, el M√©todo de Euler dice que empecemos por el punto dado por el condici√≥n incial y continuemos en la direcci√≥n indicada por el Campo de direcciones. Luego nos detengamos, miramos a la pendiente en la nueva ubicaci√≥n, y procedemos en esa direcci√≥n.\nEl M√©todo de Runge-Kutta# Otro m√©todo que podemos utilizar para encontrar soluciones num√©ricas a Ecuaciones diferenciales, y que incluso es m√°s preciso que el M√©todo de Euler, es el M√©todo de Runge-Kutta. En el cual la relaci√≥n de recurrencia va a estar dada por un promedio ponderado de t√©rminos de la siguiente manera:\n$$y_{k + 1} = y_k + \\frac{1}{6}(k_1 + 2k_2 + 2k_3 + k_4)$$ en d√≥nde:\n$$k_1 = f(x_k, y_k)h_k, \\\\ k_2 = f\\left(x_k + \\frac{h_k}{2}, y_k + \\frac{k_1}{2}\\right)h_k, \\\\ k_3 = f\\left(x_k + \\frac{h_k}{2}, y_k + \\frac{k_2}{2}\\right)h_k, \\\\ k_4 = f\\left(x_k + h_k, y_k + k_3\\right)h_k. \\\\ $$ Soluciones num√©ricas con Python# Para poder resolver Ecuaciones diferenciales en forma num√©rica con Python, podemos utilizar las herramienta de integraci√≥n que nos ofrece SciPy, particularmente los dos solucionadores de ecuaciones diferenciales ordinarias, integrate.odeint y integrate.ode. La principal diferencia entre ambos, es que integrate.ode es m√°s flexible, ya que nos ofrece la posibilidad de elegir entre distintos solucionadores. Veamos algunos ejemplos:\nComencemos por la siguiente funci√≥n:\n$$f(x, y(x)) = x + y(x)^2$$ # Defino la funci√≥n f = y(x)**2 + x f $$x + y^{2}{\\left (x \\right )}$$ # la convierto en una funci√≥n ejecutable f_np = sympy.lambdify((y(x), x), f) # Definimos los valores de la condici√≥n inicial y el rango de x sobre los # que vamos a iterar para calcular y(x) y0 = 0 xp = np.linspace(0, 1.9, 100) # Calculando la soluci√≥n numerica para los valores de y0 y xp yp = integrate.odeint(f_np, y0, xp) # Aplicamos el mismo procedimiento para valores de x negativos xn = np.linspace(0, -5, 100) yn = integrate.odeint(f_np, y0, xn) Los resultados son dos matrices unidimensionales de NumPy \\(yp\\) y \\(yn\\), de la misma longitud que las correspondientes matrices de coordenadas \\(xp\\) y \\(xn\\), que contienen las soluciones num√©ricas de la ecuaci√≥n diferencial ordinaria para esos puntos espec√≠ficos. Para visualizar la soluci√≥n, podemos graficar las matrices \\(yp\\) y \\(yn\\), junto con su Campo de direcciones.\nfig, axes = plt.subplots(1, 1, figsize=(8, 6)) plot_direction_field(x, y(x), f, ax=axes) axes.plot(xn, yn, \u0026#39;b\u0026#39;, lw=2) axes.plot(xp, yp, \u0026#39;r\u0026#39;, lw=2) plt.show() En este ejemplo, solucionamos solo una ecuaci√≥n. Generalmente, la mayor√≠a de los problemas se presentan en la forma de sistemas de ecuaciones diferenciales ordinarias, es decir, que incluyen varias ecuaciones a resolver. Para ver como podemos utilizar a integrate.odeint para resolver este tipo de problemas, consideremos el siguiente sistema de ecuaciones diferenciales ordinarias, conocido el atractor de Lorenz:\n$$x'(t) = \\sigma(y -x), \\\\ y'(t) = x(\\rho -z)-y, \\\\ z'(t) = xy - \\beta z $$ Estas ecuaciones son conocidas por sus soluciones ca√≥ticas, que dependen sensiblemente de los valores de los par√°metros \\(\\sigma\\), \\(\\rho\\) y \\(\\beta\\). Veamos como podemos resolverlas con la ayuda de Python.\n# Definimos el sistema de ecuaciones def f(xyz, t, sigma, rho, beta): x, y, z = xyz return [sigma * (y - x), x * (rho - z) - y, x * y - beta * z] # Asignamos valores a los par√°metros sigma, rho, beta = 8, 28, 8/3.0 # Condici√≥n inicial y valores de t sobre los que calcular xyz0 = [1.0, 1.0, 1.0] t = np.linspace(0, 25, 10000) # Resolvemos las ecuaciones xyz1 = integrate.odeint(f, xyz0, t, args=(sigma, rho, beta)) xyz2 = integrate.odeint(f, xyz0, t, args=(sigma, rho, 0.6*beta)) xyz3 = integrate.odeint(f, xyz0, t, args=(2*sigma, rho, 0.6*beta)) # Graficamos las soluciones from mpl_toolkits.mplot3d.axes3d import Axes3D fig, (ax1,ax2,ax3) = plt.subplots(1, 3, figsize=(12, 4), subplot_kw={\u0026#39;projection\u0026#39;:\u0026#39;3d\u0026#39;}) for ax, xyz, c in [(ax1, xyz1, \u0026#39;r\u0026#39;), (ax2, xyz2, \u0026#39;b\u0026#39;), (ax3, xyz3, \u0026#39;g\u0026#39;)]: ax.plot(xyz[:,0], xyz[:,1], xyz[:,2], c, alpha=0.5) ax.set_xlabel(\u0026#39;$x$\u0026#39;, fontsize=16) ax.set_ylabel(\u0026#39;$y$\u0026#39;, fontsize=16) ax.set_zlabel(\u0026#39;$z$\u0026#39;, fontsize=16) ax.set_xticks([-15, 0, 15]) ax.set_yticks([-20, 0, 20]) ax.set_zticks([0, 20, 40]) Como podemos ver, los solucionadores num√©ricos que nos ofrece SciPy son simples de utilizar y pueden simplificar bastante el trabajo de resolver Ecuaciones diferenciales.\nM√©todo anal√≠tico vs M√©todo num√©rico# Al resolver una ecuaci√≥n diferencial ordinaria en forma anal√≠tica, el resultado es una funci√≥n, \\(f\\), que nos permite calcular la poblaci√≥n, \\(f(t)\\), para cualquier valor de \\(t\\). Al resolver una ecuaci√≥n diferencial ordinaria en forma num√©ricamente, se obtienen dos matrices de una dimensi√≥n. Podemos pensar a estas matrices como una aproximaci√≥n discreta de la funci√≥n continua \\(f\\): \u0026ldquo;discreta\u0026rdquo;, ya que s√≥lo se define para ciertos valores de \\(t\\), y \u0026ldquo;aproximada\u0026rdquo;, porque cada valor \\(F_i\\) es s√≥lo una estimaci√≥n del verdadero valor de \\(f(t)\\). Por tanto, estas son limitaciones de las soluciones num√©ricas. La principal ventaja es que se puede calcular soluciones num√©ricas de ecuaciones diferenciales ordinarias que no tienen soluciones anal√≠ticas, que son la gran mayor√≠a de las ecuaciones diferenciales ordinarias no lineales.\nCon esto concluyo este paseo por una de las m√°s fruct√≠feras ideas de las matem√°ticas aplicadas, las Ecuaciones diferenciales; espero que les pueda servir de gu√≠a para facilitarles su soluci√≥n.\nSaludos!\nEste post fue escrito utilizando Jupyter notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2016-01-10","id":28,"permalink":"/blog/2016/01/10/ecuaciones-diferenciales-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Vivimos en un mundo en constante cambio. La posici√≥n de la Tierra cambia con el tiempo; la velocidad de un objeto en ca√≠da libre cambia con la distancia; el √°rea de un c√≠rculo cambia seg√∫n el tama√±o de su radio; la trayectoria de un proyectil cambia seg√∫n la velocidad y el √°ngulo de disparo.","tags":["python","matematica","calculo","derivada","integral","ecuaciones diferenciales"],"title":"Ecuaciones Diferenciales con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# El art√≠culo de hoy se lo voy a dedicar a uno los n√∫meros m√°s fascinantes de las Matem√°ticas, el n√∫mero \\(e\\). Este n√∫mero es de suma importancia, ya que lo podemos encontrar en una gran variedad de fen√≥menos, desde F√≠sica y Biolog√≠a, hasta Finanzas, Arte y M√∫sica. As√≠ como el famoso \\(\\pi\\), gobierna el c√≠rculo, \\(e\\) gobierna el C√°lculo y los Logaritmos.\n¬øQu√© es el n√∫mero \\(e\\)?# El n√∫mero \\(e\\) es una de las m√°s importantes constantes matem√°ticas, pertenece al grupo de los n√∫meros irracionales, es decir, que el mismo no puede ser expresado como una fracci√≥n de dos n√∫meros enteros y su expansi√≥n decimal es infinita. Asimismo, tambi√©n es un n√∫mero trascendental, lo que quiere decir que tampoco puede ser expresado algebraicamente. Sus primeros d√≠gitos son:\n$$e = 2.7182818284590452353602874713527\\dots$$ Otra cosa que hace a \\(e\\) sumamente interesante, es que es la base del Logaritmo natural, es decir que si \\(\\ln(x) = y\\), entonces \\(e^y = x\\). Tambi√©n podemos encontrar a \\(e\\), en la funci√≥n exponencial, \\(e^x\\), la cual es la inversa de la funci√≥n del Logaritmo natural, y tiene la particularidad de que la derivada de \\(e^x\\), es la misma funci√≥n \\(e^x\\). Esta caracter√≠stica, es lo que que hace a \\(e\\) fundamental para el C√°lculo.\nHistoria del n√∫mero \\(e\\)# Para poder entender m√°s en profundidad la naturaleza del n√∫mero \\(e\\), debemos recorrer un poco de su historia, que a diferencia de \\(\\pi\\), que tiene una historia milenaria, es relativamente reciente.\nInvenci√≥n del Logaritmo# La historia del n√∫mero \\(e\\) comienza en el siglo XVII, cuando John Napier invent√≥ el concepto de Logaritmo. Pocas veces en la historia de la ciencia, un concepto matem√°tico fue recibido con tanto entusiasmo por la comunidad cient√≠fica, como fue el caso del Logaritmo. La idea b√°sica detr√°s de este concepto abstracto es la siguiente: Si uno pudiera escribir cualquier n√∫mero positivo como la potencia de otro n√∫mero fijo (llamado base), entonces la multiplicaci√≥n y la divisi√≥n de n√∫meros pasar√≠a a ser equivalente a sumar o restar sus potencias; lo que simplificar√≠a y facilitar√≠a los c√°lculos. Para entenderlo mejor, veamos un ejemplo, supongamos que tenemos la siguiente tabla de potencias de 2.\n\\(n\\) -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12 \\(2^n\\) 1/8 1/4 1/2 1 2 4 8 16 32 64 128 256 512 1024 2048 4096 Ahora si quisi√©ramos saber el resultado de multiplicar 32 por 64; simplemente deber√≠amos buscar las potencias de estos dos n√∫meros en la tabla anterior, 5 para 32 y 6 para 64, luego sumamos estos dos n√∫meros, los que nos da como resultado 11 y por √∫ltimo si vamos a buscar el resultado de elevar 2 a la 11 potencia en la misma tabla, encontramos el resultado de nuestra multiplicaci√≥n, que es 2048. Utilizando una simple tabla de Logaritmos, se pod√≠an resolver multiplicaciones y divisiones complejas en segundos, algo muy √∫til en el siglo XVII, d√≥nde no exist√≠an las calculadoras!.\nSi bien John Napier desarrollo el concepto de Logaritmo, el mismo difiere un poco del concepto moderno que tenemos hoy en d√≠a; y tampoco fue el responsable de notar el poder \\(e\\) como base de los mismos. Tendr√≠amos que esperar algunos a√±os m√°s hasta que \\(e\\) se apoderara por completo de los Logaritmos, como la base m√°s natural para ellos. Veamos el camino que hubo que recorrer.\nEl √°rea de la hip√©rbola# Un problema muy com√∫n en Matem√°ticas, es el de encontrar el √°rea de una figura plana, este proceso se conoce con el nombre de cuadratura o integraci√≥n. Desde que los matem√°ticos comenzaron a trabajar en estos problemas, una de las figuras que m√°s ha resistido obstinadamente todos los intentos de cuadratura era la hip√©rbola, la cual esta definida por la funci√≥n \\(f(x) = \\frac{1}{x}\\).\nUn matem√°tico que se interes√≥ por un tema muy cercano a este, tambi√©n en el siglo XVII, fue Pierre de Fermat. Fermat se interes√≥ en la cuadratura de curvas cuya funci√≥n general tiene la forma \\(f(x) = x^n\\), d√≥nde \\(n\\) es un n√∫mero entero. Estas curvas son llamadas generalmente par√°bolas. Utilizando el m√©todo de exhauci√≥n, el cu√°l explique en mi art√≠culo anterior, Fermat fue capaz de llegar a una f√≥rmula general para encontrar el √°rea de esta familia de curvas. La f√≥rmula a la que arrib√≥ fue la siguiente:\n$$A = \\frac{a^{n + 1}}{n + 1}$$ Como podemos ver, esta no es ni m√°s ni menos que la regla de integraci√≥n para funciones de grado $n$.\n$$\\int x^{n} \\ dx = \\frac{x^{n + 1}}{n + 1}$$ Debemos recordar, sin embargo, que el trabajo de Fermat fue hecho alrededor de 1640, aproximadamente unos 30 a√±os antes de que Newton y Leibniz establecieran esta f√≥rmula como parte del c√°lculo integral. A pesar de que Fermat pudo probar que su f√≥rmula pod√≠a ser aplicada tanto para los casos de \\(n\\) siendo un entero positivo como negativo; hubo un caso que se le sigui√≥ escapando, el caso de \\(n = -1\\), es decir el caso del √°rea de la hip√©rbola, \\(f(x) = \\frac{1}{x}\\); ya que este caso hace al denominador de su formula (\\(n + 1\\)) igual a 0. A pesar de todos sus esfuerzos, Fermat nunca pudo llegar a resolver este caso.\nLa luz reci√©n llegar√≠a en el a√±o 1661, cuando Christiaan Huygens se diera cuenta de que para el caso de \\(n = -1\\), todos los rect√°ngulos utilizados en la aproximaci√≥n del √°rea bajo la hip√©rbola, ten√≠an la misma √°rea, es decir que a medida que la distancia crece geom√©tricamente desde 0, las √°rea se incrementan en la misma proporci√≥n. Lo que implica que la relaci√≥n entre el √°rea y la distancia, es logar√≠tmica!. Por tanto para resolver el problema del √°rea de la hip√©rbola, \\(y = \\frac{1}{x}\\), simplemente deber√≠amos aplicar la funci√≥n logar√≠tmica sobre \\(x\\); el √∫nico problema restante, era encontrar la base de esa funci√≥n logar√≠tmica, que como ya se pueden ir imaginando, no es ni nada m√°s ni nada menos que el n√∫mero \\(e\\).\nEs decir, que entonces para resolver el √°rea bajo la hip√©rbola, debemos calcular el Logaritmo natural de \\(x\\). Lo que llevado a la notaci√≥n actual del c√°lculo integral equivale a decir que:\n$$\\int \\left(\\frac{1}{x}\\right) \\ dx = \\ln (x)$$ Desde este momento en adelante, \\(e\\) comenzar√≠a a convertirse en el amo y se√±or de los Logaritmos. Pero no se iba a quedar solo con eso, ya que a√∫n quedaban por descubrirse otras importantes propiedades de este misterioso n√∫mero.\nEuler# Sin dudas, uno de los m√°s grandes matem√°ticos de todos los tiempos fue Leonhard Euler. Pr√°cticamente no dej√≥ una rama de las Matem√°ticas sin tocar, dejando su marca en campos tan diversos como el an√°lisis matem√°tico, la teor√≠a de n√∫meros, la mec√°nica y la hidrodin√°mica, la cartograf√≠a, la topolog√≠a, la √≥ptica y la astronom√≠a. A √©l tambi√©n le debemos muchos de los s√≠mbolos matem√°ticos que usamos hoy en d√≠a, como ser, \\(i, \\ f(x), \\ \\pi\\) y el mismo s√≠mbolo del n√∫mero \\(e\\). La m√°s influyente de sus numerosas obras fue su Introductio in analysin infinitorum, una obra en dos vol√∫menes publicados en 1748 y considerada como la base de an√°lisis matem√°tico moderno. En esta obra, Euler convierte a la funci√≥n en el concepto central del an√°lisis. Su definici√≥n de funci√≥n es la que se suele utilizar hoy en d√≠a tanto en Matem√°ticas aplicadas, como en F√≠sica.\nEn el Introductio, Euler, hace notar por primera vez, el rol central que tiene el n√∫mero \\(e\\), y m√°s precisamente la funci√≥n exponencial \\(e^x\\), y su funci√≥n inversa, la funci√≥n logar√≠tmica \\(\\ln x\\) para el C√°lculo. Euler puso a √©stas dos √∫ltimas funciones en iguales condiciones, al darles definiciones independientes a cada una de ellas. Como ser:\n$$e^x = \\lim_{n \\to \\infty} \\left(1 + \\frac{x}{n}\\right)^n \\\\ \\\\ \\\\ \\ln x = \\lim_{n \\to \\infty} n(x^{1/n} - 1)$$ Finalmente, a Euler tambi√©n le debemos una de las ecuaciones m√°s famosas e incre√≠bles de todas las Matem√°ticas. La ecuaci√≥n:\n$$e^{i\\pi} + 1 = 0$$ En una sola f√≥rmula, Euler logr√≥ conectar a las cinco m√°s importantes constantes de las Matem√°ticas, y tambi√©n tres de las m√°s importantes operaciones matem√°ticas (adici√≥n, multiplicaci√≥n y exponenciaci√≥n). Las cinco constantes simbolizan las cuatro principales ramas de las Matem√°ticas cl√°sicas: la Aritm√©tica, representada por el 0 y el 1; el √Ålgebra por \\(i\\); la Geometr√≠a, por \\(\\pi\\); y el An√°lisis, representado por \\(e\\).\nA partir de la obra de Euler, el n√∫mero \\(e\\), se convirti√≥ tambi√©n en amo y se√±or del C√°lculo; gobernando as√≠, tanto a los Logaritmos como al An√°lisis.\nCalculando a \\(e\\)# Existen muchas maneras de calcular al n√∫mero \\(e\\), por ejemplo una forma de calcularlo es aplicando el siguiente l√≠mite:\n$$e = \\lim_{n \\to \\infty} \\left(1 + \\frac{1}{n}\\right)^n$$ A medida que \\(n\\) se va haciendo cada vez m√°s grande, vamos ganando precisi√≥n en el c√°lculo de los decimales de \\(e\\).\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; %matplotlib inline import matplotlib.pyplot as plt import numpy as np import pandas as pd def f(n): return (1 + 1.0 / n)**n n = np.array([1, 5, 10, 100, 1000, 10000, 100000, 1000000, 100000000 ]) y = f(n) tabla = pd.DataFrame(list(zip(n, y)), columns=[\u0026#39;n\u0026#39;, \u0026#39;e\u0026#39;]) tabla n e 0 1 2.000000 1 5 2.488320 2 10 2.593742 3 100 2.704814 4 1000 2.716924 5 10000 2.718146 6 100000 2.718268 7 1000000 2.718280 8 100000000 2.718282 Ver C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; # Graficando (1 + 1/n)**n n = np.arange(1, 100000) plt.figure(figsize=(8,6)) plt.title(r\u0026#34;Graficando $(1 + 1/n)^n$\u0026#34;) plt.xlabel(\u0026#39;n\u0026#39;) plt.ylabel(r\u0026#34;$(1 + 1/n)^n$\u0026#34;) plt.plot(n, f(n)) plt.axhline(y=np.e, color = \u0026#39;r\u0026#39;, label=\u0026#39;$e$\u0026#39;) plt.xlim([0, 100]) plt.legend() plt.show() Como podemos ver, tanto del ejemplo num√©rico como del gr√°fico, esta definici√≥n de \\(e\\), tarda bastante en converger hacia el valor exacto. Necesitamos un valor bastante grande de \\(n\\) para ganar precisi√≥n.\nOtra definici√≥n de \\(e\\) que podemos utilizar para calcularlo y que converge mucho m√°s r√°pido, es su definici√≥n como una serie infinita de factoriales.\n$$e = \\frac{1}{0!} + \\frac{1}{1!} + \\frac{1}{2!}+ \\frac{1}{3!}+ \\frac{1}{4!}+ \\frac{1}{5!}+ \\frac{1}{6!}+ \\frac{1}{7!} + \\dots$$ Por √∫ltimo, tambi√©n podr√≠amos expresar a \\(e\\), utilizando fracciones continuas, del siguiente modo:\nAl igual que ocurre con el n√∫mero \\(\\pi\\) se conocen millones de d√≠gitos del n√∫mero \\(e\\). Por ejemplo, para generar la imagen de la cabecera del art√≠culo, yo utilic√© 390 decimales de \\(e\\).\n\\(e\\) en las finanzas# Como coment√© a lo largo de todo el art√≠culo, nos podemos topar con el n√∫mero \\(e\\) en infinidad de situaciones; pero una de las √°reas donde m√°s lo podemos encontrar es en la finanzas, ya que \\(e\\) se encuentra escondido en la definici√≥n de una de las formulas m√°s fundamentales del C√°lculo financiero, la f√≥rmula del inter√©s compuesto. Por ejemplo, si repasamos la formula del valor futuro:\n$$FV = PV \\left(1 + \\frac{r}{n}\\right)^n$$ donde \\(FV\\) es el valor futuro; \\(PV\\) es el valor presente de nuestra inversi√≥n; \\(r\\) es la tasa de inter√©s anual, expresada como valor decimal; y \\)n\\) es el n√∫mero de per√≠odos.\nY si miramos detenidamente a esta ecuaci√≥n, podemos ver cierta similitud con la definici√≥n de \\(e\\) que dimos m√°s arriba.\n$$e = \\lim_{n \\to \\infty} \\left(1 + \\frac{1}{n}\\right)^n$$ Es m√°s, si tom√°ramos el caso hipot√©tico en el que el valor presente sea igual a 1, es decir \\(PV = 1\\) y la tasa de inter√©s sea del 100 %, \\(r = 1\\). Podemos ver que la f√≥rmula del valor futuro se convierte en la definici√≥n del n√∫mero \\(e\\)!.\nPor esta raz√≥n, al n√∫mero \\(e\\) lo vamos a encontrar en infinidad de situaciones en el C√°lculo financiero. Por ejemplo, la definici√≥n de la f√≥rmula de la capitalizaci√≥n continua utiliza al n√∫mero \\(e\\) expl√≠citamente:\n$$FV = PV \\cdot e^{rt}$$ donde \\(FV\\) es el valor futuro; \\(PV\\) es el valor presente de nuestra inversi√≥n; \\(r\\) es la tasa de inter√©s efectiva y \\(t\\) es el tiempo de capitalizaci√≥n.\n√âstos son s√≥lo algunos casos, a medida que nos adentramos m√°s en la complejidades del C√°lculo financiero vamos a ver que \\(e\\) continua apareciendo una y otra vez; pero eso va a quedar para pr√≥ximos art√≠culos!\nAqu√≠ concluye el art√≠culo, espero les haya gustado y hayan sentido cierta fascinaci√≥n por este m√≠stico n√∫mero que encontramos por todas partes!\nSaludos!\nEste post fue escrito utilizando Jupyter notebook. Pueden descargar este notebook o ver su versi√≥n est√°tica en nbviewer.\n","date":"2015-12-13","id":29,"permalink":"/blog/2015/12/13/el-numero-e.-el-numero-de-las-finanzas/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# El art√≠culo de hoy se lo voy a dedicar a uno los n√∫meros m√°s fascinantes de las Matem√°ticas, el n√∫mero \\(e\\). Este n√∫mero es de suma importancia, ya que lo podemos encontrar en una gran variedad de fen√≥menos, desde F√≠sica y Biolog√≠a, hasta Finanzas, Arte y M√∫sica.","tags":["python","matematica","calculo","finanazas","logaritmo"],"title":"El n√∫mero e. El n√∫mero de las finanzas"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# El C√°lculo es una rama muy importante de la Matem√°tica moderna; tiene profundas ra√≠ces en problemas f√≠sicos y gran parte de su potencia y belleza derivan de la variedad de sus aplicaciones. Las subramas conocidas como C√°lculo integral y C√°lculo diferencial son instrumentos naturales y poderosos para atacar m√∫ltiples problemas que surgen en F√≠sica, Astronom√≠a, Ingenier√≠a, Qu√≠mica, Geolog√≠a, Biolog√≠a, y en otros campos de las ciencias. El C√°lculo no s√≥lo es un instrumento t√©cnico, sino que contiene una colecci√≥n de ideas fascinantes y atrayentes que han ocupado el pensamiento humano durante cientos de a√±os. Estas ideas est√°n relacionadas con la velocidad, el √°rea, el volumen, la raz√≥n de crecimiento, la tangente a una l√≠nea, y dem√°s.\nHistoria# El origen del C√°lculo se remonta a m√°s de 2300 a√±os, cuando los griegos intentaban resolver el problema del √°rea ideando el procedimiento que llamaron m√©todo de exhauci√≥n. La idea esencial de este m√©todo consiste en intentar determinar el √°rea de una regi√≥n por medio de aproximaciones utilizando regiones poligonales cuya √°rea sea m√°s f√°cil de calcular, la idea es continuar con el proceso aumentando los lados de los pol√≠gonos hasta llegar a la mejor aproximaci√≥n posible de la regi√≥n que queremos determinar. Este m√©todo fue usado satisfactoriamente por Arqu√≠medes (287-212 A.C.) para hallar f√≥rmulas exactas de las √°reas del c√≠rculo y de algunas otras figuras especiales. En la siguiente figura podemos ver al m√©todo de exhauci√≥n aplicado para determinar el √°rea del c√≠rculo.\nDesde Arqu√≠medes, gradualmente, el m√©todo de exhauci√≥n fue transform√°ndose en lo que hoy se conoce como C√°lculo integral, nueva y potente disciplina que, como ya mencionamos, tiene numerosas aplicaciones no s√≥lo en problemas relativos a √°reas y vol√∫menes, sino tambi√©n en problemas de otras ciencias. El C√°lculo integral, que mantiene alguno de los caracteres originales del m√©todo de exhauci√≥n, recibi√≥ su mayor impulso en el siglo XVII, debido a los esfuerzos de Isaac Newton (1642-1727) y Gottfried Leibniz (1646-1716), y su desarrollo continu√≥ durante el siglo XIX, hasta que Augustin-Louis Cauchy (1789-1857) y Bernhard Riemann (1826-1866) le dieron una base matem√°tica firme.\nFunciones# Las Funciones son los objetos fundamentales con los que tratamos en el C√°lculo. Las mismas pueden ser representadas de diferentes maneras: por una ecuaci√≥n, en una tabla, por un gr√°fico, o en palabras. Se utilizan principalmente como modelos matem√°ticos para representar fen√≥menos del mundo real.\nLa palabra Funci√≥n fue introducida en las Matem√°ticas por Leibniz, quien utilizaba este t√©rmino para designar cierto tipo de f√≥rmulas matem√°ticas. Una Funci√≥n surge cada vez que una cantidad depende de otra. M√°s precisamente la definici√≥n de Funci√≥n es esencialmente la siguiente: Dados dos conjuntos de objetos, el conjunto X y el conjunto Y, una Funci√≥n es una regla que asocia a cada objeto de X, uno y s√≥lo un, objeto en Y. El conjunto X se denomina el dominio de la Funci√≥n. Los objetos de Y, asociados con los objetos en X forman otro conjunto denominado el recorrido de la Funci√≥n. Generalmente se utilizan las letras \\(f, g, h, G\\) y \\(H\\) para designarlas. Si \\(f\\) es una funci√≥n dada y \\(x\\) es un objeto de su dominio, la notaci√≥n \\(f(x)\\) se utiliza para designar el objeto que en el recorrido corresponde a \\(x\\), en la Funci√≥n \\(f\\), y se denomina el valor de la funci√≥n \\(f\\) en \\(x\\). El s√≠mbolo \\(f(x)\\) se lee, ¬´f de x¬ª.\nMuchas veces resulta √∫til pensar en una Funci√≥n como si fuera una m√°quina. Si \\(x\\) est√° en el dominio de la funci√≥n \\(f\\), entonces cuando \\(x\\) entra en la m√°quina, se acepta como una entrada y la m√°quina produce una salida \\(f(x)\\) de acuerdo a la regla de la funci√≥n. As√≠, podemos pensar al dominio como el conjunto de todas las entradas posibles y al recorrido como el conjunto de todas las salidas posibles.\nEl m√©todo m√°s com√∫n para la visualizaci√≥n de una Funci√≥n es su gr√°fica. Si \\(f\\) es una Funci√≥n con dominio \\(D\\), a continuaci√≥n, su gr√°fica es el conjunto de pares ordenados.\n$$\\{(x, f(x)) \\mid x \\in D \\} $$ Aqu√≠ debemos tener en cuenta que el par \\((x, f(x))\\), es un par entrada-salida, el valor de \\(x\\) representa el valor de entrada, mientras que el valor de \\(f(x)\\) representa la salida de la Funci√≥n. En otras palabras, la gr√°fica de \\(f\\) se compone de todos puntos \\((x, y)\\) en el plano de coordenadas tal que \\(y=f(x)\\) y \\(x\\) est√° en el dominio de \\(f\\). La gr√°fica de una Funci√≥n \\(f\\) nos da una imagen √∫til del comportamiento o la \u0026ldquo;historia de vida\u0026rdquo; de la misma.\nFunciones con Python# Para definir las Funciones en Python utilizamos la instrucci√≥n def. As√≠ por ejemplo si quisi√©ramos definir a la Funci√≥n \\(f(x) = \\sqrt{x + 2}\\) dentro de Python, lo podr√≠amos hacer de la siguiente forma:\nimport numpy as np def f(x): return np.sqrt(x + 2) En este ejemplo, primero estamos importando la librer√≠a numpy, para trabajar m√°s f√°cilmente con [vectores](https://es.wikipedia.org/wiki/Vector_(inform%C3%A1tica), los cuales simplifican los c√°lculos num√©ricos. Luego utilizamos la instrucci√≥n def para definir la funci√≥n, que este caso se va a llamar f y va a tener como √∫nico par√°metro al objeto x. Esta funci√≥n nos va a devolver el valor de la ra√≠z cuadrada de \\(x + 2\\). Ahora, si por ejemplo quisi√©ramos saber los valores de la funci√≥n \\(f(x)\\) para los \\(x, -2, -1, 0, 2, 4\\) y \\(6\\). podr√≠amos invocar a esta funci√≥n de la siguiente manera:\nx = np.array([-2, -1, 0, 2, 4, 6]) # Creando el vector de valores de x y = f(x) y array([ 0. , 1. , 1.41421356, 2. , 2.44948974, 2.82842712]) Si quisi√©ramos verlo en forma de tabla, podemos ayudarnos de la librer√≠a pandas y su estructura de datos DataFrame, la cual tiene una forma tabular.\nimport pandas as pd tabla = pd.DataFrame( list(zip(x, y)), columns=[\u0026#39;x\u0026#39;, \u0026#39;f(x)\u0026#39;] ) tabla x f(x) 0 -2 0.000000 1 -1 1.000000 2 0 1.414214 3 2 2.000000 4 4 2.449490 5 6 2.828427 Por √∫ltimo, si quisi√©ramos graficar funciones con Python, podemos utilizar la librer√≠a Matplotlib, y pasarle los valores de \\(x\\) e \\(y\\) al m√©todo plot del objeto pyplot.\n%matplotlib inline import matplotlib.pyplot as plt def move_spines(): \u0026#34;\u0026#34;\u0026#34;Esta funcion divide pone al eje y en el valor 0 de x para dividir claramente los valores positivos y negativos.\u0026#34;\u0026#34;\u0026#34; fix, ax = plt.subplots() for spine in [\u0026#34;left\u0026#34;, \u0026#34;bottom\u0026#34;]: ax.spines[spine].set_position(\u0026#34;zero\u0026#34;) for spine in [\u0026#34;right\u0026#34;, \u0026#34;top\u0026#34;]: ax.spines[spine].set_color(\u0026#34;none\u0026#34;) return ax x = np.linspace(-2, 6, num=30) ax = move_spines() ax.grid() ax.plot(x, f(x)) plt.title(r\u0026#34;Grafico de $f(x)=\\sqrt{x + 2}$\u0026#34;) plt.ylabel(\u0026#39;f(x)\u0026#39;) plt.xlabel(\u0026#39;x\u0026#39;) plt.show() L√≠mites# Uno de los conceptos m√°s importantes dentro del C√°lculo es el concepto de L√≠mite. Se dice que una funci√≥n \\(f\\) tiende hacia el L√≠mite \\(l\\) cerca de \\(a\\), si se puede hacer que \\(f(x)\\) este tan pr√≥xima como queramos de \\(l\\), haciendo que \\(x\\) est√© suficientemente cerca de \\(a\\), pero siendo distinta de \\(a\\). As√≠ por ejemplo si analizamos la funci√≥n \\(f(x) = x^2 - x + 2\\), para los valores cercanos a 2, podr√≠amos ver los siguientes resultados.\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; def f(x): return x**2 - x + 2 x = np.array([1, 1.5, 1.9, 1.95, 1.99, 1.999, 2.001, 2.05, 2.1, 2.2, 2.5, 3 ]) y = f(x) tabla = pd.DataFrame(list(zip(x, y)), columns=[\u0026#39;x\u0026#39;, \u0026#39;f(x)\u0026#39;]) tabla x f(x) 0 1.000 2.000000 1 1.500 2.750000 2 1.900 3.710000 3 1.950 3.852500 4 1.990 3.970100 5 1.999 3.997001 6 2.001 4.003001 7 2.050 4.152500 8 2.100 4.310000 9 2.200 4.640000 10 2.500 5.750000 11 3.000 8.000000 de acuerdo con esta tabla, podemos ver que a medida que hacemos al valor de \\(x\\) cercano a 2, vemos que \\(f(x)\\) se hace muy cercana a 4. Incluso podr√≠amos hacer a \\(f(x)\\) tan cercana como queramos a 4, haciendo que \\(x\\) este lo suficientemente cerca de 2. Por lo tanto, podemos expresar esta propiedad diciendo que el \u0026quot;L√≠mite de la funci√≥n \\(f(x) = x^2 - x + 2\\) cuando \\(x\\) se acerca a 2 es igual a 4.\u0026quot; y lo podemos representar con la siguiente notaci√≥n:\n$$\\lim_{x\\to 2} \\left(x^2 -x + 2\\right) = 4$$ Gr√°ficamente lo podemos ver del siguiente modo.\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; x = np.linspace(-2, 4, num=30) ax = move_spines() ax.grid() ax.plot(x, f(x)) ax.scatter(2, 4, label=\u0026#34;limite cuando x tiende a 2\u0026#34;, color=\u0026#39;r\u0026#39;) plt.legend() plt.title(r\u0026#34;Grafico de $f(x)=x^2 -x + 2$\u0026#34;) plt.ylabel(\u0026#39;f(x)\u0026#39;) plt.xlabel(\u0026#39;x\u0026#39;) plt.show() Las leyes de los l√≠mites# Calcular el valor exacto de los L√≠mites muchas veces no suele tan f√°cil como reemplazar el valor de \\(a\\) en \\(f(x)\\). Es por esto que es importante conocer algunas propiedades de los L√≠mites, ellas son:\nLey de la suma: El l√≠mite de la suma de dos funciones es la suma de sus l√≠mites. Ley de la diferencia: El l√≠mite de la diferencia de dos funciones es la diferencia de sus l√≠mites. Ley del producto: El l√≠mite del producto de dos funciones es el producto de sus l√≠mites. ley del m√∫ltiplo constante: El l√≠mite de una constante por una funci√≥n es la constante por el l√≠mite de la funci√≥n. Ley del cociente: El l√≠mite del cociente de dos funciones es el cociente de sus l√≠mites, siempre que el l√≠mite del denominador sea diferente de cero. Es decir que si tenemos a la constante \\(C\\) y a los l√≠mites \\(\\lim_{x\\to a} f(x)\\) y \\(\\lim_{x\\to a} g(x)\\). Entonces podemos expresar estas propiedades matem√°ticamente de la siguiente forma:\n1- Ley de la suma: \\(\\lim_{x\\to a} [f(x) + g(x)] = \\lim_{x\\to a} f(x) + \\lim_{x\\to a} g(x)\\).\n2- Ley de la diferencia: \\(\\lim_{x\\to a} [f(x) - g(x)] = \\lim_{x\\to a} f(x) - \\lim_{x\\to a} g(x)\\).\n3- Ley del producto: \\(\\lim_{x\\to a} [f(x) \\cdot g(x)] = \\lim_{x\\to a} f(x) \\cdot \\lim_{x\\to a} g(x)\\).\n4- ley del multiplo constante: \\(\\lim_{x\\to a} [C \\cdot f(x)] = C \\cdot \\lim_{x\\to a} f(x)\\).\n5- Ley del cociente: \\(\\lim_{x\\to a} \\left[\\frac{f(x)}{g(x)}\\right] = \\frac{\\lim_{x\\to a} f(x)}{\\lim_{x\\to a} g(x)}\\), si \\(\\lim_{x\\to a} g(x) \\ne 0\\).\nCalculando L√≠mites con Python# Con Python, podemos resolver L√≠mites f√°cilmente utilizando la librer√≠a SymPy, la cual nos proporciona el objeto Limit para representarlos en Python. Su sintaxis es la siguiente: Limit(funci√≥n, variable, punto). Entonces para calcular el l√≠mite de \\(f(x)\\) cuando \\(x\\) tiende a 0, debemos escribir:\nLimit(f(x), x, 0)\nLo utilizamos de la siguiente forma:\nfrom sympy.interactive import printing from sympy import Limit, limit, Symbol, S # imprimir con notaci√≥n matem√°tica. printing.init_printing(use_latex=\u0026#39;mathjax\u0026#39;) x = Symbol(\u0026#39;x\u0026#39;) # Creando el simbolo x. Limit(x**2 - x + 2, x, 2) # Creando el objeto Limit $$\\lim_{x \\to 2^+}\\left(x^{2} - x + 2\\right)$$ # Resolviendo el Limite con el metodo doit() Limit(x**2 - x + 2, x, 2).doit() $$4$$ # La funcion limit nos da directamente el resultado limit(x**2 - x + 2, x, 2) $$4$$ # Resolviendo limite 1/x cuando x tiende a infinito Limit(1/x, x, S.Infinity) $$\\lim_{x \\to \\infty} \\frac{1}{x}$$ Limit(1/x, x, S.Infinity).doit() $$0$$ Como vemos, primero creamos el s√≠mbolo para representar a la variable x utilizando el objeto Symbol, y luego creamos nuestro l√≠mite utilizando el objeto Limit. Por √∫ltimo para resolver el l√≠mite, simplemente llamamos al m√©todo doit() sobre el objeto Limit que acabamos de crear. Tambi√©n podemos calcular los L√≠mites de valores de \\(x\\) que tiendan hacia el infinito utilizando la clase especial S.Infinity que nos proporciona SymPy.\nAhora que ya conocemos que es una Funci√≥n y que es un L√≠mite, ya estamos en condiciones de adentrarnos en el C√°lculo diferencial y analizar el concepto de Derivada.\nDerivadas# Para poder comprender el concepto de Derivada primero debemos abordar el problema de la recta tangente a un curva. La palabra tangente se deriva de la palabra griega Tangens, que significa \u0026ldquo;que toca\u0026rdquo;. As√≠ una tangente a una curva es una l√≠nea que toca la curva. En otras palabras, una l√≠nea tangente debe tener la misma direcci√≥n que la curva en el punto de contacto. Para un c√≠rculo podr√≠amos simplemente seguir la definici√≥n de Euclides y decir que la tangente es una l√≠nea que cruza el c√≠rculo una y s√≥lo una vez (ver figura a). Pero para curvas m√°s complicadas este definici√≥n es inadecuada. Por ejemplo en la figura b podemos ver dos l√≠neas \\(l\\) y \\(t\\) que pasan por el punto \\(P\\) en una curva \\(C\\) . La l√≠nea \\(l\\) cruza a la curva \\(C\\) s√≥lo una vez, pero ciertamente no se parece a lo que pensamos como una tangente. La l√≠nea \\(t\\), en cambio, se parece a una tangente pero intercepta a \\(C\\) dos veces.\nEl intento de resolver este problema fue lo que condujo a Fermat a descubrir algunas de las ideas rudimentarias referentes a la noci√≥n de Derivada. Aunque la derivada se introdujo inicialmente para el estudio del problema de la tangente, pronto se vio que proporcionaba tambi√©n un instrumento para el c√°lculo de velocidades y, en general para el estudio de la variaci√≥n o tasa de cambio de una funci√≥n.\nLa Derivada de una funci√≥n es una medida de la rapidez con la que cambia el valor de dicha funci√≥n, seg√∫n cambie el valor de su variable independiente. La Derivada de una funci√≥n es un concepto local, es decir, se calcula como el l√≠mite de la rapidez de cambio medio de la funci√≥n en un cierto intervalo, cuando el intervalo considerado para la variable independiente se torna cada vez m√°s peque√±o. Por ello se habla del valor de la derivada de una cierta funci√≥n en un punto dado. Entonces el valor de la Derivada de una funci√≥n en un punto puede interpretarse geom√©tricamente, ya que se corresponde con la pendiente de la recta tangente a la gr√°fica de la funci√≥n en dicho punto. La recta tangente es a su vez la gr√°fica de la mejor aproximaci√≥n lineal de la funci√≥n alrededor de dicho punto. La noci√≥n de Derivada puede generalizarse para el caso de funciones de m√°s de una variable con la derivada parcial y el diferencial.\nMatem√°ticamente, la Derivada es una caso especial de L√≠mite, el cual surge cada vez que queremos calcular la pendiente de la recta tangente o la velocidad de cambio de un objeto. √âste L√≠mite ocurre tan frecuentemente que se le ha da un notaci√≥n y un nombre determinados. As√≠ la Derivada de una funci√≥n \\(f\\) en el punto a, representada por \\(f\u0026rsquo;(a)\\), es:\n$$f'(a) = \\lim_{h \\to 0}\\frac{f(a + h) - f(a)}{h}$$ donde \\(h\\) representa la variaci√≥n de \\(a\\). Esta misma definici√≥n, puede ser representada tambi√©n del siguiente modo, utilizando la notaci√≥n de Leibniz.\n$$\\frac{dy}{dx} = \\lim_{dx \\to 0}\\frac{f(x + dx) - f(x)}{dx}$$ As√≠, por ejemplo si quisi√©ramos saber cu√°l es la Derivada de la funci√≥n \\(f(x) = x^3\\), podemos aplicar la definici√≥n anterior del siguiente modo.\nComenzamos, definiendo a \\(f(x + dx) = (x + dx)^3\\), luego expandimos a:\n$$(x + dx)^3 = f(x + dx) = x^3 + 3x^2dx + 3xdx^2 + dx^3$$ Luego reemplazamos esta funci√≥n en nuestra definici√≥n de Derivada:\n$$\\frac{dx}{dy} = \\frac{x^3 + 3x^2dx + 3xdx^2 + dx^3 - x^3}{dx}$$ Simplificamos los t√©rminos:\n$$\\frac{dx}{dy} = \\frac{3x^2dx + 3xdx^2 + dx^3}{dx} \\Rightarrow 3x^2 + 3xdx + dx^2$$ y cuando \\(dx\\) tiende a cero, obtenemos finalmente la funci√≥n Derivada:\n$$\\frac{d}{dx}x^3 = 3x^2$$ Reglas de Derivaci√≥n# Si fuera siempre necesario calcular las Derivadas directamente de la definici√≥n, como hicimos anteriormente, √©stos c√°lculos podr√≠an ser tediosos y complicados. Afortunadamente, varias reglas se han desarrollado para encontrar Derivadas sin tener que usar la definici√≥n directamente. Estas f√≥rmulas simplifican enormemente la tarea de la diferenciaci√≥n y se conocen como reglas de derivaci√≥n. Algunas de ellas son las siguientes:\nFunciones comunes Funci√≥n original Funci√≥n Derivada Constantes \\(c\\) 0 \\(x\\) 1 Cuadrado \\(x^2\\) \\(2x\\) Raiz cuadrada \\(\\sqrt{x}\\) \\(\\frac{1}{2}x^{-\\frac{1}{2}}\\) Exponenciales \\(e^x\\) \\(e^x\\) \\(a^x\\) \\(a^x(\\ln a)\\) Logaritmicas \\(\\ln x\\) \\(\\frac{1}{x}\\) \\(\\log_{a} x\\) \\(\\frac{1}{x \\ln a}\\) Trigonom√©tricas \\(\\sin x\\) \\(\\cos x\\) \\(\\cos x\\) \\(-\\sin x\\) \\(\\tan x\\) \\(\\sec^2(x)\\) Trigonom√©tricas inversas \\(\\sin^{-1}(x)\\) \\(\\frac{1}{\\sqrt{1-x^2}}\\) \\(\\cos^{-1}(x)\\) \\(\\frac{-1}{\\sqrt{1-x^2}}\\) \\(\\tan^{-1}(x)\\) \\(\\frac{1}{1-x^2}\\) 1- Regla de la funci√≥n de grado n: Esta regla nos dice que una funci√≥n de grado n, donde n es un exponente real, se representa por \\(f(x)=x^{n}\\) y su derivada es \\(f\u0026rsquo;(x)=nx^{n-1}\\). As√≠ por ejemplo, si quisi√©ramos saber la derivada de \\(f(x) = x^5\\), aplicando la regla obtenemos, \\(f\u0026rsquo;(x) = 5x^{5-1} \\Rightarrow 5x^4\\).\n2- Regla de la multiplicaci√≥n por una constante: Esta regla establece que una funci√≥n con la forma \\(f(x) = Cx\\), donde \\(C\\) es una constante; entonces la derivada de esta funci√≥n va a ser igual a: \\(f\u0026rsquo;(x)= Cx\u0026rsquo;\\); es decir a la constante por la derivada de \\(x\\). As√≠ por ejemplo si tenemos la funci√≥n \\(f(x)=5x^3\\), primero debemos a obtener la derivada de \\(x^3\\), la cual aplicando la regla anterior sabemos que es \\(3x^2\\) y luego a esta derivada la multiplicamos por la constante 5, para obtener el resultado final \\(f\u0026rsquo;(x)=15x^2\\).\n3- Regla de la suma: Esta regla establece que la derivada de la suma de dos funciones es igual a la suma de las derivadas de cada una de ellas. Es decir, \\((f+g)\u0026rsquo;(x)=f\u0026rsquo;(x)+g\u0026rsquo;(x)\\). As√≠ por ejemplo la derivada de la funci√≥n \\(f(x) = 5x^3 + x^2\\) va a ser igual a \\(f\u0026rsquo;(x) = 15x^2 + 2x\\).\n4- Regla de la diferencia: Esta regla establece que la derivada de la diferencia entre dos funciones es igual a la diferencia entre las derivadas de cada una de ellas. Es decir, \\((f-g)\u0026rsquo;(x)=f\u0026rsquo;(x)-g\u0026rsquo;(x)\\). As√≠ por ejemplo la derivada de la funci√≥n \\(f(x) = 5x^3 - x^2\\) va a ser igual a \\(f\u0026rsquo;(x) = 15x^2 - 2x\\).\n5- Regla del producto: Esta regla establece que la derivada de un producto de dos funciones es equivalente a la suma entre el producto de la primera funci√≥n sin derivar y la derivada de la segunda funci√≥n y el producto de la derivada de la primera funci√≥n por la segunda funci√≥n sin derivar. Es decir, \\((f\\cdot g)\u0026rsquo; = f\u0026rsquo;\\cdot g + f\\cdot g\u0026rsquo;\\). As√≠ por ejemplo si quisi√©ramos derivar la funci√≥n \\(h(x)=(2x + 1)(x^3 + 2)\\), primero obtenemos las derivadas de cada termino, \\(f\u0026rsquo;(x)=2\\) y \\(g\u0026rsquo;(x)=3x^2\\) y luego aplicamos la formula \\(h\u0026rsquo;(x)=2(x^3 +2) + (2x + 1)3x^2\\), los que nos da un resultado final de \\(h\u0026rsquo;(x)=8x^3 + 3x^2 + 4\\).\n6- Regla del cociente: Esta regla establece que la derivada de un cociente de dos funciones es la funci√≥n ubicada en el denominador por la derivada del numerador menos la derivada de la funci√≥n en el denominador por la funci√≥n del numerador sin derivar, todo sobre la funci√≥n del denominador al cuadrado. Es decir, \\(\\left(\\frac{f}{g}\\right)\u0026rsquo;=\\frac{f\u0026rsquo;g-fg\u0026rsquo;}{g^{2}}\\). Por ejemplo, para obtener la derivada de la funci√≥n \\(h(x) = \\frac{3x + 1}{2x}\\), aplicando la formula obtenemos que \\(h\u0026rsquo;(x) = \\frac{3 \\cdot (2x) - (3x + 1) \\cdot 2}{2x^2}\\), y simplificando llegamos al resultado final de \\(h\u0026rsquo;(x) = -\\frac{1}{2x^2}\\).\n7- Regla de la cadena: La regla de la cadena es una f√≥rmula para calcular la derivada de la composici√≥n de dos o m√°s funciones. Esto es, si \\(f\\) y \\(g\\) son dos funciones, entonces la regla de la cadena expresa la derivada de la funci√≥n compuesta \\(f(g(x))\\) en t√©rminos de las derivadas de \\(f\\) y \\(g\\). Esta derivada va a ser calculada de acuerdo a la siguiente formula: \\(f\u0026rsquo;(g(x)) = f\u0026rsquo;(g(x)) \\cdot g\u0026rsquo;(x)\\). Por ejemplo, si quisi√©ramos saber la derivada de la funci√≥n \\(h(x) = \\sin(x^2)\\), aplicando la formula obtenemos que \\(h\u0026rsquo;(x) = \\cos(g(x)) \\cdot 2x\\), lo que es igual a \\(h\u0026rsquo;(x) = 2x \\cos(x^2)\\).\nDerivadas de mayor orden# Si tenemos una funci√≥n \\(f\\), de la cual podemos obtener su derivada \\(f\u0026rsquo;\\), la cual tambi√©n es otra funci√≥n que podemos derivar, entonces podemos obtener la derivada de segundo orden de \\(f\\), la cual representaremos como \\(f\u0026rsquo;\u0026rsquo;\\). Es decir, que la derivada de segundo orden de \\(f\\), va a ser igual a la derivada de su derivada. Siguiendo el mismo proceso, podemos seguir subiendo en la jerarqu√≠a y obtener por ejemplo, la tercer derivada de \\(f\\). Utilizando la notaci√≥n de Leibniz, expresar√≠amos a la segunda derivada del siguiente modo:\n$$\\frac{d}{dy}\\left(\\frac{dy}{dx}\\right)= \\frac{d^2y}{dx^2}$$ Calculando Derivadas con Python# Con Python, podemos resolver Derivadas utilizando nuevamente la librer√≠a SymPy. En este caso, ahora vamos a utilizar el objeto Derivative. Su sintaxis es la siguiente: Derivative(funcion, variable, orden de derivaci√≥n). Lo utilizamos de la siguiente forma:\nfrom sympy import Derivative, diff, simplify fx = (2*x + 1)*(x**3 + 2) dx = Derivative(fx, x).doit() dx $$2 x^{3} + 3 x^{2} \\left(2 x + 1\\right) + 4$$ # simplificando los resultados simplify(dx) $$8 x^{3} + 3 x^{2} + 4$$ # Derivada de segundo orden con el 3er argumento. Derivative(fx, x, 2).doit() $$6 x \\left(4 x + 1\\right)$$ # Calculando derivada de (3x +1) / (2x) fx = (3*x + 1) / (2*x) dx = Derivative(fx, x).doit() simplify(dx) $$- \\frac{1}{2 x^{2}}$$ # la funci√≥n diff nos da directamente el resultado simplify(diff(fx, x)) $$- \\frac{1}{2 x^{2}}$$ # con el metodo subs sustituimos el valor de x # para obtener el resultado num√©rico. Ej x = 1. diff(fx, x).subs(x, 1) $$- \\frac{1}{2}$$ Como podemos ver, el m√©todo para calcular las Derivadas con Python, es muy similar al que vimos anteriormente al calcular los L√≠mites. En el ejemplo, tambi√©n utilizamos la funci√≥n simplify, la cual nos ayuda a simplificar los resultados; y el m√©todo subs para sustituir el valor de \\(x\\) y obtener el resultado num√©rico.\nAhora que ya conocemos al C√°lculo diferencial, es tiempo de pasar hacia la otra rama del C√°lculo, el C√°lculo integral, y analizar el concepto de Integraci√≥n.\nIntegrales# La idea de Integral es el concepto b√°sico del C√°lculo integral. Pero para poder comprender este concepto, primero debemos abordar el problema del √°rea. Como bien sabemos, el √°rea es una medida de la extensi√≥n de una superficie. Determinar esta medida para superficies con l√≠neas rectas, suele ser bastante f√°cil. Por ejemplo para un rect√°ngulo, su √°rea se define como el producto de la longitud y el ancho. O para un tri√°ngulo como la mitad de la base por la altura. El √°rea de cualquier otro pol√≠gono se encuentra al dividirlo en tri√°ngulos y luego sumar las √°reas de cada uno ellos. Pero para los casos de las regiones con l√≠neas curvas, el c√°lculo del √°rea ya no suele ser tan f√°cil. Para estos casos debemos recurrir a un m√©todo similar al de exhauci√≥n que mencion√°bamos en la introducci√≥n del art√≠culo. Es decir, que vamos a ir dividiendo la regi√≥n en varios rect√°ngulos de \\(\\Delta x\\) de ancho y luego podemos ir calculando el √°rea como la suma del las √°reas de cada uno de estos rect√°ngulos. A medida que vamos agregando m√°s rect√°ngulos, haciendo \\(\\Delta x\\) cada vez m√°s peque√±o, nos vamos aproximando cada vez m√°s al valor real del √°rea de la superficie curva. Hasta el punto de que, cuando \\(\\Delta x\\) tiende a cero, podemos alcanzar el resultado exacto del √°rea de nuestra superficie curva. Es decir, que realizando una suma de infinitamente m√°s angostos rect√°ngulos, podemos determinar el resultado exacto del √°rea de nuestra superficie curva. Este proceso lo podemos ver m√°s claramente en la siguiente figura.\nComo vemos, al igual que pasaba con el caso de las Derivadas, al querer calcular el √°rea de una superficie curva, nos encontramos ante un caso especial de L√≠mite (aqu√≠ vemos tambi√©n por qu√© el concepto de L√≠mite es tan importante para el C√°lculo!). Este tipo de L√≠mite surge en una amplia variedad de situaciones, no solo al calcular √°reas, sino que tambi√©n lo podemos encontrar al calcular la distancia recorrida por un objeto o el volumen de un s√≥lido. Por lo tanto, se le ha dado una notaci√≥n y un nombre determinado. De esta forma la definici√≥n matem√°tica de la Integral definida, ser√≠a la siguiente:\nSi \\(f\\) es una funci√≥n definida por \\(a \\leqslant x \\leqslant b\\), podemos dividir el intervalo \\([a, b]\\) en \\(n\\) subintervalos de \\(\\Delta x(b - a) / n\\) de ancho. D√≥nde \\(x_0(=a), x_1, x_2, \\dots, x_n(=b)\\) ser√°n los puntos finales de estos subintervalos y \\(x_1^, x_2^, \\dots, x_n^\\), ser√°n puntos intermedios en estos subintervalos, de tal forma que \\(x_i^\\) se encuentre en el k-simo subintervalo \\([x_{i-1}, x_i]\\). Entonces la Integral definida de \\(f\\) entre \\(a\\) y \\(b\\), es:\n$$\\int_a^b f(x) dx = \\lim_{n \\to \\infty}\\sum_{i=1}^n f(x_i^*) \\Delta x $$ El s√≠mbolo de la Integral, \\(\\int\\), fue introducido por Leibniz, viene a ser una \u0026ldquo;S\u0026rdquo; alargada y fue elegido ya que la Integral es en definitiva un L√≠mite de sumas infinitesimales. En esta notaci√≥n, \\(a\\) y \\(b\\) son los l√≠mites de la integraci√≥n y \\(dx\\) indica que \\(x\\) es la variable independiente. La suma:\n$$\\sum_{i=1}^n f(x_i^*) \\Delta x $$ que vemos en la definici√≥n, es conocida como la suma de Reimann, en honor al matem√°tico alem√°n Bernhard Reimann que la desarroll√≥.\nIntegrales definidas e indefinidas# Una distinci√≥n importante que debemos hacer al hablar de Integrales, es la diferencia entre una Integral definida y una integral indefinida o antiderivada. Mientras que la Integral definida, que representamos con el s√≠mbolo, \\(\\int_a^b f(x) dx\\), es un n√∫mero, un resultado preciso de la medida de un √°rea, distancia o volumen; la integral indefinida, que representamos como, \\(\\int f(x) dx\\), es una funci√≥n o familia de funciones. M√°s adelante, cuando hablemos del teorema fundamental del c√°lculo, veremos por qu√© esta distinci√≥n es tan importante. Pero antes, veamos como podemos hacer para calcular Integrales.\nReglas de integraci√≥n# C√≥mo podemos ver de la definici√≥n que dimos de Integrales, estas parecen sumamente complicadas de calcular. Por suerte, al igual que para el caso de Derivadas, existen varias reglas que podemos utilizar para poder calcular las integrales indefinidas, en forma m√°s sencilla. Algunas de ellas son:\nFunciones comunes Funci√≥n original Integral indefinida (\\(C\\) es una constante) Constante \\(\\int a \\ dx\\) \\(ax + C\\) Variable \\(\\int x \\ dx\\) \\(\\frac{x^2}{2} + C\\) Cuadrado \\(\\int x^2 \\ dx\\) \\(\\frac{x^3}{3} + C\\) Reciproca \\(\\int \\left(\\frac{1}{x}\\right) \\ dx\\) \\(\\ln Exponenciales \\(\\int e^x \\ dx \\) \\(e^x + C\\) \\(\\int a^x \\ dx\\) \\(\\frac{a^x}{\\ln (a)} + C\\) \\(\\int \\ln (x) \\ dx\\) \\(x \\ \\ln(x) - x + C\\) Trigonom√©tricas \\(\\int \\sin (x) \\ dx\\) \\(- \\cos (x) + C\\) \\(\\int \\cos (x) \\ dx\\) \\(\\sin (x) + C\\) \\(\\int \\sec^2(x) \\ dx\\) \\(\\tan(x) + C\\) 1- Regla de la funci√≥n de grado n: Esta regla nos dice que una funci√≥n de grado n, donde n es un exponente real distinto de -1, se representa por \\(f(x)=x^{n}\\) y su integral es \\(\\int x^{n} \\ dx = \\frac{x^{n + 1}}{n + 1} + C\\). As√≠ por ejemplo, si quisi√©ramos saber la integral de \\(f(x) = x^3\\), aplicando la regla obtenemos, \\(\\int x^3 \\ dx = \\frac{x^4}{4} + C\\).\n2- Regla de la multiplicaci√≥n por una constante: Esta regla establece que una funci√≥n con la forma \\(f(x) = Cx\\), donde \\(C\\) es una constante; entonces la integral de esta funci√≥n va a ser igual a: \\(\\int Cx \\ dx = C\\int x \\ dx\\); es decir a la constante por la integral de \\(x\\). As√≠ por ejemplo si tenemos la funci√≥n \\(f(x)=4x^3\\), primero debemos a obtener la integral de \\(x^3\\), la cual aplicando la regla anterior sabemos que es \\(\\int x^3 \\ dx = \\frac{x^4}{4} + C \\) y luego a esta integral la multiplicamos por la constante 4, para obtener el resultado final \\(\\int 4 x^3 \\ dx = x^4 + C\\).\n3- Regla de la suma: Esta regla establece que la integral de la suma de dos funciones es igual a la suma de las integrales de cada una de ellas. Es decir, \\(\\int (f + g) \\ dx = \\int f \\ dx + \\int g \\ dx\\). As√≠ por ejemplo la integral de la funci√≥n \\(f(x) = 4x^3 + x^2\\) va a ser igual a \\(\\int (4x^3 + x^2) \\ dx = x^4 + \\frac{x^3}{3} + C\\).\n4- Regla de la diferencia: Esta regla establece que la integral de la diferencia entre dos funciones es igual a la diferencia entre las integrales de cada una de ellas. Es decir, \\(\\int (f - g) \\ dx = \\int f \\ dx - \\int g \\ dx\\). As√≠ por ejemplo la integral de la funci√≥n \\(f(x) = 4x^3 - x^2\\) va a ser igual a \\(\\int (4x^3 - x^2) \\ dx = x^4 - \\frac{x^3}{3} + C\\).\nEn todos estos ejemplos, podemos ver la aparici√≥n de una misteriosa constante \\(C\\), esta es la que se conoce como constante de integraci√≥n. Esta constante expresa una ambig√ºedad inherente a la construcci√≥n de las integrales. Es por esta ambig√ºedad que cuando hablamos de la integral indefinida decimos que expresa una familia de funciones \\(f(x) + C\\).\nTeorema fundamental del C√°lculo# El teorema fundamental del c√°lculo establece una conexi√≥n entre las dos ramas del C√°lculo: el C√°lculo diferencial y el C√°lculo integral. Como ya hemos visto, el C√°lculo diferencial surgi√≥ del problema de la tangente , mientras que el C√°lculo integral surgi√≥ de un problema aparentemente sin relaci√≥n con este, el problema del √°rea. Fue Isaac Barrow, quien descubri√≥ que estos dos problemas est√°n en realidad estrechamente relacionados. De hecho, se dio cuenta de que la derivaci√≥n y la integraci√≥n son procesos inversos. El teorema fundamental del c√°lculo nos da la relaci√≥n inversa precisa entre la Derivada y la Integral. Fueron Newton y Leibniz quienes aprovecharon esta relaci√≥n y la utilizaron para desarrollar el C√°lculo. En particular, vieron que esta relaci√≥n les permit√≠a calcular √°reas e Integrales con mucha facilidad y sin tener que calcularlas como l√≠mites de sumas. Es decir, que si tomamos una funci√≥n \\(f\\), y obtenemos primero su Derivada, y luego calculamos la Integral sobre esta funci√≥n Derivada $f'(x)$. Obtenemos nuevamente funci√≥n original \\(f\\). Lo que una hace, la otra lo deshace. El teorema fundamental del c√°lculo es sin duda el teorema m√°s importante en el C√°lculo y, de hecho, se ubica como uno de los grandes logros de la mente humana. Matem√°ticamente, este teorema se suele dividir en dos partes y nos dice lo siguiente:\nTeorema fundamental del Calculo, parte 1. si \\(f\\) es una funci√≥n continua en el intervalo \\([a, b]\\), entonces la funci√≥n \\(g\\) definida como:\n$$g(x) = \\int_a^x f(t) dt \\qquad a \\leqslant x \\leqslant b $$ es continua en el intervalo \\([a, b]\\) y diferenciable en \\((a, b)\\), y \\(g\u0026rsquo;(x) = f(x)\\).\nTeorema fundamental del Calculo, parte 2. si \\(f\\) es una funci√≥n continua en el intervalo \\([a, b]\\), entonces:\n$$\\int_a^b f(x) dx = F(b) - F(a) $$ en donde \\(f\\) es la antiderivada de \\(f\\), o sea, una funci√≥n tal que F\u0026rsquo; = f.\nEn definitiva, lo que nos dice la primera parte es que las operaciones de derivaci√≥n y de integraci√≥n son operaciones inversas. La segunda parte nos proporciona un m√©todo para calcular integrales definidas, en base a la antiderivada o integral indefinida.\nAs√≠, por ejemplo, si quisi√©ramos calcular la Integral:\n$$\\int_0^3 (x^3 - 6x) dx$$ primero obtenemos su integral indefinida.\n$$\\int (x^3 - 6x) dx = \\frac{x^4}{4} - 6\\frac{x^2}{2}$$ y por √∫ltimo aplicamos la segunda parte del teorema fundamental del c√°lculo para obtener la integral definida en \\([0, 3]\\), reemplazando estos valores en la integral indefinida que acabamos de obtener.\n$$\\int_0^3 (x^3 - 6x) dx = \\left(\\frac{1}{4} \\cdot 3^4 - 3 \\cdot 3^2 \\right) - \\left(\\frac{1}{4} \\cdot 0^4 - 3 \\cdot 0^2 \\right) = -\\frac{27}{4}$$ Calculando Integrales con Python# Con Python, podemos resolver Integrales con la ayuda de la, en este punto ya invaluable, librer√≠a SymPy. En este caso, vamos a utilizar el objeto Integral. Su sintaxis es la siguiente: Integral(funcion, variable). Lo utilizamos de la siguiente forma:\nfrom sympy import Integral, integrate fx = x**3 - 6*x dx = Integral(fx, x).doit() dx $$\\frac{x^{4}}{4} - 3 x^{2}$$ # la funci√≥n integrate nos da el mismo resultado integrate(fx, x) $$\\frac{x^{4}}{4} - 3 x^{2}$$ El objeto Integral tambi√©n nos permite calcular integrales definidas. En este caso, en el segundo argumento le pasamos una tupla cuyo primer elemento es la variable de integraci√≥n, su segundo elemento es el l√≠mite inferior de integraci√≥n y el √∫ltimo es el l√≠mite superior.\n# Calculando integral definida para [0, 3] Integral(fx, (x, 0, 3)).doit() $$- \\frac{27}{4}$$ # Comprobando Teorema fundamental del calculo. # Integraci√≥n y diferenciacion son operaciones inversas. diff(integrate(fx)) $$x^{3} - 6 x$$ integrate(diff(fx)) $$x^{3} - 6 x$$ Como podemos ver, el m√©todo para calcular las Integrales con Python, es muy similar a lo que ya ven√≠amos utilizando al calcular L√≠mites y Derivadas. Para calcular Integrales en forma num√©rica, tambi√©n podemos recurrir al m√≥dulo scipy.integrate, el cual es muy √∫til para resolver ecuaciones diferenciales, pero eso ya va a quedar para otro art√≠culo.\nCon esto concluyo esta introducci√≥n por el fascinante mundo del C√°lculo, espero lo hayan disfrutado tanto como yo!\nSaludos!\nEste post fue escrito utilizando Jupyter notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2015-12-02","id":30,"permalink":"/blog/2015/12/02/introduccion-al-calculo-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# El C√°lculo es una rama muy importante de la Matem√°tica moderna; tiene profundas ra√≠ces en problemas f√≠sicos y gran parte de su potencia y belleza derivan de la variedad de sus aplicaciones. Las subramas conocidas como C√°lculo integral y C√°lculo diferencial son instrumentos naturales y poderosos para atacar m√∫ltiples problemas que surgen en F√≠sica, Astronom√≠a, Ingenier√≠a, Qu√≠mica, Geolog√≠a, Biolog√≠a, y en otros campos de las ciencias.","tags":["python","matematica","calculo","derivada","integral","limite"],"title":"Introducci√≥n al C√°lculo con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\nLa l√≠nea consta de un n√∫mero infinito de puntos; el plano, de un n√∫mero infinito de l√≠neas; el volumen, de un n√∫mero infinito de planos; el hipervolumen, de un n√∫mero infinito de vol√∫menes\u0026hellip; No, decididamente no es √©ste\u0026hellip; el mejor modo de iniciar mi relato.\nJorge Luis Borges - El libro de arena\nIntroducci√≥n# El concepto de infinito ha obsesionado a la mente humana por miles de a√±os. La idea de que las cosas pueden seguir y seguir para siempre, que pueden no tener ni principio ni final, ni centro, ni l√≠mites; desaf√≠a a la intuici√≥n. Cient√≠ficos, fil√≥sofos y te√≥logos por igual, han tratado de entenderlo, de cortar su tama√±o, de averiguar su forma y dimensi√≥n; y en √∫ltima instancia, de decidir si es un concepto bienvenido en nuestras descripciones del Universo que nos rodea. A pesar de todas las dificultades que puede traer el lidiar con el infinito, √©ste sigue siendo un tema fascinante. Se encuentra en el coraz√≥n de todo tipo de preguntas fundamentales del hombre. ¬øSe puede vivir para siempre? ¬øEl universo tiene un final? ¬øTuvo un comienzo? ¬øTiene el universo un \u0026ldquo;borde\u0026rdquo; o simplemente no existen l√≠mites a su tama√±o? Aunque es f√°cil pensar en las listas de n√∫meros o secuencias de \u0026rsquo;tics\u0026rsquo; de un reloj que contin√∫an para siempre, hay otros tipos de infinito que parecen ser m√°s dif√≠ciles de concebir.\n¬øQu√© es el infinito?# Generalmente existe la tendencia de pensar en el infinito simplemente como un n√∫mero muy grande, s√≥lo un poco m√°s grande que el mayor n√∫mero que se pueda imaginar, siempre fuera de su alcance; pero el infinito no es un n√∫mero; sino que es un concepto, la idea de algo que no tiene fin.\nEl infinito en la matem√°ticas# Si bien el concepto de infinito parece ser un tema elusivo y complejo, aun as√≠ no a escapado a su estudio por parte de las matem√°ticas. Su estudio ya comenz√≥ en la antigua Grecia, quienes con teoremas tales como que la cantidad de n√∫meros primos no tiene l√≠mites, se enfrentaron ante la perspectiva de lo infinito. Arist√≥teles evit√≥ la actualidad del infinito mediante la definici√≥n de una infinidad potencial, de forma de permitir que √©stos teoremas contin√∫en como v√°lidos. Seg√∫n esta distinci√≥n hecha por Arist√≥teles entre el infinito actual y el infito potencial, los n√∫meros enteros son potencialmente infinitos porque siempre puede sumarse uno para conseguir un n√∫mero mayor, pero el conjunto infinito de n√∫meros enteros como tal, es decir el infinito actual de n√∫meros enteros, no existe. Seg√∫n los griegos el infinito actual nunca puede existir, sino que solo puede existir el infinito potencial; como reflejo del pensamiento griego, Arist√≥teles concluye que el infinito es imperfecto, inacabado e impensable. Esta definici√≥n de un infinito potencial y no real, funcion√≥ y satisfizo a los matem√°ticos y fil√≥sofos por casi dos milenios.\nNotaci√≥n del infinito# El s√≠mbolo \\(\\infty\\) que representa al infinito se lo debemos al matem√°tico ingles John Wallis, quien fue un precursor del c√°lculo infinitesimal e introdujo la notaci√≥n \\(\\infty\\), que representa una curva sin fin, en el siglo diecisiete.\nEn camino hacia un mejor entendimiento del infinito# A medida que se continuaba explorando el concepto del infinito, nuevas paradojas continuaban apareciendo. En la edad media se entend√≠a que un c√≠rculo m√°s grande deber√≠a contener m√°s puntos que un c√≠rculo m√°s peque√±o, sin embargo se puede encontrar una correspondencia de uno a uno entre los puntos de ambos c√≠rculos, como lo demuestra la siguiente figura:\nEn 1600, Galileo entendi√≥ que el problema era el estar utilizando un razonamiento finito sobre cosas infinitas. Postul√≥ que \u0026ldquo;Es un error hablar de cantidades infinitas como siendo la mayor o menor o igual que a la otra\u0026rdquo; y afirm√≥ que el infinito no es un noci√≥n inconsistente, sino que obedece a reglas diferentes.\nOtro de los temas que confund√≠an a los matem√°ticos, era el comportamiento de las series infinitas, sobre todo aquellas que no eran convergentes. Recordemos que una serie es convergente cuando la suma de sus t√©rminos da como resultado un n√∫mero finito; en caso contrario, se dice que la serie es divergente. As√≠ por ejemplo la serie geom√©trica, sabemos que converge en 1, y se puede demostrar geom√©tricamente:\n$$\\sum\\limits_{i=1}^\\infty\\frac{1}{2^n} = \\frac{1}{2} + \\frac{1}{4} + \\frac{1}{8} + \\frac{1}{16} + \\dots = 1$$ Pero en cambio el resultado de la siguiente serie no se puede determinar:\n$$ S = 1 - 1 + 1 - 1 + 1 - 1 + 1 - 1 + 1 \\dots $$ Si por ejemplo agrupamos los t√©rminos de la siguiente forma, el resultado parece ser obviamente cero.\n$$ S = (1 - 1) + (1 - 1) + (1 - 1) + (1 - 1) \\dots $$ Pero si los agrupamos de esta otra forma, el resultado parece ser ahora claramente uno.\n$$ S = 1 + (- 1 + 1) + (- 1 + 1) + (- 1 + 1) + (- 1 + 1) \\dots $$ y no termina ac√°, ya que incluso podr√≠amos agrupar la serie del siguiente modo y el resultado ser√≠a \\(\\frac{1}{2}\\).\n$$ S = 1 - ( 1 - 1 + 1 - 1 + 1 - 1 + 1 - \\dots) \\Rightarrow S = 1 - S \\Rightarrow 2S = 1 \\Rightarrow S = \\frac{1}{2} $$ es decir que el valor de la serie S, podr√≠a ser tanto 1, como 0, como \\(\\frac{1}{2}\\).No es sorprendente que argumentos como √©ste, hac√≠an que los matem√°ticos se pusieran muy nerviosos al tratar con los infinitos y quisieran evitarlos o incluso eliminarlos a toda costa.\nFuera de toda esta ambig√ºedad y confusi√≥n, la claridad surgi√≥ repentinamente en el siglo XIX, debido a los esfuerzos en solitario de un hombre brillante. Georg Cantor produjo una teor√≠a que respondi√≥ a todas las objeciones de sus predecesores y revel√≥ la riqueza inesperada escondida en el √°mbito de lo infinito. De repente, los infinitos actuales se convirtieron en parte de las matem√°ticas.\nCantor y la teor√≠a de conjuntos para explicar el infinito# Georg Cantor, aplicando ideas sumamente originales, postul√≥ la teor√≠a de conjuntos y utilizando el concepto de la cardinalidad de los conjuntos, se propuso explicar el concepto de infinito.\nSegun la teor√≠a de Cantor, dos conjuntos se dicen que tienen el mismo n√∫mero de elementos, es decir, que tienen la misma cardinalidad, si existe una funci√≥n definida entre ellos de forma tal que a cada elemento le corresponde s√≥lo otro elemento del otro conjunto, y viceversa; o sea, que exista una correspondencia de uno a uno entre los elementos de ambos conjuntos. A partir de esta definici√≥n se puede establecer la idea de conjunto infinito. Se dice que un conjunto es infinito si existe un subconjunto con la misma cardinalidad que √©l. Esta definici√≥n plantea una contradicci√≥n con la intuici√≥n, pues todo subconjunto como parte del conjunto total parece que deba tener menos elementos. Eso es as√≠, efectivamente, en los conjuntos finitos, pero no en los infinitos.\n¬øDistintos tipos de infinitos? Infinitos numerables y no numerables# Otro de los grandes aportes de Cantor, fue la distinci√≥n entre los infinitos numerables y no numerables. Cantor defini√≥ a los infinitos numerables como aquellos en los que se puede encontrar una correspondencia uno-a-uno con la lista de n√∫meros naturales 1, 2, 3, 4, 5, 6,. . . As√≠, por ejemplo, los n√∫meros pares son un infinito numerable, tambi√©n lo son todos los n√∫meros impares y ambos tienen el mismo tama√±o. Por ejemplo, la correspondencia entre los primeros n√∫meros impares ser√≠a la siguiente:\n$$ 1 \\longrightarrow 3 \\\\ 2 \\longrightarrow 5 \\\\ 3 \\longrightarrow 7 \\\\ 4 \\longrightarrow 9 \\\\ 5 \\longrightarrow 11 \\\\ 6 \\longrightarrow 13 \\\\ 7 \\longrightarrow 15 \\\\ 8 \\longrightarrow 17 \\\\ 9 \\longrightarrow 19 \\\\ 10 \\longrightarrow \\dots \\\\ $$ Por lo tanto, todos los conjuntos infinitos numerables tienen el mismo \u0026ldquo;tama√±o\u0026rdquo; o cardinalidad en el sentido de Cantor. El tambi√©n pensaba que √©stos eran los infinitos m√°s peque√±os que pudieran existir y por lo tanto los represent√≥ utilizando la primera letra del alfabeto hebreo, el s√≠mbolo Aleph-cero, \\(\\aleph_{0}\\). Un punto importante a tener en cuenta, es que esta definici√≥n excluye cualquier conjunto finito de objetos, ya que un conjunto finito s√≥lo puede ponerse en correspondencia uno-a-uno con otro conjunto que contenga el mismo n√∫mero de miembros. Este an√°lisis llev√≥ a algunas conclusiones sorprendentes. Cantor mostr√≥ que los n√∫meros racionales, los cuales se forman al dividir un n√∫mero entero por otro (por ejemplo \\(\\frac{1}{2}\\), o \\(\\frac{13}{2}\\)) son tambi√©n un conjunto infinito numerable. El truco fue encontrar un sistema para poder contarlos sin perderse ninguno. √âl utiliz√≥ el famoso proceso ahora conocido como la diagonal de Cantor para hacer esto. Fue contando cada uno de ellos de acuerdo al siguiente ordenamiento:\nCantor tambi√©n pudo demostrar con un nuevo tipo de argumento matem√°tico que existen infinitos m√°s grandes y que no se pueden contar. Esto son los que hoy en d√≠a se conocen como infinitos no numerables. Dentro de esta categor√≠a podemos encontrar al conjunto de los n√∫meros reales, los cuales incluyen a los n√∫meros irracionales, quienes no se pueden escribir como fracciones y tienen una expansi√≥n infinita de sus parte decimal.\nPara demostrar que los n√∫meros reales son un infinito no numerable, Cantor comenz√≥ por suponer que se pod√≠an contar, lo que significaba que deber√≠a ser capaz de elaborar una receta sistem√°tica para contar todos los decimales interminables que no terminaran en una cadena infinita de ceros. Los primeros de estos n√∫meros podr√≠an parecerse a √©stos:\n$$ 1 \\longrightarrow 0.\\underline{2}34567891\\dots \\\\ 2 \\longrightarrow 0.5\\underline{7}5603737\\dots \\\\ 3 \\longrightarrow 0.46\\underline{3}214516\\dots \\\\ 4 \\longrightarrow 0.846\\underline{2}16388\\dots \\\\ 5 \\longrightarrow 0.5621\\underline{9}4632\\dots \\\\ 6 \\longrightarrow 0.46673\\underline{2}271\\dots \\\\ \\dots $$ Luego cre√≥ un nuevo decimal tomando el primer d√≠gito despu√©s del punto decimal del primer n√∫mero, el segundo d√≠gito del segundo n√∫mero, y as√≠ sucesivamente para siempre(como por ejemplo los d√≠gitos subrayados en la serie de arriba). Siguiendo el ejemplo, el nuevo decimal comenzar√≠a como sigue:\n\\(0.273292\\dots\\)\nPor √∫ltimo a este decimal reci√©n creado, le sum√≥ 1 a cada uno de sus infinitos d√≠gitos. Obteniendo el n√∫mero\n\\(0.384303\\dots\\)\nEl problema es que este n√∫mero que acab√≥ creando no puede aparecer en la lista ordenada original de todos los decimales que hab√≠a asumido deb√≠a existir. Deber√° ser distinto a cada n√∫mero de la lista por lo menos en uno de sus d√≠gitos, ya que fue construido expresamente de ese modo. Por lo tanto, los n√∫meros reales (a veces llamados los n√∫meros decimales o el continuo de n√∫meros) no se pueden contar y son un infinito no numerable. As√≠ mismo, tambi√©n deben ser m√°s grande que los n√∫meros naturales o los n√∫meros racionales. Cantor represent√≥ a este nuevo tipo de infinito con el s√≠mbolo \\(\\aleph_{1}\\); ya que cre√≠a que no deb√≠a existir otro tipo de infinito que fuera mayor que el de los n√∫meros naturales y menor que el de los n√∫meros reales, aunque nunca fue capaz de demostrarlo. Esta es lo que se conoce como la hip√≥tisis del continuo.\nEste descubrimiento de Cantor, que existen infinitos de diferentes tama√±os y se pueden distinguir de una manera completamente inequ√≠voca, fue uno de los grandes descubrimientos de las matem√°ticas. Tambi√©n estaba completamente en contra de la opini√≥n dominante de la √©poca.\nUna jerarqu√≠a de infinitos# Adem√°s de todos los grandes aportes que Cantor realiz√≥ a la comprensi√≥n del infinito, su descubrimiento m√°s espectacular fue que los infinitos no s√≥lo son incontables, sino que son insuperables!. Descubri√≥ que debe existir una jerarqu√≠a ascendente interminable de infinitos. No hay uno m√°s grande que todos que pueda contener a todos ellos. No hay un universo de universos que podemos anotar y capturar. Cantor fue capaz de demostrar que existe una jerarqu√≠a ascendente de infinitos sin fin, una jerarqu√≠a infinita de infinitos!. Para demostrar este punto, utiliz√≥ el concepto de conjunto potencia.\nRecordemos que un conjunto potencia de un conjunto A, expresado por \\(P_{A}\\), es el conjunto formado por todos los distintos subconjuntos de A. As√≠ por ejemplo el conjunto potencia del conjunto \\(A={1,2,3}\\); va a ser igual a \\(P_{A}={\\emptyset,{1},{2},{3},{1,2},{2,3}, {1,3},{1,2,3}}\\). Un teorema importante de la teor√≠a de conjuntos establece que si A es un conjunto con k elementos, es decir que n(A) = k; entonces el conjunto potencia de A tiene exactamente \\(2^k\\) elementos. En nuestro ejemplo anterior podemos ver que n(A)=3, por lo tanto \\(n(P_{A}) = 2^3\\), lo que es igual a los 8 elementos que vimos que tiene el conjunto potencia de A.\nEntonces, si se toma cualquier conjunto infinito, siempre es posible generar una que es infinitamente m√°s grande considerando a su conjunto potencia, es decir, el conjunto que contiene todos sus subconjuntos. As√≠, de un conjunto infinito como \\(\\aleph_{0}\\) podemos crear un conjunto infinitamente m√°s grande mediante la formaci√≥n de su conjunto potencia, \\(P_{\\aleph_{0}}\\). Luego podemos hacer lo mismo otra vez, formando el conjunto potencia de \\(P_{\\aleph_{0}}\\), el cual ser√° infinitamente m√°s grande que \\(P_{\\aleph_{0}}\\). Y as√≠ sucesivamente, sin fin. De esta forma las matem√°ticas crean una jerarqu√≠a sin fin de infinitos ascendentes . El infinito nunca puede ser capturado por las f√≥rmulas. Tambi√©n muestra que el n√∫mero de posibles verdades es infinito.\nHotel infinito# Una de las im√°genes que se suele utilizar para ilustrar el concepto de infinito es el relato del hotel infinito ideado por el gran matem√°tico David Hilber, el cual explica de manera simple e intuitiva, las paradojas relacionadas con el concepto del infinito.\nEl hotel infinito es un hotel como cualquier otro, con la √∫nica excepci√≥n de que cuenta con un n√∫mero infinito de habitaciones!\nTan pronto se abrieron las puertas de este hotel la gente comenz√≥ a abarrotarlo y pronto se encontraron con que el hotel de habitaciones infinitas se encontraba lleno de infinitos hu√©spedes. En ese momento surgi√≥ la primera paradoja, as√≠ que se tom√≥ como medida que los hu√©spedes siempre tendr√≠an habitaci√≥n asegurada pero con el acuerdo previo de que tendr√≠an que cambiar de habitaci√≥n cada vez que se les pidiera. Fue entonces cuando lleg√≥ un nuevo huesped al hotel que ya se encontraba lleno. El hombre pidi√≥ su habitaci√≥n y el recepcionista, consciente de que no habr√≠a ning√∫n problema, tom√≥ un micr√≥fono por el que avis√≥ a todos los hu√©spedes que por favor revisaran el n√∫mero de su habitaci√≥n, le sumaran uno y se cambiaran a ese n√∫mero de habitaci√≥n, de esta manera el nuevo hu√©sped pudo dormir tranquilamente en la habitaci√≥n n√∫mero 1. Pero, ¬øqu√© pas√≥ entonces con el hu√©sped que se encontraba en la √∫ltima habitaci√≥n? Sencillamente no hay √∫ltima habitaci√≥n ya que \\(\\infty + 1\\) sigue siendo \\(\\infty\\).\nLa segunda paradoja surgi√≥ cuando lleg√≥ un representante de una agencia de viajes solicitando hospedar a un infinito n√∫mero de turistas en el hotel de infinitas habitaciones, las cuales ya estaban todas ellas ocupadas. Pero el recepcionista nuevamente no tuvo ning√∫n problema, tom√≥ el micr√≥fono y pidi√≥ a todos los hu√©spedes que se mudaran a la habitaci√≥n correspondiente al resultado de multiplicar por 2 el n√∫mero de su habitaci√≥n actual. De esa forma todos los hu√©spedes se mudaron a una habitaci√≥n par, y todas las habitaciones impares quedaron libres. Como hay infinitos n√∫meros impares, los infinitos turistas pudieron alojarse sin m√°s problemas.\nLa tercer paradoja se present√≥ cuando lleg√≥ otro representante de la agencia de viajes a√∫n m√°s preocupado que el primero y avis√≥ que ahora la agencia ten√≠a un infinito n√∫mero de excursiones con un infinito n√∫mero de turistas cada una. ¬øc√≥mo podr√≠an hospedar a un n√∫mero infinito de infinitos turistas en un hotel que ya se encontraba lleno? El recepcionista permaneci√≥ inmutable, tom√≥ tranquilamente el micr√≥fono y se comunic√≥ solamente con las habitaciones cuyo n√∫mero fuera primo o alguna potencia de √©stos (\\(p^n\\)), les pidi√≥ que elevaran el n√∫mero 2 al n√∫mero de la habitaci√≥n en la que se encontraban (\\(2^{p^n}\\)) y se cambiaran a esa habitaci√≥n. Entonces asign√≥ a cada una de las excursiones un n√∫mero primo (distinto de 2), a cada uno de los turistas de cada una de las excursiones un n√∫mero impar (t), de manera que la habitaci√≥n de cada uno de los turistas, se calculaba tomando el n√∫mero primo de su excursi√≥n (p) y elevandolo al n√∫mero que les toc√≥ dentro de su excursi√≥n (t) lo que da \\(p^t\\). Existiendo un n√∫mero infinito de n√∫meros primos y un n√∫mero infinito de n√∫meros impares, f√°cilmente se logr√≥ hospedar a un n√∫mero infinito de infinitos hu√©spedes dentro del hotel infinito.\nLas paradojas de Zen√≥n y el concepto de l√≠mite# Como venimos viendo, el concepto de infinito esta repleto de paradojas; y algunas de las m√°s famosas de ellas son tambi√©n de las m√°s antiguas. Se las debemos a Zen√≥n de Elea, quien fue disc√≠pulo del fil√≥sofo Parm√©nides y por lo tanto sosten√≠a, al igual que su maestro, que el universo era intemporal e inmutable y que por lo tanto el movimiento era solo una ilusi√≥n. Para sostener su tesis, Zen√≥n de Elea propuso una serie de argumentos que hoy en d√≠a se conocen como las paradojas de Zen√≥n.\nLa primera paradoja pretende demostrar que el movimiento es imposible, porque si quisi√©ramos caminar de un punto a otro, primero deber√≠amos cruzar la mitad de la distancia, luego la mitad de la distancia restante, luego la mitad del resto, y as√≠ sucesivamente. O sea, que si los dos puntos est√°n a un kil√≥metro de distancia, en primer lugar debemos llegar a \\(\\frac{1}{2}\\) km desde su inicio, a continuaci√≥n, \\(\\frac{3}{4}\\) km desde su inicio, luego a \\(\\frac{7}{8}\\) km y as√≠ sucesivamente. Seg√∫n el argumento de Zen√≥n s√≥lo se podr√≠a llegar si se toma un n√∫mero infinito de pasos; y como los griegos rechazaban el infinito, por lo tanto deber√≠amos rechazar el movimiento tambi√©n y concluir que este no es posible.\nEn su segunda paradoja, Zen√≥n crea el escenario de una carrera entre el famoso atleta Aquiles y un rival mucho m√°s lento, una tortuga. Aquiles comienza en la posici√≥n 0, mientras que la tortuga, que s√≥lo puede correr a la mitad de la velocidad de Aquiles, tiene una ventaja inicial de un kil√≥metro. Ambos comienzan a correr al mismo tiempo. De acuerdo al planteamiento de este escenario, podr√≠amos pensar que Aquiles, al correr el doble de r√°pido que la tortuga, la superar√≠a en la marca de 2 kil√≥metros. Sin embargo, cuando Aquiles llega a la marca de 1 km, la tortuga ya ha avanzado a \\(1 + \\frac{1}{2}\\) km; cuando Aquiles llega al punto \\(1\\frac{1}{2}\\) km, la tortuga ha alcanzado \\(1 + \\frac{1}{2} + \\frac{1}{4}\\) km; y as√≠ sucesivamente. Cuando, despu√©s de N pasos, Aquiles alcanza una distancia de \\(2 - \\frac{1}{2}^{N-1}\\) desde el punto inicial, la tortuga se encuentra todav√≠a en la delantera, ya que est√° a una distancia de \\(2 - \\frac{1}{2}^{N+1}\\). No importa lo grande que sea N (el n√∫mero de divisiones del viaje), Aquiles nunca se adelanta a la tortuga!\nResolviendo las paradojas# Para poder resolver estas paradojas, debemos recurrir al c√°lculo infinitesimal, y m√°s particularmente al concepto de l√≠mite. El l√≠mite de una sucesi√≥n es la noci√≥n intuitiva de que la sucesi√≥n se aproxima arbitrariamente a un √∫nico punto o valor. Es decir, que la sucesi√≥n va a converger en un valor determinado. En el caso de las dos paradojas de Zen√≥n, nos encontramos ante series geom√©tricas, las cuales ya vimos que tienen un l√≠mite y convergen en un valor finito. Por tanto se puede demostrar que Aquiles realmente alcanzar√° a la tortuga. Los tiempos en los que Aquiles recorre la distancia que lo separa del punto anterior en el que se encontraba la tortuga son cada vez m√°s y m√°s peque√±os (hasta el infinito m√°s peque√±os), y su suma da un resultado finito, que es el momento en que alcanzar√° a la tortuga.\nL√≠mites al infinito# Como pudimos ver a lo largo de todo el art√≠culo, el infinito es un concepto muy especial. Sabemos que no podemos llegar a el, pero todav√≠a podemos tratar de averiguar el valor de las funciones o sucesiones que tratan con el infinito. Por ejemplo, si quisi√©ramos saber cual es el resultado de \\(\\frac{1}{\\infty}\\), deber√≠amos responder que no existe soluci√≥n, ya que el infinito no es un n√∫mero, y por lo tanto no podr√≠amos determinar el resultado de esa operaci√≥n. Sin embargo, a pesar de que no podemos trabajar con el infinito en s√≠ mismo, podemos aproximarnos bastante a el y ver que sucede a medida que vamos dividiendo a uno por valores cada vez m√°s grandes. Por ejemplo, si reemplazamos al \\(\\infty\\), por \\(x\\), tendr√≠amos una ecuaci√≥n a la que podr√≠amos ir aplicando valores cada vez m√°s grandes de \\(x\\) y ver a que resultado llegamos. Ayud√°ndonos de nuestro buen amigo Python podr√≠amos ver los resultados f√°cilmente y comprobar que a medida que \\(x\\) se hace cada vez m√°s grande, el valor de la funci√≥n \\(\\frac{1}{x}\\) se aproxima cada vez m√°s a cero.\nVer C√≥digo # \u0026lt;!-- collapse=True --\u0026gt; %matplotlib inline import matplotlib.pyplot as plt import numpy as np import pandas as pd def f(x): return x**-1.0 # 1/x == x**-1 x = np.array([1, 2, 4, 10, 100, 1000, 10000, 100000, 1000000 ]) y = f(x) # Construyendo tabla de valores x e y con pandas tabla = pd.DataFrame(x, columns=[\u0026#39;x\u0026#39;]) tabla[\u0026#39;1/x\u0026#39;] = y tabla x 1/x 0 1 1.000000 1 2 0.500000 2 4 0.250000 3 10 0.100000 4 100 0.010000 5 1000 0.001000 6 10000 0.000100 7 100000 0.000010 8 1000000 0.000001 # \u0026lt;!-- collapse=True --\u0026gt; # Graficando 1/x x = np.arange(1, 1000) # Rango de valores de x plt.figure(figsize=(8,6)) plt.title(r\u0026#34;Graficando $\\frac{1}{x}$\u0026#34;) plt.xlabel(\u0026#39;x\u0026#39;) plt.ylabel(r\u0026#34;$f(x) = \\frac{1}{x}$\u0026#34;) plt.plot(x, f(x), label=r\u0026#34;$\\frac{1}{x}$ tiende a cero a medida que x crece\u0026#34;) plt.ylim([0, .12]) plt.legend() plt.show() Como nos muestran tanto los c√°lculos como el gr√°fico, a medida que vamos aumentando el valor de \\(x\\), la funci√≥n \\(\\frac{1}{x}\\) se va aproximando cada vez m√°s a cero. Por tanto podemos conjeturar que la funci√≥n \\(\\frac{1}{x}\\) tiende a cero. Si bien, no podemos decir que pasa en el infinito, si podemos decir que pasa a medida que nos vamos acercando cada vez m√°s a √©l. Por tanto podr√≠amos decir que a medida que el valor \\(x\\) tiende a \\(\\infty\\), el valor de la funci√≥n \\(\\frac{1}{x}\\), se aproxima a su l√≠mite cero. En el lenguaje de las matem√°ticas, lo expresar√≠amos de la siguiente forma:\n$$\\lim_{x\\to \\infty} \\left(\\frac{1}{x}\\right) = 0$$ El concepto de L√≠mite es una de las herramientas principales sobre las que se sustenta el c√°lculo. Muchas veces, una funci√≥n puede ser indefinida en un punto, pero podemos pensar en lo que pasa a medida que la funci√≥n \u0026ldquo;se acerca\u0026rdquo; cada vez m√°s a ese punto. Otras veces, la funci√≥n puede ser definida en un punto, pero puede aproximarse a un l√≠mite diferente.\nCon esto termina este art√≠culo, para concluir cerremos la curva sin fin que representa al infinito con otra frase de Borges, que fue con quien comenzamos el art√≠culo!\nHay un concepto que es el corruptor y el desatinador de los otros. No hablo del Mal cuyo limitado imperio es la √©tica; hablo del infinito.\nJorge Luis Borges - Avatares de la tortuga\nEspero les haya parecido interesante y no se hayan sentido tan desconcertados al toparse con el concepto de infinito como la probe serpiente pit√≥n de la cabecera del art√≠culo!.\nSaludos!\nEste post fue escrito utilizando IPython notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2015-11-22","id":31,"permalink":"/blog/2015/11/22/hasta-el-infinito-y-mas-alla/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Matem√°ticas, an√°lisis de datos y python. El contenido esta bajo la licencia BSD.\nLa l√≠nea consta de un n√∫mero infinito de puntos; el plano, de un n√∫mero infinito de l√≠neas; el volumen, de un n√∫mero infinito de planos; el hipervolumen, de un n√∫mero infinito de vol√∫menes\u0026hellip; No, decididamente no es √©ste\u0026hellip; el mejor modo de iniciar mi relato.","tags":["python","matematica","limites","infinito"],"title":"Hasta el infinito y m√°s alla"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Si bien el conjunto de los n√∫meros reales \\(\\mathbb{R}\\), parece contener todos los n√∫meros que podr√≠amos llegar a necesitar. Existe todav√≠a una dificultad, el hecho de que s√≥lo se pueden tomar ra√≠ces cuadradas de los n√∫meros positivos (o cero) y no de los negativos. Desde el punto de vista matem√°tico resultar√≠a conveniente poder extraer ra√≠ces cuadradas de n√∫meros negativos tanto como de n√∫meros positivos. Por tal motivo, que tal si \u0026ldquo;inventamos\u0026rdquo; una ra√≠z cuadrada para el n√∫mero -1 y la expresamos con el s√≠mbolo \u0026ldquo;\\(i\\)\u0026rdquo;, de modo que tenemos\n$$i^2 = -1$$ La cantidad \\(i\\) no puede ser, por supuesto, un n√∫mero real puesto que el producto de un n√∫mero real por s√≠ mismo es siempre positivo (o cero, si el propio n√∫mero es el cero). Por esta raz√≥n se ha aplicado convencionalmente el t√©rmino \u0026ldquo;imaginario\u0026rdquo; a los n√∫meros cuyos cuadrados son negativos. Sin embargo, es importante resaltar el hecho de que estos n√∫meros imaginarios no son menos reales que los n√∫meros reales a los que estamos acostumbrados.\nCuando estos n√∫meros imaginarios se combinan con los n√∫meros reales obtenemos lo que se conoce como n√∫meros complejos; de esta forma, los n√∫meros complejos vienen a completar a los n√∫meros reales y nos permiten realizar todo tipo de operaciones algebraicas.\n¬øQu√© es un n√∫mero complejo?# Los n√∫meros complejos incluyen todas las ra√≠ces de los polinomios, a diferencia de lo que pasaba cuando solo ten√≠amos a los n√∫meros reales. Todo n√∫mero complejo puede representarse como la suma de un n√∫mero real y un n√∫mero imaginario, el cual es un m√∫ltiplo real de la unidad imaginaria, que se indica con la letra \\(i\\). O sea, que los n√∫meros complejos tienen la forma\n$$a + bi$$ donde a y b son n√∫meros reales llamados parte real y parte imaginaria , respectivamente, del n√∫mero complejo. Las reglas para sumar y multiplicar tales n√∫meros se siguen de las reglas ordinarias del √°lgebra, con la regla a√±adida de que \\(i^2 = -1\\). Veamos las distintas operaciones matem√°ticas que podemos hacer con estos n√∫meros.\nOperaciones con n√∫meros complejos# Las operaciones que podemos realizar con los n√∫meros complejos son las siguientes:\nSuma# Para sumar dos n√∫meros complejos simplemente sumamos cada elemento en forma separada. Es decir que:\n$$(a+bi) + (c+di) = (a+c) + (b+d)i$$ As√≠, por ejemplo \\((2 + 2i) + (1 + 5i) = 3 + 7i\\).\nProducto por escalar# Para calcular el producto escalar de un n√∫mero complejo, multiplicamos al escalar por cada una de sus partes, la real y la imaginaria. Es decir que:\n$$r(a+bi) = (ra) + (rb)i$$ As√≠, por ejemplo \\(3(2 + 3i) = 6 + 9i\\).\nMultiplicaci√≥n# Para multiplicar dos n√∫meros complejos, debemos realizar su multiplicaci√≥n binomial. Es decir que:\n$$(a+bi)(c+di) = ac + adi + bci + bdi^2$$ En este punto, debemos recordad que \\(i^2\\) es igual a -1; lo que nos facilita la soluci√≥n del c√°lculo de la multiplicaci√≥n. Tambi√©n existe una formula m√°s simple para obtener el resultado de la multiplicaci√≥n de dos n√∫meros complejos, que es:\n$$(a+bi)(c+di) = (ac-bd) + (ad+bc)i$$ As√≠, por ejemplo \\((3 + 2i)(2 + 6i) = (3\\times2 - 2\\times6) + (3\\times6 + 2\\times2)i = -6 + 22i\\).\nIgualdad# Dos n√∫meros complejos van a ser iguales si y solo si sus partes reales e imaginarias son iguales. Es decir que:\n$$(a + bi) = (c + di) \\iff a = c \\wedge b = d$$ As√≠, por ejemplo \\((3 + 2i) = (3 + 2i)\\), ya que 3 = 3 y 2 = 2.\nResta# La resta de dos n√∫meros complejos, funciona de forma similar a la suma.\n$$(a+bi) - (c+di) = (a-c) + (b-d)i$$ As√≠, por ejemplo \\((2 + 2i) - (1 + 5i) = 1 -3i\\).\nConjugado# El conjugado de un n√∫mero complejo se obtiene cambiando el signo de su componente imaginario. Por lo tanto, el conjugado de un n√∫mero complejo\n\\(z = a + bi\\), es \\(\\overline{z} = a - bi\\).\nPara expresar que estamos buscando el conjugado, escribimos una l√≠nea sobre el n√∫mero complejo. As√≠, por ejemplo\n$$ \\overline{ 2 + 3i} = 2 - 3i$$ Divisi√≥n# Para dividir dos n√∫meros complejos, debemos utilizar el conjugado; ya que para realizar la divisi√≥n debemos multiplicar tanto el divisor como el dividendo por el conjugado del divisor. As√≠, por ejemplo si quisi√©ramos dividir:\n$$\\frac{2 + 3i}{4 - 5i}$$ Debemos realizar el siguiente c√°lculo:\n$$\\frac{2 + 3i}{4 - 5i}\\times\\frac{4 + 5i}{4 + 5i}$$ y teniendo en cuenta que la multiplicaci√≥n de un n√∫mero complejo por su conjugado, responde a la formula:\n$$(a + bi)(a - bi) = a^2 + b^2$$ Podemos resolver la divisi√≥n de la siguiente forma:\n$$\\frac{2 + 3i}{4 - 5i}\\times\\frac{4 + 5i}{4 + 5i} = \\frac{8 + 10i + 12i + 15i^2}{16 + 25} = \\frac{-7 + 22i}{41}$$ Lo que nos lleva al resultado final:\n$$ = \\frac{-7}{41} + \\frac{22}{41}i$$ Para simplificar el procedimiento, y no tener que realizar tantos c√°lculos, podr√≠amos utilizar la siguiente formula:\n$$\\frac{(a + bi)}{(c + di)} = {(ac+bd) + (bc-ad)i \\over c^2+d^2} = \\left({ac + bd \\over c^2 + d^2} + {(bc - ad)i \\over c^2 + d^2}\\right)$$ Valor absoluto o m√≥dulo de un n√∫mero complejo# El valor absoluto, m√≥dulo o magnitud de un n√∫mero complejo viene dado por la siguiente expresi√≥n:\n$$ |a + bi| = \\sqrt{a^2 + b^2} $$ As√≠, por ejemplo \\(|4 + 3i| = \\sqrt{16 + 9} = 5\\).\nPlano de los n√∫meros complejos o Diagrama de Argand# El plano de Argand es un plano eucl√≠deo ordinario con coordenadas cartesianas est√°ndar \\(x\\) e \\(x\\) , donde \\(x\\) indica la distancia horizontal (positiva hacia la derecha y negativa hacia la izquierda) y donde \\(x\\) indica la distancia vertical (positiva hacia arriba y negativa hacia abajo). El n√∫mero complejo \\(z = x + yi\\) viene representado entonces por el punto del plano de Argand cuyas coordenadas son \\(( x , y )\\).\nN√≥tese que 0 (considerado como un n√∫mero complejo) viene representado por el origen de coordenadas, y 1 viene representado por un punto en el eje x. El plano de Argand proporciona simplemente un modo de organizar nuestra familia de n√∫meros complejos en una imagen geom√©tricamente √∫til. Las operaciones algebraicas b√°sicas de la suma y multiplicaci√≥n de n√∫meros complejos encuentran ahora una forma geom√©trica clara. Consideremos por ejemplo la suma. Supongamos que \\(u\\) y \\(v\\) son dos n√∫meros complejos representados en el plano de Argand de acuerdo con el esquema anterior. Entonces su suma \\(u + v\\) viene representada como la suma vectorial de los dos puntos; es decir, el punto \\(u + v\\) est√° en el lugar que completa el paralelogramo formado por \\(u, v\\) y el origen 0.\nN√∫meros complejos en Python# Python trae soporte por defecto para los n√∫meros complejos, d√≥nde la parte imaginaria va a estar representada por la letra j en lugar de utilizar la i como en la notaci√≥n matem√°tica. Veamos algunos ejemplos de las cosas que podemos hacer con ellos.\n# Creando un n√∫mero complejo c1 = 4 + 3j c1 (4+3j) # Verificando el tipo de dato type(c1) complex # Creando un n√∫mero complejo con complex c2 = complex(2, -3) c2 (2-3j) # sumando dos n√∫meros complejos c1 = 2 + 2j c2 = 1 + 5j c1 + c2 (3+7j) # Restando dos n√∫meros complejos c1 - c2 (1-3j) # Producto escalar c1 = 2 + 3j 3 * c1 (6+9j) # Multiplicando dos n√∫meros complejos c1 = 3 + 2j c2 = 2 + 6j c1 * c2 (-6+22j) # Igualdad ente n√∫meros complejos c2 = 3 + 2j c1 == c2 True # Conjugado de un n√∫mero complejo c1 = 2 + 3j c1.conjugate() (2-3j) # Divisi√≥n de n√∫meros complejos c1 = 1 + 1j c2 = -1 + 1j c1 / c2 (-0-1j) # Valor absoluto o magnitud c1 = 4 + 3j abs(c1) 5.0 # Parte real c1.real 4.0 # Parte imaginaria c1.imag 3.0 # Grafico en plano de argand # Graficos embebidos %matplotlib inline import matplotlib.pyplot as plt def move_spines(): \u0026#34;\u0026#34;\u0026#34;Crea la figura de pyplot y los ejes. Mueve las lineas de la izquierda y de abajo para que se intersecten con el origen. Elimina las lineas de la derecha y la de arriba. Devuelve los ejes.\u0026#34;\u0026#34;\u0026#34; fix, ax = plt.subplots() for spine in [\u0026#34;left\u0026#34;, \u0026#34;bottom\u0026#34;]: ax.spines[spine].set_position(\u0026#34;zero\u0026#34;) for spine in [\u0026#34;right\u0026#34;, \u0026#34;top\u0026#34;]: ax.spines[spine].set_color(\u0026#34;none\u0026#34;) return ax ax = move_spines() ax.set_xlim(-5, 5) ax.set_ylim(-5, 5) ax.grid() ax.scatter(c1.real, c1.imag) plt.title(\u0026#34;Plano de Argand\u0026#34;) plt.show() Aplicaciones de los n√∫meros complejos# Dado que los n√∫meros complejos proporcionan un sistema para encontrar las ra√≠ces de polinomios y los polinomios se utilizan como modelos te√≥ricos en diversos campos, los n√∫meros complejos gozan de un gran importancia en varias √°reas especializadas. Entre estas √°reas especializadas se encuentran la ingenier√≠a, la ingenier√≠a el√©ctrica y la mec√°nica cu√°ntica. Algunos temas en los que se utilizan n√∫meros complejos incluyen la investigaci√≥n de la corriente el√©ctrica, longitud de onda, el flujo de l√≠quido en relaci√≥n a los obst√°culos, el an√°lisis de la tensi√≥n en las vigas, el movimiento de los amortiguadores en autom√≥viles, el estudio de resonancia de las estructuras, el dise√±o de dinamos y motores el√©ctricos, y la manipulaci√≥n de grandes matrices utilizadas en el modelado. Por ejemplo, en ingenier√≠a electrica para el an√°lisis de circuitos de corriente alterna, es necesario representar cantidades multidimensionales. Con el fin de realizar esta tarea, los n√∫meros escalares fueron abandonados y los n√∫meros complejos se utilizan para expresar las dos dimensiones de frecuencia y desplazamiento de fase.\nAs√≠, por ejemplo si sabemos que el voltaje en un circuito es 45 + 10j voltios y la impedancia es de 3 + 4j ohms. Si queremos saber cual es la corriente, simplemente deber√≠amos resolver la ecuaci√≥n \\(E = I \\dot Z\\) donde E es la tensi√≥n, I es la corriente, y Z es la impedancia. Reemplazando los t√©rminos en la formula, obtenemos que:\n$$ I = \\frac{45 + 10j}{3 + 4j}$$ # Calculando I I = (45 + 10j) / (3 + 4j) I (7-6j) Por tanto, la corriente es de 7 - 6j amps.\nAdem√°s de todas estas aplicaciones, los n√∫meros complejos nos permiten tambi√©n realizar uno de los gr√°ficos m√°s hermosos de las matem√°ticas como es el fractal de Julia!!.\n# importando librer√≠as necesarias import numpy as np import numba # Graficando el fractal de Julia def py_julia_fractal(z_re, z_im, j): \u0026#39;\u0026#39;\u0026#39;Crea el grafico del fractal de Julia.\u0026#39;\u0026#39;\u0026#39; for m in range(len(z_re)): for n in range(len(z_im)): z = z_re[m] + 1j * z_im[n] for t in range(256): z = z ** 2 - 0.05 + 0.68j if np.abs(z) \u0026gt; 2.0: j[m, n] = t break jit_julia_fractal = numba.jit(nopython=True)(py_julia_fractal) N = 1024 j = np.zeros((N, N), np.int64) z_real = np.linspace(-1.5, 1.5, N) z_imag = np.linspace(-1.5, 1.5, N) jit_julia_fractal(z_real, z_imag, j) fig, ax = plt.subplots(figsize=(8, 8)) ax.imshow(j, cmap=plt.cm.RdBu_r, extent=[-1.5, 1.5, -1.5, 1.5]) ax.set_xlabel(\u0026#34;$\\mathrm{Re}(z)$\u0026#34;, fontsize=18) ax.set_ylabel(\u0026#34;$\\mathrm{Im}(z)$\u0026#34;, fontsize=18) plt.show() O el tambi√©n famoso conjunto de Mandelbrot.\n# Graficando el conjunto de Mandelbrot def mandelbrot( h,w, maxit=20 ): \u0026#39;\u0026#39;\u0026#39;Crea el grafico del fractal de Mandelbrot del tama√±o (h,w).\u0026#39;\u0026#39;\u0026#39; y,x = np.ogrid[ -1.4:1.4:h*1j, -2:0.8:w*1j ] c = x+y*1j z = c divtime = maxit + np.zeros(z.shape, dtype=int) for i in range(maxit): z = z**2 + c diverge = z*np.conj(z) \u0026gt; 2**2 div_now = diverge \u0026amp; (divtime==maxit) divtime[div_now] = i z[diverge] = 2 return divtime plt.figure(figsize=(8,8)) plt.imshow(mandelbrot(2000,2000)) plt.show() Con esto termino este art√≠culo, espero que les haya gustado y les sea de utilidad.\nSaludos!\nEste post fue escrito utilizando IPython notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2015-10-12","id":32,"permalink":"/blog/2015/10/12/numeros-complejos-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Si bien el conjunto de los n√∫meros reales \\(\\mathbb{R}\\), parece contener todos los n√∫meros que podr√≠amos llegar a necesitar. Existe todav√≠a una dificultad, el hecho de que s√≥lo se pueden tomar ra√≠ces cuadradas de los n√∫meros positivos (o cero) y no de los negativos.","tags":["python","matematica","complejos","programacion"],"title":"N√∫meros complejos con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Una caracter√≠stica notable de los seres humanos es su inherente necesidad y capacidad de agrupar objetos de acuerdo a criterios espec√≠ficos. La idea de la clasificaci√≥n de ciertos objetos en grupos similares, o conjuntos, es uno de los conceptos m√°s fundamentales de la matem√°tica moderna. La teor√≠a de conjuntos ha sido el marco unificador para todas las matem√°ticas desde que el matem√°tico alem√°n Georg Cantor la formulara alrededor de 1870. Ning√∫n campo de las matem√°ticas podr√≠a describirse hoy en d√≠a sin hacer referencia a alg√∫n tipo de conjunto abstracto. En t√©rminos m√°s generales, el concepto de membres√≠a de un conjunto, que se encuentra en el coraz√≥n de la teor√≠a de conjuntos, explica c√≥mo sentencias con sustantivos y predicados son formulados en nuestro lenguaje, o en cualquier lenguaje abstracto como las matem√°ticas. Debido a esto, la teor√≠a de conjuntos est√° √≠ntimamente ligada a la l√≥gica y sirve de base para todas las matem√°ticas.\n¬øQu√© es un conjunto?# Un conjunto es una colecci√≥n de objetos distintos, a menudo llamados elementos o miembros. Existen dos caracter√≠sticas hacen de los conjuntos algo totalmente distinto a cualquier otra colecci√≥n de objetos. En primer lugar, un conjunto est√° siempre \u0026ldquo;bien definido\u0026rdquo;, es decir que si realizamos la pregunta ¬øEste objeto particular, se encuentra en esta colecci√≥n?; siempre debe existir una respuesta clara por s√≠ o por no basada en una regla o algunos criterios dados. La segunda caracter√≠stica, es que no hay dos miembros de un mismo conjunto que sean exactamente iguales, es decir, que no hay elementos repetidos. Un conjunto puede contener cualquier cosa imaginable, incluyendo n√∫meros, letras, colores, incluso otros conjuntos!. Sin embargo, ninguno de los objetos del conjunto puede ser el propio conjunto. Descartamos esta posibilidad para evitar encontrarnos con la Paradoja de Russell, un problema famoso en la l√≥gica matem√°tica desenterrado por el gran l√≥gico brit√°nico Bertrand Russell en 1901.\nNotaci√≥n de Conjuntos# Cuando escribimos a los conjuntos utilizamos letras may√∫sculas para sus nombres y para representar al conjunto propiamente dicho simplemente listamos sus elementos separ√°ndolos por comas y luego englobamos todos estos elementos dentro de un par de llaves. As√≠, por ejemplo, A = {1,2,3, \u0026hellip;, 10} es el conjunto de los 10 primeros n√∫meros naturales o para contar, B = {Rojo, Azul, Verde} es el conjunto de colores primarios, N = {1,2,3, \u0026hellip;} es el conjunto de todos los n√∫meros naturales, y Z = {\u0026hellip;, - 3, -2, -1,0,1,2,3, \u0026hellip;} es el conjunto de todos los n√∫meros enteros. Los puntos suspensivos \u0026ldquo;\u0026hellip;\u0026rdquo; se utilizan para describir el car√°cter infinito de los n√∫meros en los conjuntos N y Z.\nTambi√©n se utiliza el s√≠mbolo \\( \\in \\) para expresar que determina objeto pertenece o es miembro de un conjunto y el s√≠mbolo \\( \\notin \\) para indicar que no pertenece a un conjunto. Utilizando los ejemplos anteriores, podr√≠amos por ejemplo escribir que \\(7 \\in A\\) y \\(12 \\notin A\\).\nDado que muchos conjuntos no se pueden describir listando todos sus miembros, ya que en muchos casos esto es imposible, tambi√©n se utiliza la mucho m√°s potente notaci√≥n de constructor de conjuntos o predicado. En esta notaci√≥n escribimos el conjunto de acuerdo a qu√© tipos de objetos pertenecen al conjunto, que se colocan a la izquierda del s√≠mbolo \u0026ldquo;|\u0026rdquo;, que significa \u0026ldquo;de tal manera que,\u0026rdquo; dentro de las llaves; as√≠ como las condiciones que estos objetos deben cumplir para pertenecer al conjunto, las cuales se colocan a la derecha de \u0026ldquo;|\u0026rdquo; dentro de las llaves. Por ejemplo, el conjunto de los n√∫meros racionales, o fracciones, que se denota por Q no puede ser descrito por el m√©todo de listar todos sus miembros. En su lugar, se define a Q utilizando la notaci√≥n de predicado de la siguiente manera: \\(Q={\\frac{p}{q} \\mid p, q \\in Z\\) y \\(q \\ne 0 }\\) Esto se lee \u0026ldquo;Q es el conjunto de todas las fracciones de la forma p sobre q, tal que p y q son n√∫meros enteros y q no es cero.\u0026rdquo; Tambi√©n podr√≠amos escribir al conjunto A de nuestro ejemplo anterior como \\(A = {x \\mid x \\in N\\) y \\(x \u0026lt; 11}\\).\nConjuntos num√©ricos# Dentro de las matem√°ticas, los principales conjuntos num√©ricos que podemos encontrar y que tienen un car√°cter universal son:\n\\(\\mathbb{N} = {1,2,3, \u0026hellip;}\\) es el conjunto de los n√∫meros naturales. \\(\\mathbb{W} = {0,1,2,3, \u0026hellip;}\\) es el conjunto de los n√∫meros enteros positivos. \\(\\mathbb{Z} = {\u0026hellip;,-3,-2,-1,0,1,2,3, \u0026hellip;}\\) es el conjunto de todos los n√∫meros enteros. \\(\\mathbb{Q} ={\\frac{p}{q} \\mid p, q \\in Z\\) y \\(q \\ne 0 }\\) es el conjunto de los n√∫meros racionales. \\(\\mathbb{R}\\), es el conjunto de los n√∫meros reales. Estos son todos los n√∫meros que pueden ser colocados en una recta num√©rica unidimensional que se extiende sin fin tanto en negativo como positivo. \\(\\mathbb{I}\\), es el conjunto de los n√∫meros irracionales. Algunos de los n√∫meros m√°s importantes en matem√°ticas pertenecen a este conjunto,incluyendo \\(\\pi, \\sqrt{2}, e\\) y \\(\\phi\\). \\(\\mathbb{C}\\), es el conjunto de los n√∫meros complejos. Estos son los n√∫meros que contienen una parte real y otra parte imaginaria. Igualdad entre conjuntos# El concepto de igualdad en los conjuntos, difiere levemente del cl√°sico concepto de igualdad que solemos tener. Dos conjuntos A y B se dice que son iguales (expresado por A = B), si y s√≥lo si ambos conjuntos tienen exactamente los mismos elementos. Por ejemplo el conjunto A={1,2,3,4} es igual al conjunto B={4,3,2,1}. Un conjunto importante, y que todav√≠a no hemos mencionado es el conjunto vac√≠o, el cual no tiene elementos y por tanto no puede ser igualado con ning√∫n otro conjunto. Se expresa con el s√≠mbolo \\(\\emptyset\\) o {}.\nCardinalidad# La cardinalidad de un conjunto A es el n√∫mero de elementos que pertenecen a A y lo expresamos como n(A). La cardinalidad de un conjunto puede ser pensada tambien como una medida de su \u0026ldquo;tama√±o\u0026rdquo;. Si la cardinalidad de un conjunto es un n√∫mero entero, entonces el conjunto se dice que es finito. De lo contrario, el conjunto se dice que es infinito. As√≠ por ejemplo la cardinalidad del conjunto A={1,2,\u0026hellip;,9,10} es 10 y lo expresamos como n(A)=10.\nSubconjunto y subconjunto propio# Si todos los elementos de un conjunto A son tambi√©n elementos de otro conjunto B, entonces A se llama un subconjunto de B y lo expresamos como \\(A \\subseteq B\\). En cierto sentido, se puede pensar al subconjunto A como dentro, o contenido en el conjunto B. Si un conjunto A es un subconjunto de B y los dos conjuntos no son iguales, entonces llamamos A un subconjunto propio de B y lo expresamos como \\(A \\subset B\\). En este caso, se dice que el conjunto A esta propiamente contenido en B. Algunas propiedades importantes relacionadas con subconjuntos y subconjuntos propios son las siguientes:\nCualquier conjunto A es un subconjunto de s√≠ mismo. Por lo tanto \\(A \\subseteq A\\). Esto es claramente cierto. Menos obvio es el hecho de que el conjunto vac√≠o es un subconjunto de cualquier conjunto A. Por lo tanto \\(\\emptyset \\subseteq A\\). Esta propiedad se prueba a trav√©s de la contradicci√≥n, ya que si asumimos que existe un conjunto A del que el conjunto vac√≠o no es un subconjunto, entonces esto quiere decir que el conjunto vac√≠o debe contener un elemento que no se encuentra en A y esto es absurdo ya que el conjunto vac√≠o no contiene ning√∫n elemento. El conjunto vac√≠o es un subconjunto propio de cualquier conjunto A, siempre y cuando A no se tambi√©n un conjunto vac√≠o. Para los conjuntos finitos A y B, si \\(A \\subseteq B\\), entonces \\(n(A) \\leq n(B)\\). De forma similar, para los conjuntos finitos A y B, si \\(A \\subset B\\), entonces \\(n(A) \u0026lt; n(B)\\). Conjunto potencia# El conjunto potencia de un conjunto A, expresado por \\(P_{A}\\), es el conjunto formado por todos los distintos subconjuntos de A. As√≠ por ejemplo el conjunto potencia del conjunto \\(A={1,2,3}\\); va a ser igual a \\(P_{A}={\\emptyset,{1},{2},{3},{1,2},{2,3}, {1,3},{1,2,3}}\\).\nUn teorema importante de la teor√≠a de conjuntos establece que si A es un conjunto con k elementos, es decir que n(A) = k; entonces el conjunto potencia de A tiene exactamente \\(2^k\\) elementos. Escribimos esto como \\(n(P_{A}) = 2^k\\). En nuestro ejemplo anterior podemos ver que n(A)=3, por lo tanto \\(n(P_{A}) = 2^3\\), lo que es igual a los 8 elementos que vimos que tiene el conjunto potencia de A.\nAlgebra de conjuntos# El √°lgebra de conjuntos es el estudio de las operaciones b√°sicas que podemos realizar con los conjuntos. Las operaciones b√°sicas del √°lgebra de conjuntos son:\nUni√≥n. La uni√≥n de dos conjuntos A y B es el conjunto \\(A \\cup B\\) que contiene todos los elementos de A y de B. Intersecci√≥n. La intersecci√≥n de dos conjuntos A y B es el conjunto \\(A \\cap B\\) que contiene todos los elementos comunes de A y B. Diferencia. La diferencia entre dos conjuntos A y B es el conjunto \\(A \\setminus B\\) que contiene todos los elementos de A que no pertenecen a B. Complemento. El complemento de un conjunto A es el conjunto \\(A^‚àÅ\\) que contiene todos los elementos que no pertenecen a A. Producto cartesiano. El producto cartesiano de dos conjuntos A y B es el conjunto \\(A \\times B\\) que contiene todos los pares ordenados (a, b) cuyo primer elemento pertenece a A y su segundo elemento pertenece a B.\nConjuntos con Python# Luego de todo este repaso por los fundamentos de la teor√≠a de conjuntos, es tiempo de ver como podemos utilizar a los conjuntos dentro de Python; ya que el lenguaje trae como una de sus estructuras de datos por defecto a los conjuntos. Tambi√©n veremos que podemos utilizar el constructor FiniteSet que nos proporciona sympy, el cual tiene ciertas ventajas sobre la versi√≥n por defecto de Python.\n# Creando un conjunto en python A = {1,2,3} A {1, 2, 3} # Creando un conjunto a partir de una lista lista = [\u0026#34;bananas\u0026#34;, \u0026#34;manzanas\u0026#34;, \u0026#34;naranjas\u0026#34;, \u0026#34;limones\u0026#34;] B = set(lista) B {'bananas', 'limones', 'manzanas', 'naranjas'} # Los conjuntos eliminan los elementos duplicados lista = [\u0026#34;bananas\u0026#34;, \u0026#34;manzanas\u0026#34;, \u0026#34;naranjas\u0026#34;, \u0026#34;limones\u0026#34;, \u0026#34;bananas\u0026#34;, \u0026#34;bananas\u0026#34;, \u0026#34;limones\u0026#34;, \u0026#34;naranjas\u0026#34;] B = set(lista) B {'bananas', 'limones', 'manzanas', 'naranjas'} # Creando el conjunto vac√≠o O = set() O set() # Cardinalidad de un conjunto con len(). print(\u0026#34;La cardinalidad del conjunto A = {0} es {1}\u0026#34;.format(A,len(A))) La cardinalidad del conjunto A = {1, 2, 3} es 3 # comprobando membres√≠a 2 in A True # Igualdad entre conjuntos. El orden de los elementos no importa. A = {1,2,3,4} B = {4,2,3,1} A == B True # Subconjunto. No hay distincion entre subconjunto y propio # para el conjunto por defecto de python. A = {1,2,3} B = {1,2,3,4,5} A.issubset(B) True # Subconjunto propio A.issubset(B) and A != B True # Union de conjuntos A = {1,2,3,4,5} B = {4,5,6,7,8,9,10} A.union(B) {1, 2, 3, 4, 5, 6, 7, 8, 9, 10} # Intersecci√≥n de conjuntos A.intersection(B) {4, 5} # Diferencia entre conjuntos A - B {1, 2, 3} B - A {6, 7, 8, 9, 10} # Utilizando FiniteSet de sympy from sympy import FiniteSet C = FiniteSet(1, 2, 3) C {1, 2, 3} # Generando el conjunto potencia. Esto no se puede # hacer utilizando el conjunto por defecto de python. C.powerset() {EmptySet(), {1}, {2}, {3}, {1, 2}, {1, 3}, {2, 3}, {1, 2, 3}} # Cardinalidad print(\u0026#34;La cardinalidad del conjunto potencia del conjunto C = {0} es {1}\u0026#34;. format(C, len(C.powerset()))) La cardinalidad del conjunto potencia del conjunto C = {1, 2, 3} es 8 # Igualdad A = FiniteSet(1, 2, 3) B = FiniteSet(1, 3, 2) A == B True A = FiniteSet(1, 2, 3) B = FiniteSet(1, 3, 4) A == B False # Subconjunto y subconjunto propio A = FiniteSet(1,2,3) B = FiniteSet(1,2,3,4,5) A.is_subset(B) True A.is_proper_subset(B) True # A == B. El test de subconjunto propio da falso B = FiniteSet(2,1,3) A.is_proper_subset(B) False # Union de dos conjuntos A = FiniteSet(1, 2, 3) B = FiniteSet(2, 4, 6) A.union(B) {1, 2, 3, 4, 6} # Interseccion de dos conjuntos A = FiniteSet(1, 2) B = FiniteSet(2, 3) A.intersect(B) {2} # Diferencia entre conjuntos A - B {1} # Calculando el producto cartesiano. Con el conjunto por # defecto de python no podemos hacer esto con el operador * A = FiniteSet(1, 2) B = FiniteSet(3, 4) P = A * B P {1, 2} x {3, 4} for elem in P: print(elem) (1, 3) (1, 4) (2, 3) (2, 4) # Elevar a la n potencia un conjunto. Calcula el n # producto cartesiano del mismo conjunto. A = FiniteSet(1, 2, 3, 4) P2 = A ** 2 P2 {1, 2, 3, 4} x {1, 2, 3, 4} P3 = A ** 3 P3 {1, 2, 3, 4} x {1, 2, 3, 4} x {1, 2, 3, 4} for elem in P3: print(elem) (1, 1, 1) (1, 1, 2) (1, 1, 3) (1, 1, 4) (1, 2, 1) (1, 2, 2) (1, 2, 3) (1, 2, 4) (1, 3, 1) (1, 3, 2) (1, 3, 3) (1, 3, 4) (1, 4, 1) (1, 4, 2) (1, 4, 3) (1, 4, 4) (2, 1, 1) (2, 1, 2) (2, 1, 3) (2, 1, 4) (2, 2, 1) (2, 2, 2) (2, 2, 3) (2, 2, 4) (2, 3, 1) (2, 3, 2) (2, 3, 3) (2, 3, 4) (2, 4, 1) (2, 4, 2) (2, 4, 3) (2, 4, 4) (3, 1, 1) (3, 1, 2) (3, 1, 3) (3, 1, 4) (3, 2, 1) (3, 2, 2) (3, 2, 3) (3, 2, 4) (3, 3, 1) (3, 3, 2) (3, 3, 3) (3, 3, 4) (3, 4, 1) (3, 4, 2) (3, 4, 3) (3, 4, 4) (4, 1, 1) (4, 1, 2) (4, 1, 3) (4, 1, 4) (4, 2, 1) (4, 2, 2) (4, 2, 3) (4, 2, 4) (4, 3, 1) (4, 3, 2) (4, 3, 3) (4, 3, 4) (4, 4, 1) (4, 4, 2) (4, 4, 3) (4, 4, 4) # graficos embebidos %matplotlib inline # Dibujanto el diagrama de venn de 2 conjuntos from matplotlib_venn import venn2, venn2_circles import matplotlib.pyplot as plt A = FiniteSet(1, 3, 5, 7, 9, 11, 13, 15, 17, 19) B = FiniteSet(2, 3, 5, 7, 11, 13, 17, 19, 8) plt.figure(figsize=(6, 8)) v = venn2(subsets=[A, B], set_labels=(\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;)) v.get_label_by_id(\u0026#39;10\u0026#39;).set_text(A - B) v.get_label_by_id(\u0026#39;11\u0026#39;).set_text(A.intersection(B)) v.get_label_by_id(\u0026#39;01\u0026#39;).set_text(B - A) c = venn2_circles(subsets=[A, B], linestyle=\u0026#39;dashed\u0026#39;) c[0].set_ls(\u0026#39;solid\u0026#39;) plt.show() Adem√°s de las aplicaciones que pueden tener los conjuntos de Python en matem√°ticas, los mismos tambi√©n pueden ser una estructura de datos poderosa y ayudarnos a resolver varios problemas de programaci√≥n en forma muy sencilla. A tenerlos en cuenta!\nCon esto termino este art√≠culo; espero que les haya gustado y les sea de utilidad.\nSaludos!\nEste post fue escrito utilizando IPython notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2015-10-11","id":33,"permalink":"/blog/2015/10/11/conjuntos-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Una caracter√≠stica notable de los seres humanos es su inherente necesidad y capacidad de agrupar objetos de acuerdo a criterios espec√≠ficos. La idea de la clasificaci√≥n de ciertos objetos en grupos similares, o conjuntos, es uno de los conceptos m√°s fundamentales de la matem√°tica moderna.","tags":["python","matematica","conjuntos","programacion"],"title":"Conjuntos con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nUna de las ramas de estudio que cada vez esta ganando m√°s popularidad dentro de las ciencias de la computaci√≥n es el aprendizaje autom√°tico o Machine Learning. Muchos de los servicios que utilizamos en nuestro d√≠a a d√≠a como google, gmail, netflix, spotify o amazon se valen de las herramientas que les brinda el Machine Learning para alcanzar un servicio cada vez m√°s personalizado y lograr as√≠ ventajas competitivas sobre sus rivales.\nQu√© es Machine Learning?# Pero, ¬øqu√© es exactamente Machine Learning?. El Machine Learning es el dise√±o y estudio de las herramientas inform√°ticas que utilizan la experiencia pasada para tomar decisiones futuras; es el estudio de programas que pueden aprenden de los datos. El objetivo fundamental del Machine Learning es generalizar, o inducir una regla desconocida a partir de ejemplos donde esa regla es aplicada. El ejemplo m√°s t√≠pico donde podemos ver el uso del Machine Learning es en el filtrado de los correo basura o spam. Mediante la observaci√≥n de miles de correos electr√≥nicos que han sido marcados previamente como basura, los filtros de spam aprenden a clasificar los mensajes nuevos. El Machine Learning combina conceptos y t√©cnicas de diferentes √°reas del conocimiento, como las matem√°ticas, estad√≠sticas y las ciencias de la computaci√≥n; por tal motivo, hay muchas maneras de aprender la disciplina.\nTipos de Machine Learning# El Machine Learning tiene una amplia gama de aplicaciones, incluyendo motores de b√∫squeda, diagn√≥sticos m√©dicos, detecci√≥n de fraude en el uso de tarjetas de cr√©dito, an√°lisis del mercado de valores, clasificaci√≥n de secuencias de ADN, reconocimiento del habla y del lenguaje escrito, juegos y rob√≥tica. Pero para poder abordar cada uno de estos temas es crucial en primer lugar distingir los distintos tipos de problemas de Machine Learning con los que nos podemos encontrar.\nAprendizaje supervisado# En los problemas de aprendizaje supervisado se ense√±a o entrena al algoritmo a partir de datos que ya vienen etiquetados con la respuesta correcta. Cuanto mayor es el conjunto de datos m√°s el algoritmo puede aprender sobre el tema. Una vez conclu√≠do el entrenamiento, se le brindan nuevos datos, ya sin las etiquetas de las respuestas correctas, y el algoritmo de aprendizaje utiliza la experiencia pasada que adquiri√≥ durante la etapa de entrenamiento para predecir un resultado. Esto es similar al m√©todo de aprendizaje que se utiliza en las escuelas, donde se nos ense√±an problemas y las formas de resolverlos, para que luego podamos aplicar los mismos m√©todos en situaciones similares.\nAprendizaje no supervisado# En los problemas de aprendizaje no supervisado el algoritmo es entrenado usando un conjunto de datos que no tiene ninguna etiqueta; en este caso, nunca se le dice al algoritmo lo que representan los datos. La idea es que el algoritmo pueda encontrar por si solo patrones que ayuden a entender el conjunto de datos. El aprendizaje no supervisado es similar al m√©todo que utilizamos para aprender a hablar cuando somos bebes, en un principio escuchamos hablar a nuestros padres y no entendemos nada; pero a medida que vamos escuchando miles de conversaciones, nuestro cerebro comenzar√° a formar un modelo sobre c√≥mo funciona el lenguaje y comenzaremos a reconocer patrones y a esperar ciertos sonidos.\nAprendizaje por refuerzo# En los problemas de aprendizaje por refuerzo, el algoritmo aprende observando el mundo que le rodea. Su informaci√≥n de entrada es el feedback o retroalimentaci√≥n que obtiene del mundo exterior como respuesta a sus acciones. Por lo tanto, el sistema aprende a base de ensayo-error. Un buen ejemplo de este tipo de aprendizaje lo podemos encontrar en los juegos, donde vamos probando nuevas estrategias y vamos seleccionando y perfeccionando aquellas que nos ayudan a ganar el juego. A medida que vamos adquiriendo m√°s practica, el efecto acumulativo del refuerzo a nuestras acciones victoriosas terminar√° creando una estrategia ganadora.\nSobreentrenamiento# Como mencionamos cuando definimos al Machine Learning, la idea fundamental es encontrar patrones que podamos generalizar para luego poder aplicar esta generalizaci√≥n sobre los casos que todav√≠a no hemos observado y realizar predicciones. Pero tambi√©n puede ocurrir que durante el entrenamiento solo descubramos casualidades en los datos que se parecen a patrones interesantes, pero que no generalicen. Esto es lo que se conoce con el nombre de sobreentrenamiento o sobreajuste.\nEl sobreentrenamiento es la tendencia que tienen la mayor√≠a de los algoritmos de Machine Learning a ajustarse a unas caracter√≠sticas muy espec√≠ficas de los datos de entrenamiento que no tienen relaci√≥n causal con la funci√≥n objetivo que estamos buscando para generalizar. El ejemplo m√°s extremo de un modelo sobreentrenado es un modelo que solo memoriza las respuestas correctas; este modelo al ser utilizado con datos que nunca antes ha visto va a tener un rendimiento azaroso, ya que nunca logr√≥ generalizar un patr√≥n para predecir.\nComo evitar el sobreentrenamiento# Como mencionamos anteriormente, todos los modelos de Machine Learning tienen tendencia al sobreentrenamiento; es por esto que debemos aprender a convivir con el mismo y tratar de tomar medidas preventivas para reducirlo lo m√°s posible. Las dos principales estrategias para lidiar son el sobreentrenamiento son: la retenci√≥n de datos y la validaci√≥n cruzada.\nEn el primer caso, la idea es dividir nuestro conjunto de datos, en uno o varios conjuntos de entrenamiento y otro/s conjuntos de evaluaci√≥n. Es decir, que no le vamos a pasar todos nuestros datos al algoritmo durante el entrenamiento, sino que vamos a retener una parte de los datos de entrenamiento para realizar una evaluaci√≥n de la efectividad del modelo. Con esto lo que buscamos es evitar que los mismos datos que usamos para entrenar sean los mismos que utilizamos para evaluar. De esta forma vamos a poder analizar con m√°s precisi√≥n como el modelo se va comportando a medida que m√°s lo vamos entrenando y poder detectar el punto cr√≠tico en el que el modelo deja de generalizar y comienza a sobreajustarse a los datos de entrenamiento.\nLa validaci√≥n cruzada es un procedimiento m√°s sofisticado que el anterior. En lugar de solo obtener una simple estimaci√≥n de la efectividad de la generalizaci√≥n; la idea es realizar un an√°lisis estad√≠stico para obtener otras medidas del rendimiento estimado, como la media y la varianza, y as√≠ poder entender c√≥mo se espera que el rendimiento var√≠e a trav√©s de los distintos conjuntos de datos. Esta variaci√≥n es fundamental para la evaluaci√≥n de la confianza en la estimaci√≥n del rendimiento. La validaci√≥n cruzada tambi√©n hace un mejor uso de un conjunto de datos limitado; ya que a diferencia de la simple divisi√≥n de los datos en uno el entrenamiento y otro de evaluaci√≥n; la validaci√≥n cruzada calcula sus estimaciones sobre todo el conjunto de datos mediante la realizaci√≥n de m√∫ltiples divisiones e intercambios sistem√°ticos entre datos de entrenamiento y datos de evaluaci√≥n.\nPasos para construir un modelo de machine learning# Construir un modelo de Machine Learning, no se reduce solo a utilizar un algoritmo de aprendizaje o utilizar una librer√≠a de Machine Learning; sino que es todo un proceso que suele involucrar los siguientes pasos:\nRecolectar los datos. Podemos recolectar los datos desde muchas fuentes, podemos por ejemplo extraer los datos de un sitio web o obtener los datos utilizando una API o desde una base de datos. Podemos tambi√©n utilizar otros dispositivos que recolectan los datos por nosotros; o utilizar datos que son de dominio p√∫blico. El n√∫mero de opciones que tenemos para recolectar datos no tiene fin!. Este paso parece obvio, pero es uno de los que m√°s complicaciones trae y m√°s tiempo consume.\nPreprocesar los datos. Una vez que tenemos los datos, tenemos que asegurarnos que tiene el formato correcto para nutrir nuestro algoritmo de aprendizaje. Es pr√°cticamente inevitable tener que realizar varias tareas de preprocesamiento antes de poder utilizar los datos. Igualmente este punto suele ser mucho m√°s sencillo que el paso anterior.\nExplorar los datos. Una vez que ya tenemos los datos y est√°n con el formato correcto, podemos realizar un pre an√°lisis para corregir los casos de valores faltantes o intentar encontrar a simple vista alg√∫n patr√≥n en los mismos que nos facilite la construcci√≥n del modelo. En esta etapa suelen ser de mucha utilidad las medidas estad√≠sticas y los gr√°ficos en 2 y 3 dimensiones para tener una idea visual de como se comportan nuestros datos. En este punto podemos detectar valores at√≠picos que debamos descartar; o encontrar las caracter√≠sticas que m√°s influencia tienen para realizar una predicci√≥n.\nEntrenar el algoritmo. Aqu√≠ es donde comenzamos a utilizar las t√©cnicas de Machine Learning realmente. En esta etapa nutrimos al o los algoritmos de aprendizaje con los datos que venimos procesando en las etapas anteriores. La idea es que los algoritmos puedan extraer informaci√≥n √∫til de los datos que le pasamos para luego poder hacer predicciones.\nEvaluar el algoritmo. En esta etapa ponemos a prueba la informaci√≥n o conocimiento que el algoritmo obtuvo del entrenamiento del paso anterior. Evaluamos que tan preciso es el algoritmo en sus predicciones y si no estamos muy conforme con su rendimiento, podemos volver a la etapa anterior y continuar entrenando el algoritmo cambiando algunos par√°metros hasta lograr un rendimiento aceptable.\nUtilizar el modelo. En esta ultima etapa, ya ponemos a nuestro modelo a enfrentarse al problema real. Aqu√≠ tambi√©n podemos medir su rendimiento, lo que tal vez nos obligue a revisar todos los pasos anteriores.\nLibrer√≠as de Python para machine learning# Como siempre me gusta comentar, una de las grandes ventajas que ofrece Python sobre otros lenguajes de programaci√≥n; es lo grande y prolifera que es la comunidad de desarrolladores que lo rodean; comunidad que ha contribuido con una gran variedad de librer√≠as de primer nivel que extienden la funcionalidades del lenguaje. Para el caso de Machine Learning, las principales librer√≠as que podemos utilizar son:\nScikit-Learn# Scikit-learn es la principal librer√≠a que existe para trabajar con Machine Learning, incluye la implementaci√≥n de un gran n√∫mero de algoritmos de aprendizaje. La podemos utilizar para clasificaciones, extraccion de caracter√≠sticas, regresiones, agrupaciones, reducci√≥n de dimensiones, selecci√≥n de modelos, o preprocesamiento. Posee una API que es consistente en todos los modelos y se integra muy bien con el resto de los paquetes cient√≠ficos que ofrece Python. Esta librer√≠a tambi√©n nos facilita las tareas de evaluaci√≥n, diagnostico y validaciones cruzadas ya que nos proporciona varios m√©todos de f√°brica para poder realizar estas tareas en forma muy simple.\nStatsmodels# Statsmodels es otra gran librer√≠a que hace foco en modelos estad√≠sticos y se utiliza principalmente para an√°lisis predictivos y exploratorios. Al igual que Scikit-learn, tambi√©n se integra muy bien con el resto de los paquetes cientificos de Python. Si deseamos ajustar modelos lineales, hacer una an√°lisis estad√≠stico, o tal vez un poco de modelado predictivo, entonces Statsmodels es la librer√≠a ideal. Las pruebas estad√≠sticas que ofrece son bastante amplias y abarcan tareas de validaci√≥n para la mayor√≠a de los casos.\nPyMC# pyMC es un m√≥dulo de Python que implementa modelos estad√≠sticos bayesianos, incluyendo la cadena de Markov Monte Carlo(MCMC). pyMC ofrece funcionalidades para hacer el an√°lisis bayesiano lo mas simple posible. Incluye los modelos bayesianos, distribuciones estad√≠sticas y herramientas de diagnostico para la covarianza de los modelos. Si queremos realizar un an√°lisis bayesiano esta es sin duda la librer√≠a a utilizar.\nNTLK# NLTK es la librer√≠a l√≠der para el procesamiento del lenguaje natural o NLP por sus siglas en ingl√©s. Proporciona interfaces f√°ciles de usar a m√°s de 50 cuerpos y recursos l√©xicos, como WordNet, junto con un conjunto de bibliotecas de procesamiento de texto para la clasificaci√≥n, tokenizaci√≥n, el etiquetado, el an√°lisis y el razonamiento sem√°ntico.\nObviamente, aqu√≠ solo estoy listando unas pocas de las muchas librer√≠as que existen en Python para trabajar con problemas de Machine Learning, los invito a realizar su propia investigaci√≥n sobre el tema.\nAlgoritmos m√°s utilizados# Los algoritmos que m√°s se suelen utilizar en los problemas de Machine Learning son los siguientes:\nRegresi√≥n Lineal Regresi√≥n Log√≠stica Arboles de Decision Random Forest SVM o M√°quinas de vectores de soporte. KNN o K vecinos m√°s cercanos. K-means Todos ellos se pueden aplicar a casi cualquier problema de datos y obviamente estan todos implementados por la excelente librer√≠a de Python, Scikit-learn. Veamos algunos ejemplos de ellos.\nRegresi√≥n Lineal# Se utiliza para estimar los valores reales (costo de las viviendas, el n√∫mero de llamadas, ventas totales, etc.) basados en variables continuas. La idea es tratar de establecer la relaci√≥n entre las variables independientes y dependientes por medio de ajustar una mejor l√≠nea recta con respecto a los puntos. Esta l√≠nea de mejor ajuste se conoce como l√≠nea de regresi√≥n y esta representada por la siguiente ecuaci√≥n lineal:\n$$Y = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + ... + \\beta_{n}X_{n}$$ Veamos un peque√±o ejemplo de como se implementa en Python. En este ejemplo voy a utilizar el dataset Boston que ya viene junto con Scikit-learn y es ideal para practicar con Regresiones Lineales; el mismo contiene precios de casas de varias √°reas de la ciudad de Boston.\n# graficos embebidos %matplotlib inline # importando pandas, numpy y matplotlib import pandas as pd import numpy as np import matplotlib.pyplot as plt # importando los datasets de sklearn from sklearn import datasets boston = datasets.load_boston() boston_df = pd.DataFrame(boston.data, columns=boston.feature_names) boston_df[\u0026#39;TARGET\u0026#39;] = boston.target boston_df.head() # estructura de nuestro dataset. CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT TARGET 0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33 36.2 # importando el modelo de regresi√≥n lineal from sklearn.linear_model import LinearRegression rl = LinearRegression() # Creando el modelo. rl.fit(boston.data, boston.target) # ajustando el modelo LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False) # Lista de coeficientes B para cada X list(zip(boston.feature_names, rl.coef_)) [('CRIM', -0.10717055656035711), ('ZN', 0.046395219529796805), ('INDUS', 0.020860239532172288), ('CHAS', 2.6885613993179822), ('NOX', -17.795758660308522), ('RM', 3.8047524602580101), ('AGE', 0.00075106170332574131), ('DIS', -1.4757587965198171), ('RAD', 0.30565503833910218), ('TAX', -0.012329346305270897), ('PTRATIO', -0.95346355469055977), ('B', 0.0093925127221893244), ('LSTAT', -0.5254666329007841)] # haciendo las predicciones predicciones = rl.predict(boston.data) predicciones_df = pd.DataFrame(predicciones, columns=[\u0026#39;Pred\u0026#39;]) predicciones_df.head() # predicciones de las primeras 5 lineas Pred 0 30.008213 1 25.029861 2 30.570232 3 28.608141 4 27.942882 # Calculando el desvio np.mean(boston.target - predicciones) 5.6871503553921065e-15 Como podemos ver, el desv√≠o del modelo es peque√±o, por lo que sus resultados para este ejemplo son bastante confiables.\nRegresi√≥n Log√≠stica# Los modelos lineales, tambi√©n pueden ser utilizados para clasificaciones; es decir, que primero ajustamos el modelo lineal a la probabilidad de que una cierta clase o categor√≠a ocurra y, a luego, utilizamos una funci√≥n para crear un umbral en el cual especificamos el resultado de una de estas clases o categor√≠as. La funci√≥n que utiliza este modelo, no es ni m√°s ni menos que la funci√≥n log√≠stica.\n$$f(x) = \\frac{1}{1 + e^{-1}}$$ Veamos, aqu√≠ tambi√©n un peque√±o ejemplo en Python.\n# Creando un dataset de ejemplo from sklearn.datasets import make_classification X, y = make_classification(n_samples=1000, n_features=4) # Importando el modelo from sklearn.linear_model import LogisticRegression rlog = LogisticRegression() # Creando el modelo # Dividiendo el dataset en entrenamiento y evaluacion X_entrenamiento = X[:-200] X_evaluacion = X[-200:] y_entrenamiento = y[:-200] y_evaluacion = y[-200:] rlog.fit(X_entrenamiento, y_entrenamiento) #ajustando el modelo # Realizando las predicciones y_predic_entrenamiento = rlog.predict(X_entrenamiento) y_predic_evaluacion = rlog.predict(X_evaluacion) # Verificando la exactitud del modelo entrenamiento = (y_predic_entrenamiento == y_entrenamiento).sum().astype(float) / y_entrenamiento.shape[0] print(\u0026#34;sobre datos de entrenamiento: {0:.2f}\u0026#34;.format(entrenamiento)) evaluacion = (y_predic_evaluacion == y_evaluacion).sum().astype(float) / y_evaluacion.shape[0] print(\u0026#34;sobre datos de evaluaci√≥n: {0:.2f}\u0026#34;.format(evaluacion)) sobre datos de entrenamiento: 0.92 sobre datos de evaluaci√≥n: 0.91 Como podemos ver en este ejemplo tambi√©n nuestro modelo tiene bastante precisi√≥n clasificando las categor√≠as de nuestro dataset.\nArboles de decisi√≥n# Los Arboles de Decision son diagramas con construcciones l√≥gicas, muy similares a los sistemas de predicci√≥n basados en reglas, que sirven para representar y categorizar una serie de condiciones que ocurren de forma sucesiva, para la resoluci√≥n de un problema. Los Arboles de Decision est√°n compuestos por nodos interiores, nodos terminales y ramas que emanan de los nodos interiores. Cada nodo interior en el √°rbol contiene una prueba de un atributo, y cada rama representa un valor distinto del atributo. Siguiendo las ramas desde el nodo ra√≠z hacia abajo, cada ruta finalmente termina en un nodo terminal creando una segmentaci√≥n de los datos. Veamos aqu√≠ tambi√©n un peque√±o ejemplo en Python.\n# Creando un dataset de ejemplo X, y = datasets.make_classification(1000, 20, n_informative=3) # Importando el arbol de decisi√≥n from sklearn.tree import DecisionTreeClassifier from sklearn import tree ad = DecisionTreeClassifier(criterion=\u0026#39;entropy\u0026#39;, max_depth=5) # Creando el modelo ad.fit(X, y) # Ajustando el modelo #generando archivo para graficar el arbol with open(\u0026#34;mi_arbol.dot\u0026#34;, \u0026#39;w\u0026#39;) as archivo_dot: tree.export_graphviz(ad, out_file = archivo_dot) # utilizando el lenguaje dot para graficar el arbol. !dot -Tjpeg mi_arbol.dot -o arbol_decision.jpeg Luego de usar el lenguaje dot para convertir nuestro arbol a formato jpeg, ya podemos ver la imagen del mismo.\n# verificando la precisi√≥n print(\u0026#34;precisi√≥n del modelo: {0: .2f}\u0026#34;.format((y == ad.predict(X)).mean())) precisi√≥n del modelo: 0.96 En este ejemplo, nuestro √°rbol tiene una precisi√≥n del 89%. Tener en cuenta que los Arboles de Decision tienen tendencia a sobreentrenar los datos.\nRandom Forest# En lugar de utilizar solo un arbol para decidir, ¬øpor qu√© no utilizar todo un bosque?!!. Esta es la idea central detr√°s del algoritmo de Random Forest. Tarbaja construyendo una gran cantidad de arboles de decision muy poco profundos, y luego toma la clase que cada √°rbol eligi√≥. Esta idea es muy poderosa en Machine Learning. Si tenemos en cuenta que un sencillo clasificador entrenado podr√≠a tener s√≥lo el 60 por ciento de precisi√≥n, podemos entrenar un mont√≥n de clasificadores que sean por lo general acertados y luego podemos utilizar la sabidur√≠a de todos los aprendices juntos. Con Python los podemos utilizar de la siguiente manera:\n# Creando un dataset de ejemplo X, y = datasets.make_classification(1000) # Importando el random forest from sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier() # Creando el modelo rf.fit(X, y) # Ajustando el modelo # verificando la precisi√≥n print(\u0026#34;precisi√≥n del modelo: {0: .2f}\u0026#34;.format((y == rf.predict(X)).mean())) precisi√≥n del modelo: 0.99 SVM o M√°quinas de vectores de soporte# La idea detr√°s de SVM es encontrar un plano que separe los grupos dentro de los datos de la mejor forma posible. Aqu√≠, la separaci√≥n significa que la elecci√≥n del plano maximiza el margen entre los puntos m√°s cercanos en el plano; √©stos puntos se denominan vectores de soporte. Pasemos al ejemplo.\n# importanto SVM from sklearn import svm # importando el dataset iris iris = datasets.load_iris() X = iris.data[:, :2] # solo tomamos las primeras 2 caracter√≠sticas y = iris.target h = .02 # tama√±o de la malla del grafico # Creando el SVM con sus diferentes m√©todos C = 1.0 # parametro de regulacion SVM svc = svm.SVC(kernel=\u0026#39;linear\u0026#39;, C=C).fit(X, y) rbf_svc = svm.SVC(kernel=\u0026#39;rbf\u0026#39;, gamma=0.7, C=C).fit(X, y) poly_svc = svm.SVC(kernel=\u0026#39;poly\u0026#39;, degree=3, C=C).fit(X, y) lin_svc = svm.LinearSVC(C=C).fit(X, y) # crear el area para graficar x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) # titulos de los graficos titles = [\u0026#39;SVC con el motor lineal\u0026#39;, \u0026#39;LinearSVC\u0026#39;, \u0026#39;SVC con el motor RBF\u0026#39;, \u0026#39;SVC con el motor polinomial\u0026#39;] for i, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)): # Realizando el gr√°fico, se le asigna un color a cada punto plt.subplot(2, 2, i + 1) plt.subplots_adjust(wspace=0.4, hspace=0.4) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8) # Graficando tambien los puntos de datos plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired) plt.xlabel(\u0026#39;largo del petalo\u0026#39;) plt.ylabel(\u0026#39;ancho del petalo\u0026#39;) plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max()) plt.xticks(()) plt.yticks(()) plt.title(titles[i]) plt.show() KNN o k vecinos m√°s cercanos# Este es un m√©todo de clasificaci√≥n no param√©trico, que estima el valor de la probabilidad a posteriori de que un elemento \\(x\\) pertenezca a una clase en particular a partir de la informaci√≥n proporcionada por el conjunto de prototipos. La regresi√≥n KNN se calcula simplemente tomando el promedio del punto k m√°s cercano al punto que se est√° probando.\n# Creando el dataset iris iris = datasets.load_iris() X = iris.data y = iris.target iris.feature_names ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] # importando KNN from sklearn.neighbors import KNeighborsRegressor knnr = KNeighborsRegressor(n_neighbors=10) # Creando el modelo con 10 vecinos knnr.fit(X, y) # Ajustando el modelo # Verificando el error medio del modelo print(\u0026#34;El error medio del modelo es: {:.2f}\u0026#34;.format(np.power(y - knnr.predict(X), 2).mean())) El error medio del modelo es: 0.02 K-means# K-means es probablemente uno de los algoritmos de agrupamiento m√°s conocidos y, en un sentido m√°s amplio, una de las t√©cnicas de aprendizaje no supervisado m√°s conocidas. K-means es en realidad un algoritmo muy simple que funciona para reducir al m√≠nimo la suma de las distancias cuadradas desde la media dentro del agrupamiento. Para hacer esto establece primero un n√∫mero previamente especificado de conglomerados, K, y luego va asignando cada observaci√≥n a la agrupaci√≥n m√°s cercana de acuerdo a su media. Veamos el ejemplo\n# Creando el dataset grupos, pos_correcta = datasets.make_blobs(1000, centers=3, cluster_std=1.75) # Graficando los grupos de datos f, ax = plt.subplots(figsize=(7, 5)) colores = [\u0026#39;r\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;b\u0026#39;] for i in range(3): p = grupos[pos_correcta == i] ax.scatter(p[:,0], p[:,1], c=colores[i], label=\u0026#34;Grupo {}\u0026#34;.format(i)) ax.set_title(\u0026#34;Agrupamiento perfecto\u0026#34;) ax.legend() plt.show() # importando KMeans from sklearn.cluster import KMeans # Creando el modelo kmeans = KMeans(n_clusters=3) kmeans.fit(grupos) # Ajustando el modelo KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300, n_clusters=3, n_init=10, n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001, verbose=0) # verificando los centros de los grupos kmeans.cluster_centers_ array([[-9.90500465, -4.48254047], [-8.1907267 , 7.77491011], [ 1.9875472 , 4.07789958]]) # Graficando segun modelo f, ax = plt.subplots(figsize=(7, 5)) colores = [\u0026#39;r\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;b\u0026#39;] for i in range(3): p = grupos[pos_correcta == i] ax.scatter(p[:,0], p[:,1], c=colores[i], label=\u0026#34;Grupo {}\u0026#34;.format(i)) ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=100, color=\u0026#39;black\u0026#39;, label=\u0026#39;Centros\u0026#39;) ax.set_title(\u0026#34;Agrupamiento s/modelo\u0026#34;) ax.legend() plt.show() Con esto doy por conclu√≠da esta introducci√≥n al Machine Learning con Python, espero les sea √∫til.\nSaludos!\nEste post fue escrito utilizando IPython notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2015-10-10","id":34,"permalink":"/blog/2015/10/10/machine-learning-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nUna de las ramas de estudio que cada vez esta ganando m√°s popularidad dentro de las ciencias de la computaci√≥n es el aprendizaje autom√°tico o Machine Learning. Muchos de los servicios que utilizamos en nuestro d√≠a a d√≠a como google, gmail, netflix, spotify o amazon se valen de las herramientas que les brinda el Machine Learning para alcanzar un servicio cada vez m√°s personalizado y lograr as√≠ ventajas competitivas sobre sus rivales.","tags":["python","estadistica","programacion","machine learning","analisis de datos"],"title":"Machine Learning con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nEl t√≠o Petros y la conjetura# Hace un tiempo atr√°s, quede atrapado en la lectura de la apasionante novela de Apostolos Doxiadis, El t√≠o Petros y la conjetura de Goldbach. La novela trata basicamente de la relaci√≥n entre un joven, en busca de su vocaci√≥n, y su t√≠o, quien en el pasado fue un prodigio de las matem√°ticas, pero que luego se recluy√≥ de su familia y de la comunidad cient√≠fica consumido por el intento solitario de demostrar uno de los problemas abiertos m√°s dif√≠ciles de la teor√≠a de n√∫meros, la Conjetura de Goldbach. Lo que m√°s me sorprendi√≥ del problema que consumi√≥ la vida del querido t√≠o Petros, es lo simple que es su enunciado; la Conjetura de Goldbach nos dice que \u0026ldquo;Todo n√∫mero par mayor que 2 puede escribirse como suma de dos n√∫meros primos.\u0026rdquo;\nEste enunciado, junto con la otra conjetura postulada por Goldbach, conocida como la Conjetura debil de Goldbach, que nos dice que \u0026ldquo;Todo n√∫mero impar mayor que 5 puede expresarse como suma de tres n√∫meros primos.\u0026rdquo;; trae a colaci√≥n el concepto de los n√∫meros primos como bloques constructores de los enteros.\nLos n√∫meros primos# Pero, ¬øc√≥mo es esto de que los n√∫meros primos pueden ser considerados como bloques constructores de los n√∫meros enteros?\nUn n√∫mero primo es un entero positivo que solo puede ser dividido en dos factores distintos, 1 y si mismo. De esta definici√≥n se desprende que el n√∫mero 1 no es un n√∫mero primo ya que solo puede ser dividido en un solo factor; en cambio, el n√∫mero 2 si es primo, el √∫nico n√∫mero primo que es par.\nLa idea de que los n√∫meros primos pueden ser considerados como bloques constructores de los n√∫meros enteros surge del enunciado del Teorema fundamental de la aritm√©tica que nos dice que \u0026ldquo;Todo entero positivo puede ser representado de forma √∫nica como un producto de factores primos\u0026rdquo;; as√≠ por ejemplo el n√∫mero \\(28\\) puede ser representado como \\(2^2 * 7\\); o el \\(60\\) como \\(2^2 * 3 * 5\\). Este teorema es un concepto fundamental en criptograf√≠a, los principales algoritmos de cifrado que se utilizan hoy en d√≠a residen en la factorizaci√≥n de primos, ya que es un proceso que requiere de mucho esfuerzo para calcularse mientras m√°s grande sea el n√∫mero que queremos factorizar.\nOtro aspecto interesante de los n√∫meros primos es que parecen surgir en forma aleatoria, hay veces que pueden aparecer en pares como (11, 13), (29, 31) o (59, 61) pero otras veces puede haber un largo intervalo entre ellos. A√∫n no se ha encontrado una formula que pueda predecir cual va a ser el pr√≥ximo n√∫mero primo. Como parece ser bastante dif√≠cil encontrar un patr√≥n en los n√∫meros primos, el esfuerzo de los matem√°ticos paso de intentar encontrar un patr√≥n a intentar comprender la distribuci√≥n de los n√∫meros primos dentro de todos los enteros; es decir, intentar responder la pregunta de ¬øCu√°l es la probabilidad de que un n√∫mero sea primo si elijo un n√∫mero al azar en el rango de 0 a N?. Uno de los primeros en dar una respuesta bastante aproximada a esta pregunta fue Gauss, quien con tan solo 15 a√±os de edad propuso la formula \\(\\pi(x)\\approx\\frac{x}{ln(x)}\\) para responder a esa pregunta.\nLa formula propuesta Gauss implica que a medida que los n√∫meros se hacen cada vez m√°s grandes, los n√∫meros primos son cada vez m√°s escasos. Esto es lo que se conoce como teorema de los n√∫meros primos. A pesar de que los n√∫meros primos son cada vez m√°s escasos mientras m√°s grandes, siguen surgiendo indefinidamente, son infinitos; esto esta bien demostrado por el ya famoso teorema de Euclides; quien incluyo una de las m√°s bellas demostraciones de las matem√°ticas en su obra Elementos en el 300 AC.\nEncontrando los n√∫meros primos# Para encontrar todos los n√∫meros primos menores que un n√∫mero N, uno de los algoritmos m√°s eficientes y m√°s f√°ciles de utilizar es lo que se conoce como la criba de Erat√≥stenes, el procedimiento que se utiliza consiste en crear una tabla con todos los n√∫meros naturales comprendidos entre 2 y N, y luego ir tachando los n√∫meros que no son primos de la siguiente manera: Comenzando por el 2, se tachan todos sus m√∫ltiplos; comenzando de nuevo, cuando se encuentra un n√∫mero entero que no ha sido tachado, ese n√∫mero es declarado primo, y se procede a tachar todos sus m√∫ltiplos, as√≠ sucesivamente. La siguiente animaci√≥n describe el procedimiento graficamente. Ejemplo en Python# Veamos un ejemplo en Python de como implementar la criba de Erat√≥stenes y un proceso de factorizaci√≥n de primos.\n# Factorizando primos en Python import numpy as np def criba_eratostenes(n): \u0026#34;\u0026#34;\u0026#34;Criba Eratostenes\u0026#34;\u0026#34;\u0026#34; l=[] multiplos = set() for i in range(2, n+1): if i not in multiplos: l.append(i) multiplos.update(range(i*i, n+1, i)) return l def factorizar_primos(n): \u0026#34;\u0026#34;\u0026#34;Factoriza un entero positivo en primos \u0026gt;\u0026gt;\u0026gt;factorizar_primos(28) [2, 2, 7] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;=1: return \u0026#34;Ingresar un entero mayor a 1\u0026#34; factores = [] primos = criba_eratostenes(n) pindex = 0 p = primos[pindex] num = n while p != num: if num % p == 0: factores.append(p) num //= p else: pindex += 1 p = primos[pindex] factores.append(p) return factores factorizar_primos(28) # Factores primos de 28 [2, 2, 7] # Factores primos de 1982 factorizar_primos(1982) [2, 991] # Factores primos de 2015 factorizar_primos(2015) [5, 13, 31] La conjetura de Goldbach y Python# La Conjetura de Goldbach, es otro ejemplo de como tambi√©n podemos construir todos los n√∫meros enteros con simples operaciones aritm√©ticas como la suma y no m√°s que tres n√∫meros primos. Al d√≠a de hoy, la conjetura continua sin poder ser demostrada; y es considerada uno de los problemas m√°s dif√≠ciles de las matem√°ticas. La mayor√≠a de los matem√°ticos estiman que es verdadera, ya que se ha mostrado cierta hasta por lo menos el \\(10^{18}\\); aunque algunos dudan que sea cierta para n√∫meros extremadamente grandes, el gran Ramanujan dicen que se inclu√≠a en este √∫ltimo grupo.\nObviamente, como este blog esta dedicado a Python, no podr√≠a concluir este art√≠culo sin incluir una implementaci√≥n de la Conjetura de Goldbach en uno de los lenguajes que mejor se lleva con las matem√°ticas!\nEn esta implementaci√≥n vamos a utilizar tres funciones, en primer lugar una criba de primos vectorizada utilizando numpy, para lograr un mejor rendimiento que con la criba de Erat√≥stenes del ejemplo anterior; y luego vamos a tener dos sencillas funciones adicionales, una que nos devuelva la composici√≥n de Goldbach para cualquier n√∫mero par que le pasemos como par√°metro.(esta funci√≥n solo nos va a devolver la primer soluci√≥n que encuentra, ya que pueden existir varias soluciones para algunos enteros pares). Por √∫ltimo, la restante funci√≥n va a listar los resultados de la Conjetura de Goldbach para un rango de n√∫meros enteros.\n# La conjetura de Goldbach en Python import numpy as np def criba_primos(n): \u0026#34;\u0026#34;\u0026#34;Criba generadora de n√∫meros primos. Input n\u0026gt;=6, devuleve un array de primos, 2 \u0026lt;= p \u0026lt; n \u0026#34;\u0026#34;\u0026#34; criba = np.ones(n / 3 + (n % 6 == 2), dtype=np.bool) for i in range(1, int(n**0.5 / 3 + 1)): if criba[i]: k = 3 * i + 1 | 1 criba[k * k / 3::2 * k] = False criba[k * (k - 2 * (i \u0026amp; 1) + 4) / 3::2 * k] = False return np.r_[2, 3, ((3 * np.nonzero(criba)[0][1:] + 1) | 1)] def goldbach(n): \u0026#34;\u0026#34;\u0026#34;imprime la composici√≥n de goldbach para n. \u0026gt;\u0026gt;\u0026gt; goldbach(28) (5, 23) \u0026#34;\u0026#34;\u0026#34; primos = criba_primos(n) lo = 0 hi = len(primos) - 1 while lo \u0026lt;= hi: sum = primos[lo] + primos[hi] if sum == n: break elif sum \u0026lt; n: lo += 1 else: hi -= 1 else: print(\u0026#34;No se encontro resultado de la conjetura de Goldbach para {}\u0026#34;.format(n)) return primos[lo], primos[hi] def goldbach_list(lower, upper): \u0026#34;\u0026#34;\u0026#34;Imprime la composici√≥n de Goldbach para todos los n√∫meros pares grandes que \u0026#39;lower\u0026#39; y menores o iguales que \u0026#39;upper\u0026#39;. \u0026gt;\u0026gt;\u0026gt; goldbach_list(9,20) 10 = 3 + 7 12 = 5 + 7 14 = 3 + 11 16 = 3 + 13 18 = 5 + 13 20 = 3 + 17 \u0026#34;\u0026#34;\u0026#34; # La conjetura se aplica a pares mayores que 2. if lower % 2 != 0: lower += 1 if lower \u0026lt; 4: lower = 4 if upper % 2 != 0: upper -= 1 for n in range(lower, upper + 1, 2): gb = goldbach(n) print(\u0026#34;{0} = {1} + {2}\u0026#34;.format(n, gb[0], gb[1])) # Goldbach entre 2000 y 2016 goldbach_list(2000, 2016) 2000 = 3 + 1997 2002 = 3 + 1999 2004 = 5 + 1999 2006 = 3 + 2003 2008 = 5 + 2003 2010 = 7 + 2003 2012 = 13 + 1999 2014 = 3 + 2011 2016 = 5 + 2011 # Goldbach entre 9.999.980 y 10.000.000 goldbach_list(9999980, 10000000) 9999980 = 7 + 9999973 9999982 = 11 + 9999971 9999984 = 11 + 9999973 9999986 = 13 + 9999973 9999988 = 17 + 9999971 9999990 = 17 + 9999973 9999992 = 19 + 9999973 9999994 = 3 + 9999991 9999996 = 5 + 9999991 9999998 = 7 + 9999991 10000000 = 29 + 9999971 # Goldbach entre 99.999.980 y 100.000.000 goldbach_list(99999980, 100000000) 99999980 = 193 + 99999787 99999982 = 11 + 99999971 99999984 = 13 + 99999971 99999986 = 139 + 99999847 99999988 = 17 + 99999971 99999990 = 19 + 99999971 99999992 = 3 + 99999989 99999994 = 5 + 99999989 99999996 = 7 + 99999989 99999998 = 67 + 99999931 100000000 = 11 + 99999989 Con esto termino, el que quiera puede divertirse intentando comprobar la Conjetura de Goldbach, aunque corre el riesgo de terminar desperdiciando su tiempo como el bueno del t√≠o Petros en la novela. Espero que les haya parecido interesante el art√≠culo.\nSaludos!\nEste post fue escrito utilizando IPython notebook. Pueden descargar este notebook o ver su versi√≥n est√°tica en nbviewer.\n","date":"2015-09-13","id":35,"permalink":"/blog/2015/09/13/de-tios-primos-teoremas-y-conjeturas/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nEl t√≠o Petros y la conjetura# Hace un tiempo atr√°s, quede atrapado en la lectura de la apasionante novela de Apostolos Doxiadis, El t√≠o Petros y la conjetura de Goldbach. La novela trata basicamente de la relaci√≥n entre un joven, en busca de su vocaci√≥n, y su t√≠o, quien en el pasado fue un prodigio de las matem√°ticas, pero que luego se recluy√≥ de su familia y de la comunidad cient√≠fica consumido por el intento solitario de demostrar uno de los problemas abiertos m√°s dif√≠ciles de la teor√≠a de n√∫meros, la Conjetura de Goldbach.","tags":["python","matematica","programacion"],"title":"De t√≠os, primos, teoremas y conjeturas"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# En el vertiginoso mundo actual de las finanzas; d√≥nde la velocidad, frecuencia y volumen de los datos aumentan a un ritmo considerable; la aplicaci√≥n combinada de tecnolog√≠a y software, junto con algoritmos avanzados y diferentes m√©todos para recopilar, procesar y analizar datos se ha vuelto fundamental para obtener la informaci√≥n necesaria para una eficiente toma de decisiones. Es dentro de este contexto, que se viene produciendo un gran crecimento en la utilizaci√≥n de Python dentro de la industria de las finanzas.\nPython se esta comenzando a utilizar ampliamente en diversos sectores de las finanzas, como la banca, la gesti√≥n de inversiones, los seguros, e incluso en los bienes ra√≠ces; se utiliza principalmente para la construcci√≥n de herramientas que ayuden en la creaci√≥n de modelos financieros, gesti√≥n de riesgos, y el comercio. Incluso las grandes corporaciones financieras, como Bank of America o JP Morgan, estan comenzando a utilizar Python para construir su infraestructura para la gesti√≥n de posiciones financieras, precios de activos, gesti√≥n de riesgos, sistemas de comercio y comercio algoritmico.\nAlgunas de las razones que hacen de Python un lenguaje de programaci√≥n tan atractivo en el mundo de las finanzas son:\nSu simple sintaxis: Python es mundialmente conocido por lo f√°cil que resulta leerlo, muchas veces no existen casi diferencias entre seudo c√≥digo y Python; tampoco suelen existir muchas diferencias entre expresar un algoritmo matematicamente o en Python.\nSu ecosistema: Python es mucho m√°s que un lenguaje de programaci√≥n, es todo un ecosistema en s√≠ mismo; ya que podemos encontrar un sinn√∫mero de herramientas para realizar cualquier tipo de tareas; en Python podemos encontrar m√≥dulos para realizar c√°lculos cient√≠ficos, m√≥dulos para desarrollar aplicaciones webs, m√≥dulos para realizar tareas de administraci√≥n de sistemas, m√≥dulos para trabajar con bases de datos; entre otros. Todos ellos muy f√°cilmente integrables dentro del lenguaje. La variedad del ecosistema de herramientas de Python, nos ofrece la posibilidad de desarrollar una soluci√≥n completa a cualquier tipo de problema utilizando un solo lenguaje de programaci√≥n.\nSu integraci√≥n: Otras de las caracter√≠sticas por la que Python es tambi√©n famoso, es por su f√°cil integraci√≥n con otros lenguajes de programaci√≥n. Generalmente, las grandes empresas suelen tener herramientas desarrolladas en distintos lenguajes de programaci√≥n; las caracter√≠sticas din√°micas de Python, hacen que sea ideal para unir todos esos distintos componentes en una sola gran aplicaci√≥n. Python puede ser enlazado f√°cilmente a herramientas desarrollas en C, C++ o Fortran.\nEficiencia y productividad: Por √∫ltimo, otra de las caracter√≠sticas que hacen a Python tan atractivo, es que con √©l, se pueden lograr resultados de calidad en una forma mucho m√°s eficiente y productiva. La mayor√≠a de sus m√≥dulos est√°n ampliamente testeados y cuentan con el soporte de una amplia comunidad de usuarios; sus caracter√≠sticas din√°micas e interactivas lo hacen ideal para el an√°lisis exploratorio de datos facilitando los an√°lisis financieros. Tambi√©n es sabido, que la elegancia de su sintaxis hace que se necesiten mucho menos l√≠neas de c√≥digo para desarrollar un programa en Python que en casi cualquier otro lenguaje de programaci√≥n.\nPrincipales librer√≠as# Las principales librer√≠as que vamos a utilizar para realizar tareas de analisis financiero con Python son muchas de las que ya he venido hablando en anteriores art√≠culos; principalmente:\nPandas: La cual fue dise√±ada desde un comienzo para facilitar el an√°lisis de datos financieros, principalmente las series de tiempo propias del mercado cambiario de acciones. Con las estructuras de datos que nos brinda esta librer√≠a se vuelve sumamente f√°cil modelar y resolver problemas financieros.\nNumpy: El principal modulo matem√°tico que nos ofrece Python, en el no solo vamos a encontrar las siempre pr√°cticas matrices que facilitan en sobremanera el manejo de informaci√≥n num√©rica; sino que tambi√©n vamos a poder encontrar un gran n√∫mero de funciones matem√°ticas.\nMatplotlib: La siempre vigente librer√≠a para realizar gr√°ficos en Python.\nstatsmodels: Si de estad√≠stica se trata, no hay como esta librer√≠a para realizar cualquier tipo de analisis estad√≠stico.\nPuLP: La cual nos permite crear modelos de programaci√≥n lineal en forma muy sencilla.\nQuandl: Este m√≥dulo nos permite interactuar f√°cilmente con la API de quandl.com para obtener en forma muy sencilla todo tipo de informaci√≥n financiera.\nZipline: Zipline es una librer√≠a para el comercio algoritmico; esta basada en eventos y trata de aproximarse lo m√°s cerca posible a como operan los verdades sistemas de comercio electr√≥nico de las principales bolsas del mundo. Zipline es una de las principales tecnolog√≠as detr√°s del popular sitio quantopian.com, la comunidad web que pone a prueba distintos algoritmos de comercio algoritmico.\nBueno, pero basta de introducciones y pasamos a describir los principales conceptos financieros y como podemos calcularlos con la ayuda de Python, ya que el tiempo es dinero!!.\nConceptos b√°sicos de Finanzas# Los conceptos m√°s b√°sicos que podemos encontrar dentro de las finanzas son: valor futuro, valor presente, y la tasa interna de retorno. Estos conceptos nos dicen cuanto nuestro dinero va a crecer si lo depositamos en un banco (valor futuro), cuanto vale hoy la promesa de unos pagos que recibiremos en el futuro(valor presente), y qu√© tasa de rendimiento podemos obtener de nuestras inversiones (tasa interna de retorno). Recordemos que todos los activos financieros y toda planificaci√≥n financiera siempre tiene una dimensi√≥n de tiempo; as√≠ por ejemplo si depositamos USD 100 en un banco que nos paga una tasa de inter√©s anual de 6% , luego de un a√±o obtendr√≠amos un importe de USD 106.\nValor Futuro# El valor futuro o FV (por sus siglas en ingl√©s), nos indica el valor en el futuro que tendr√° el dinero depositado hoy en una cuenta bancaria que nos pague intereses. El valor futuro de USD X depositado hoy en una cuenta que paga r% de inter√©s anual y que es dejado en la cuenta durante n a√±os es \\(X * (1 + r)^n\\). El valor futuro es nuestro primer ejemplo de inter√©s compuesto, es decir, el principio de que podemos ganar intereses sobre los intereses. De la definici√≥n que dimos del valor futuro, podemos obtener su expresi√≥n matem√°tica:\n$$FV = X * ( 1 + r )^n$$ Como podemos ver, su c√°lculo es bastante simple. Veamos un ejemplo de como calcular el FV de un dep√≥sito de USD 1000 a 3 a√±os y con una tasa de inter√©s del 6% anual.\n# graficos embebidos %matplotlib inline # Ejemplo FV con python # $1000 al 6% anual por 3 a√±os. # importando librer√≠as import numpy as np import matplotlib.pyplot as plt x = -1000 # deposito r = .06 # tasa de interes n = 3 # cantidad de a√±os # usando la funcion fv de numpy FV = np.fv(pv=x, rate=r, nper=n, pmt=0) FV 1191.016 # Controlando el resultado x * (1 + r)**n -1191.016 # Graficando las funciones con interes de 6 y 12 % a 20 a√±os. t = list(range(0, 21)) def fv6(num): return np.fv(pv=-1000, rate=r, pmt=0, nper=num) def fv12(num): return np.fv(pv=-1000, rate=.12, pmt=0, nper=num) plt.title(\u0026#34;Graficando FV 6 y 12 %\u0026#34;) plt.plot(t, fv6(t), label=\u0026#34;interes 6 %\u0026#34;) plt.plot(t, fv12(t), label=\u0026#34;interes 12 %\u0026#34;) plt.legend(loc=\u0026#39;upper left\u0026#39;) plt.show() Al graficar dos funciones de FV, una con una tasa de interes del 6% y otra con una tasa m√°s alta del 12%, podemos ver que el valor futuro suele ser bastante sensitivo a los cambios en la tasa de interes, peque√±as variaciones en ella pueden generar grandes saltos a lo largo del tiempo.\nAnualidades# Como podemos ver en el ejemplo anterior, la funci√≥n fv de Numpy tiene varios par√°metros, esto es as√≠, porque existen otros casos en los que el c√°lculo del valor futuro se puede volver m√°s complicado; es aqu√≠ cuando comenzamos a hablar de anulidades. La idea de las anualidades es no solo quedarse con el c√°lculo simple de cuanto me va a rendir un solo deposito inicial a fin de un per√≠odo, sino tambi√©n poder calcular el valor futuro de m√∫ltiples dep√≥sitos que se reinvierten a una misma tasa de inter√©s. Supongamos por ejemplo que queremos hacer 10 dep√≥sitos anuales de USD 1000 cada uno, los cuales vamos a ir depositando al comienzo de cada a√±o. ¬øCu√°l ser√≠a en este caso el valor futuro de nuestra anualidad luego del d√©cimo a√±o?. Ayud√©monos de Python para calcular la respuesta!\n# Calculando el valor de la anualidad con 6% anual x = -1000 # valor de depositos r = .06 # tasa de interes n = 10 # cantidad de a√±os # usando la funcion fv de numpy FV = np.fv(pv=0, rate=r, nper=n, pmt=x, when=\u0026#39;begin\u0026#39;) FV 13971.642638923764 Aqu√≠ comenzamos con un valor presente(PV) de cero, luego realizamos el primer deposito de USD 1000 al comienzo del primer a√±o y continuamos con los sucesivos dep√≥sitos al comienzo de cada uno de los restantes a√±os. Para poder entender mejor como funciona la funci√≥n fv de Numpy voy a explicar un poco m√°s sus par√°metros.\npv = este parametro es el valor presente de nuestra inversi√≥n o anualidad; en nuestro ejemplo empezamos con un valor de cero; ya que luego vamos a ir realizando los diferentes dep√≥sitos de USD 1000. rate = es la tasa efectiva de inter√©s que nos rendir√° la anualidad por cada per√≠odo. nper = Es el n√∫mero de per√≠odos. Tener en cuenta que si aqu√≠ estamos utilizando como unidad de medida de a√±os, nuestra tasa de inter√©s deber√° estar expresada en la misma unidad. pmt = El valor de los dep√≥sitos que vamos a ir invirtiendo en nuestra anualidad. En nuestro caso el valor de -1000 refleja el importe que vamos a ir depositando a√±o a a√±o.(se expresa con signo negativo, ya que un deposito implica una salida de dinero). when = Este par√°metro nos dice cuando se van a hacer efectivos nuestros dep√≥sitos, ya que el resultado puede ser muy distinto si realizamos el deposito al comienzo(como en nuestro ejemplo) o al final de cada per√≠odo. # mismo caso pero con la diferencia de que los depositos se # realizan al final de cada per√≠odo. FV = np.fv(pv=0, rate=r, nper=n, pmt=x, when=\u0026#39;end\u0026#39;) FV 13180.79494238091 En este √∫ltimo ejemplo, el valor es menor por las perdidas relativas de inter√©s que vamos teniendo al realizar los dep√≥sitos al final de cada per√≠odo en lugar de al comienzo.\nValor Presente# El valor presente o PV (por sus siglas en ingl√©s), nos indica el valor que tienen hoy un pago o pagos que recibiremos en el futuro. Supongamos por ejemplo que sabemos que un t√≠o nos va a regalar USD 1000 dentro de 3 a√±os porque somos su sobrino favorito, si tambi√©n sabemos que un banco nos pagar√≠a un 6% de inter√©s por los dep√≥sitos en una caja de ahorro, podr√≠amos calcular el valor presente que tendr√≠a ese pago futuro de nuestro t√≠o en el d√≠a de hoy. La formula para calcular el valor presente la podemos derivar de la que utilizamos para calcular el valor futuro y se expresar√≠a del siguiente modo:\n$$PV =\\frac{fv}{(1 + r)^n}$$ Aplicando la esta formula sobre los datos con que contamos, podr√≠amos calcular el valor de hoy de la promesa de pago de USD 1000 de nuestro t√≠o, los que nos dar√≠a un valor presente de USD 839.62 como se desprende del siguiente c√°lculo.\nfv = 1000 # valor futuro r = .06 # tasa de interes n = 3 # cantidad de a√±os fv / ((1 + r)**n) 839.6192830323018 # usando la funcion pv de numpy PV = np.pv(fv=fv, rate=r, nper=n, pmt=0) PV -839.6192830323018 Estos USD 839.62 en realidad lo que representan es que si nosotros hoy deposit√°ramos en la caja de ahorro de un banco que nos pague 6% anual de inter√©s el importe de USD 839.62, obtendr√≠amos dentro de los 3 a√±os los mismos USD 1000 que nos ofreci√≥ dar nuestro t√≠o dentro de 3 a√±os; o lo que es lo mismo que decir que el valor futuro dentro de 3 a√±os de USD 839.62 son los USD 1000 de nuestro querido t√≠o.\n# Calculando el valor futuro de los 839.62 np.fv(pv=-839.62, rate=r, nper=n, pmt=0) 1000.00085392 Valor presente y anualidades# Al igual que en el caso del valor futuro, aqu√≠ tambi√©n podemos encontrarnos con las anualidades, es decir, una serie de pagos iguales que recibiremos. El valor presente de una anualidad nos va a decir el valor que tienen hoy los futuros pagos de la anualidad. As√≠, por ejemplo si nuestro t√≠o en lugar de regalarnos USD 1000 dentro de 3 a√±os, decide darnos USD 250 al final de cada a√±o durante 5 a√±os; y asumiendo la misma tasa de inter√©s que nos ofrece el banco de 6% anual. El valor presente de esta anualidad ser√≠a USD 1053.09.\n# Calculando el valor de la anualidad PV = np.pv(fv=0, rate=r, nper=5, pmt=-250, when=\u0026#39;end\u0026#39;) PV 1053.090946391429 Eligiendo la tasa de descuento# Uno de los puntos sobre el que hacer m√°s foco al calcular el valor presente de un flujo de fondos futuro, es el de como elegir la tasa para descontar estos fondos, ya que la tasa que utilicemos es la pieza clave para la exactitud de nuestros c√°lculos. El principio b√°sico que nos debe guiar la elegir la tasa de descuento es el de tratar de elegir que sea apropiada al riesgo y la duraci√≥n de los flujos de fondos que estamos descontando. En el ejemplo que venimos viendo, como sabemos que nuestro t√≠o es una persona muy solvente y de palabra, podemos considerar que no existe mucho riesgo en ese flujo de fondos, por lo que utilizar la tasa de inter√©s de una caja de ahorro parece ser un buen criterio para descontar ese flujo. En los casos de las empresas, las mismas suelen utilizar el costo del capital como una tasa de descuento apropiada para descontar el flujo futuro de sus inversiones o proyectos.\nValor Presente Neto# Un concepto que merece una especial menci√≥n, por su importancia dentro del mundo de las finanzas, cuando hablamos del valor presente, es el de Valor Presente Neto. Cuando estamos descontando flujos de fondos futuros para traerlos a su valor actual, puede ser que √©stos flujos no sean homogeneos, por lo que ya no podr√≠amos tratarlos como una anualidad, ya que los pagos son por importes distintos todos los a√±os; para estos casos debemos utilizar el Valor Presente Neto.\nEl Valor Presente Neto o NPV (por sus siglas en ingl√©s) de una serie de flujos futuros de fondos es su igual a su valor presente menos el importe de la inversi√≥n inicial necesaria para obtener esos mismos flujos de fondos futuros. Su expresi√≥n matem√°tica ser√≠a la siguiente:\n$$NPV = \\sum\\limits_{t=1}^n \\frac{V_{t}}{(1 + r)^t} - I_{0}$$ donde, \\(V_{t}\\) representa el flujo de fondos de cada per√≠odo \\(t\\); \\(I_{0}\\) es el valor inicial de la inversi√≥n; \\(r\\) es la tasa de descuento utilizada; y \\(n\\) es la cantidad de per√≠odos considerados.\nVolviendo al ejemplo que ven√≠amos utilizando de nuestro generoso t√≠o, esta vez no ofrece pagarnos USD 500 al final del primer a√±o, USD 750 al final del segundo a√±o, USD 1000 al final del tercer a√±o, USD 1250 al final del cuarto a√±o y USD 500 al final del quinto a√±o. El NPV de este flujo de fondos ser√≠a de USD 3342.56.\n# Calculando el valor presente neto. NPV = np.npv(rate=.06, values=[0, 500, 750, 1000, 1250, 500]) NPV 3342.560891731083 El Valor Presente Neto es sumamente utilizado en los an√°lisis financieros, principalmente para evaluar inversiones o proyectos. Como regla general se considera que si el NPV de un proyecto o inversi√≥n es positivo, se trata de un proyecto rentable en el que deber√≠amos invertir; en cambio si el NPV es negativo estamos ante un mal negocio.\nSi por ejemplo, tendr√≠amos que invertir hoy USD 4000 para poder obtener un flujo de fondos de USD 500 al final del primer a√±o, USD 750 al final del segundo a√±o, USD 1000 al final del tercer a√±o, USD 1250 al final del cuarto a√±o y USD 500 al final del quinto a√±o; estar√≠amos haciendo un mal negocio, ya que como sabemos el valor presente de esos flujos de fondos es de USD 3342.56, un valor mucho menor a los USD 4000 que deber√≠amos invertir.\n# Calculando el NPV de la inversi√≥n de 4000. NPV = np.npv(rate=.06, values=[-4000, 500, 750, 1000, 1250, 500]) NPV -657.4391082689172 En el ejemplo podemos ver que al utilizar la funci√≥n npv de Numpy, el primer valor en la lista de valores que le pasamos al par√°metro values debe ser el monto de la inversi√≥n inicial, y como implica un desembolso de dinero, su signo es negativo.\nSi en lugar de tener que invertir USD 4000, tendr√≠amos que invertir USD 3000 para obtener el mismo flujo de fondos, ya estar√≠amos realizando una buena inversi√≥n, con NPV positivo.\n# Calculando el NPV de la inversi√≥n de 3000. NPV = np.npv(rate=.06, values=[-3000, 500, 750, 1000, 1250, 500]) NPV 342.56089173108285 Tasa interna de Retorno# La tasa interna de retorno o IRR (por sus siglas en ingl√©s) es la tasa de descuento que hace que el Valor Presente Neto de los flujos de fondos futuros sea igual a cero; tambi√©n puede ser definida como la tasa de inter√©s compuesto que nos paga nuestra inversi√≥n.\nAl igual que como sucede con el Valor Presente Neto, podemos utilizar a la tasa interna de retorno para tomar decisiones financieras. Aqu√≠ la regla general es que, al momento de decidir entre diferentes inversiones, deber√≠amos elegir aquella con una tasa interna de retorno m√°s alta; ya que es la que en menos tiempo no va a devolver nuestra inversi√≥n inicial.\nVeamos un ejemplo, supongamos que tenemos USD 1000 para invertir, y que podemos decidir invertir ese dinero en una compa√±√≠a que nos va a pagar USD 300 al final de cada uno de los pr√≥ximos cuatro a√±os; o por otra lado, podemos invertir el dinero en una caja de ahorro de un banco que nos va a pagar 5% anual. ¬øD√≥nde deber√≠amos invertir nuestro dinero?\n# Calculando la tasa interna de retorno de la inversion en la compa√±√≠a IRR = np.irr([-1000, 300, 300, 300, 300]) IRR * 100 7.713847295208343 Al calcular la IRR de la inversi√≥n que podr√≠amos hacer en la compa√±√≠a, vemos que nos da un valor de 7.71%; esta tasa es m√°s alta que la tasa del 5% que nos ofrece el banco por el deposito en su caja de ahorro, por lo que deber√≠amos decidir invertir nuestro dinero en la compa√±√≠a en lugar de en el banco.\nLa IRR graficamente# Como se desprende de su definici√≥n, la tasa interna de retorno es la tasa que hace que el NPV se haga cero, por lo que si nos propusiesemos graficar el NPV en funci√≥n de la tasa de descuento, podr√≠amos encontrar a simple vista, cual es la IRR de un determinado flujo de fondos. Veamos un ejemplo, graficando el flujo de fondos con el que trabajamos anteriormente.\n# Graficando el NPV en funci√≥n de la tasa de descuento def npv_irr(tasas): result = [] for tasa in tasas: result.append(np.npv(tasa/100,[-1000, 300, 300, 300, 300] )) return result tasas = list(range(14)) plt.title(\u0026#34;NPV y la tasa de descuento\u0026#34;) plt.plot(tasas, npv_irr(tasas), marker=\u0026#39;o\u0026#39;, label=\u0026#39;NPV\u0026#39;) plt.axhline(0, color=\u0026#39;red\u0026#39;) axes = plt.gca() axes.set_ylim([-200,250]) plt.xticks(tasas) plt.legend(loc=\u0026#39;upper right\u0026#39;) plt.show() Como podemos ver en el gr√°fico, la funci√≥n de NPV, se hace cero en aproximadamente 7.71%; es decir, el valor de la IRR para ese flujo de fondos.\nInformaci√≥n financiera y Pandas# En las finanzas, una de las formas de datos m√°s comunes e importantes con la que nos vamos a encontrar son las series de tiempo; para trabajar con este tipo de informaci√≥n en Python, no existe mejor librer√≠a que Pandas; sus dos estructuras de datos b√°sicas, las Series y el Dataframe, nos ayudan a manipular informaci√≥n financiera de forma muy conveniente. Adem√°s Pandas nos proporciona una gran bater√≠a de m√©todos y funciones que nos facilitan la obtenci√≥n y el an√°lisis de datos financieros. Veamos algunos ejemplos de las cosas que podemos con Pandas.\n# Importando pandas y datetime import pandas as pd import pandas_datareader.data as web import datetime as dt # Extrayendo informaci√≥n financiera de Yahoo! Finance inicio = dt.datetime(2014, 1, 1) fin = dt.datetime(2014, 12, 31) msft = web.DataReader(\u0026#34;MSFT\u0026#34;, \u0026#39;yahoo\u0026#39;, inicio, fin) # informaci√≥n de Microsoft aapl = web.DataReader(\u0026#34;AAPL\u0026#34;, \u0026#39;yahoo\u0026#39;, inicio, fin) # informaci√≥n de Apple msft[:3] High Low Open Close Volume Adj Close Date 2014-01-02 37.400002 37.099998 37.349998 37.160000 30632200.0 32.951675 2014-01-03 37.220001 36.599998 37.200001 36.910000 31134800.0 32.729988 2014-01-06 36.889999 36.110001 36.849998 36.130001 43603700.0 32.038334 aapl[:3] High Low Open Close Volume Adj Close Date 2014-01-02 79.575714 78.860001 79.382858 79.018570 58671200.0 67.251503 2014-01-03 79.099998 77.204285 78.980003 77.282860 98116900.0 65.774300 2014-01-06 78.114288 76.228569 76.778572 77.704285 103152700.0 66.132957 # Seleccionando solo el Adj Close price de Enero 2014 msft01 = msft[\u0026#39;2014-01\u0026#39;][[\u0026#39;Close\u0026#39;]] aapl01 = aapl[\u0026#39;2014-01\u0026#39;][[\u0026#39;Close\u0026#39;]] msft01[:3] Close Date 2014-01-02 37.160000 2014-01-03 36.910000 2014-01-06 36.130001 aapl01.head() # head() nos muestra los primeros 5 registros Close Date 2014-01-02 79.018570 2014-01-03 77.282860 2014-01-06 77.704285 2014-01-07 77.148575 2014-01-08 77.637146 # tambien se puede seleccionar un rango de tiempo msft[\u0026#39;2014-02\u0026#39;:\u0026#39;2014-02-13\u0026#39;] # desde el 1 al 13 de febrero High Low Open Close Volume Adj Close Date 2014-02-03 37.990002 36.430000 37.740002 36.480000 64063100.0 32.348686 2014-02-04 37.189999 36.250000 36.970001 36.349998 54697900.0 32.233410 2014-02-05 36.470001 35.799999 36.290001 35.820000 55814400.0 31.763430 2014-02-06 36.250000 35.689999 35.799999 36.180000 35351800.0 32.082653 2014-02-07 36.590000 36.009998 36.320000 36.560001 33260500.0 32.419632 2014-02-10 36.799999 36.290001 36.630001 36.799999 26767000.0 32.632454 2014-02-11 37.259998 36.860001 36.880001 37.169998 32141400.0 32.960541 2014-02-12 37.599998 37.299999 37.349998 37.470001 27051800.0 33.226574 2014-02-13 37.860001 37.330002 37.330002 37.610001 37635500.0 33.350716 # combinando ambos resultados close = pd.concat([msft01, aapl01], keys=[\u0026#39;MSFT\u0026#39;, \u0026#39;AAPL\u0026#39;]) close[:5] Close Date MSFT 2014-01-02 37.160000 2014-01-03 36.910000 2014-01-06 36.130001 2014-01-07 36.410000 2014-01-08 35.759998 # seleccionando los primeros 5 registros de AAPL close.loc[\u0026#39;AAPL\u0026#39;][:5] Close Date 2014-01-02 79.018570 2014-01-03 77.282860 2014-01-06 77.704285 2014-01-07 77.148575 2014-01-08 77.637146 # insertando una nueva columna con el simbolo msft.insert(0, \u0026#39;Symbol\u0026#39;, \u0026#39;MSFT\u0026#39;) aapl.insert(0, \u0026#39;Symbol\u0026#39;, \u0026#39;AAPL\u0026#39;) msft.head() Symbol High Low Open Close Volume Adj Close Date 2014-01-02 MSFT 37.400002 37.099998 37.349998 37.160000 30632200.0 32.951675 2014-01-03 MSFT 37.220001 36.599998 37.200001 36.910000 31134800.0 32.729988 2014-01-06 MSFT 36.889999 36.110001 36.849998 36.130001 43603700.0 32.038334 2014-01-07 MSFT 36.490002 36.209999 36.330002 36.410000 35802800.0 32.286613 2014-01-08 MSFT 36.139999 35.580002 36.000000 35.759998 59971700.0 31.710232 # concatenando toda la informaci√≥n y reseteando el indice combinado = pd.concat([msft, aapl]).sort_index() datos_todo = combinado.reset_index() datos_todo.head() Date Symbol High Low Open Close Volume Adj Close 0 2014-01-02 MSFT 37.400002 37.099998 37.349998 37.160000 30632200.0 32.951675 1 2014-01-02 AAPL 79.575714 78.860001 79.382858 79.018570 58671200.0 67.251503 2 2014-01-03 MSFT 37.220001 36.599998 37.200001 36.910000 31134800.0 32.729988 3 2014-01-03 AAPL 79.099998 77.204285 78.980003 77.282860 98116900.0 65.774300 4 2014-01-06 MSFT 36.889999 36.110001 36.849998 36.130001 43603700.0 32.038334 # Armando una tabla pivot del precio de cierre pivot = datos_todo.pivot(index=\u0026#39;Date\u0026#39;, columns=\u0026#39;Symbol\u0026#39;, values=\u0026#39;Close\u0026#39;) pivot.head() Symbol AAPL MSFT Date 2014-01-02 79.018570 37.160000 2014-01-03 77.282860 36.910000 2014-01-06 77.704285 36.130001 2014-01-07 77.148575 36.410000 2014-01-08 77.637146 35.759998 # Obteniendo datos de multiples empresas def all_stocks(symbols, start, end): def data(symbols): return web.DataReader(symbols, \u0026#39;yahoo\u0026#39;, start, end) datas = map(data, symbols) return pd.concat(datas, keys=symbols, names=[\u0026#39;symbols\u0026#39;,\u0026#39;Date\u0026#39;]) simbolos = [\u0026#39;AAPL\u0026#39;,\u0026#39;MSFT\u0026#39;,\u0026#39;GOOG\u0026#39;,\u0026#39;IBM\u0026#39;] all_data = all_stocks(simbolos, inicio, fin) all_data.head() High Low Open Close Volume Adj Close symbols Date AAPL 2014-01-02 79.575714 78.860001 79.382858 79.018570 58671200.0 67.251503 2014-01-03 79.099998 77.204285 78.980003 77.282860 98116900.0 65.774300 2014-01-06 78.114288 76.228569 76.778572 77.704285 103152700.0 66.132957 2014-01-07 77.994286 76.845711 77.760002 77.148575 79302300.0 65.660004 2014-01-08 77.937141 76.955711 76.972855 77.637146 64632400.0 66.075813 all_data.loc[\u0026#39;GOOG\u0026#39;].head() # informaci√≥n de google High Low Open Close Volume Adj Close Date 2014-01-02 555.263550 550.549194 554.125916 552.963501 3666400.0 552.963501 2014-01-03 554.856201 548.894958 553.897461 548.929749 3355000.0 548.929749 2014-01-06 555.814941 549.645081 552.908875 555.049927 3561600.0 555.049927 2014-01-07 566.162659 556.957520 558.865112 565.750366 5138400.0 565.750366 2014-01-08 569.953003 562.983337 569.297241 566.927673 4514100.0 566.927673 # Graficando los datos. solo_cierre = all_data[[\u0026#39;Close\u0026#39;]].reset_index() pivot_cierre = solo_cierre.pivot(\u0026#39;Date\u0026#39;, \u0026#39;symbols\u0026#39;, \u0026#39;Close\u0026#39;) pivot_cierre.head() symbols AAPL GOOG IBM MSFT Date 2014-01-02 79.018570 552.963501 185.529999 37.160000 2014-01-03 77.282860 548.929749 186.639999 36.910000 2014-01-06 77.704285 555.049927 186.000000 36.130001 2014-01-07 77.148575 565.750366 189.710007 36.410000 2014-01-08 77.637146 566.927673 187.970001 35.759998 # Graficando la informaci√≥n de Apple plot=pivot_cierre[\u0026#39;AAPL\u0026#39;].plot(figsize=(12,8)) # Graficando todos plot = pivot_cierre.plot(figsize=(12,8)) Como podemos ver Pandas es una librer√≠a muy vers√°til, con ella podemos hacer todo tipo de manipulaciones de datos, desde obtener los datos desde la web hasta realizar concatenaciones, tablas pivot o incluso realizar gr√°ficos.\nCon esto termino esta introducci√≥n a finanzas con Python; los dejo para que se entretengan con sus propios ejemplos, a practicar!\nSaludos!\nEste post fue escrito utilizando IPython notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2015-08-28","id":36,"permalink":"/blog/2015/08/28/introduccion-a-finanzas-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# En el vertiginoso mundo actual de las finanzas; d√≥nde la velocidad, frecuencia y volumen de los datos aumentan a un ritmo considerable; la aplicaci√≥n combinada de tecnolog√≠a y software, junto con algoritmos avanzados y diferentes m√©todos para recopilar, procesar y analizar datos se ha vuelto fundamental para obtener la informaci√≥n necesaria para una eficiente toma de decisiones.","tags":["python","matematica","calculo","finanazas","analisis de datos"],"title":"Introducci√≥n a Finanzas con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nBatman siempre fue mi superh√©roe favorito porque es uno de los pocos h√©roes que no posee ning√∫n superpoder, sino que debe recurrir a su intelecto y a la ciencia para construir las bati-herramientas que utiliza para combatir al crimen. Adem√°s posee ese toque de oscuridad producto de la dualidad entre realizar el bien, protegiendo a la gente de ciudad g√≥tica, y su sed de venganza contra el crimen y la corrupci√≥n que acab√≥ con la vida de su familia.\nEs un personaje con muchos recursos, en cada nueva aparici√≥n podemos verlo utilizar nuevas y muy modernas bati-herramientas; su intelecto es tan agudo que incluso escondi√≥ una ecuaci√≥n matem√°tica en su bati-se√±al!!\nLa ecuaci√≥n de batman fue creada por el profesor de matem√°ticas Matthew Register y se populariz√≥ a trav√©s de un post de uno de sus alumnos en la red social reddit; su expresi√≥n matem√°tica es la siguiente:\n$$ \\begin{eqnarray} ((\\frac{x}{7})^2 \\cdot \\sqrt{\\frac{||x|-3|}{(|x|-3)}}+ (\\frac{y}{3})^2 \\cdot \\sqrt{\\frac{|y+3 \\cdot \\frac{\\sqrt{33}}{7}|}{y+3 \\cdot \\frac{\\sqrt{33}}{7}}}-1) \\cdot (|\\frac{x}{2}|-((3 \\cdot \\frac{\\sqrt{33}-7)}{112}) \\\\\\ \\cdot x^2-3+\\sqrt{1-(||x|-2|-1)^2}-y) \\cdot (9 \\cdot \\sqrt{\\frac{|(|x|-1) \\cdot (|x|-0.75)|}{((1-|x|)*(|x|-0.75))}}-8 \\\\\\ \\cdot |x|-y) \\cdot (3 \\cdot |x|+0.75 \\cdot \\sqrt{\\frac{|(|x|-0.75) \\cdot (|x|-0.5)|}{((0.75-|x|) \\cdot (|x|-0.5))}}-y) \\cdot \\\\\\ (2.25 \\cdot \\sqrt{\\frac{|(x-0.5) \\cdot (x+0.5)|}{((0.5-x) \\cdot (0.5+x))}}-y) \\cdot (\\frac{6 \\cdot \\sqrt{10}}{7}+(1.5-0.5 \\cdot |x|) \\\\\\ \\cdot \\sqrt{\\frac{||x|-1|}{|x|-1}}-(\\frac{6 \\cdot \\sqrt{10}}{14}) \\cdot \\sqrt{4-(|x|-1)^2}-y) =0 \\end{eqnarray} $$ Si bien a simple vista la ecuaci√≥n parece sumamente compleja e imposible de graficar, la misma se puede descomponer en seis curvas distintas, mucho m√°s simples.\nLa primera de estas curvas, es la funci√≥n del elipse \\((\\frac{x}{7})^2 + (\\frac{y}{3})^2 = 1\\), restringida a la regi√≥n \\(\\sqrt{\\frac{||x|-3|}{(|x|-3)}}\\) y \\(\\sqrt{\\frac{|y+3 \\cdot \\frac{\\sqrt{33}}{7}|}{y+3 \\cdot \\frac{\\sqrt{33}}{7}}}\\) para cortar la parte central.\nLos cinco t√©rminos siguientes pueden ser entendidos como simples funciones de x, tres de los cuales son lineales. Por ejemplo, la siguiente funci√≥n es la que grafica las curvas de la parte inferior de la bati-se√±al.\n\\(y = |\\frac{x}{2}|-(\\frac{3 \\cdot \\sqrt{33} -7}{112})\\cdot x^2 - 3 + \\sqrt{1-(||x|-2| -1)^2}\\)\nLas restantes ecuaciones de las curvas que completan el gr√°fico, son las siguientes:\n\\(y = \\frac{6\\cdot\\sqrt{10}}{7} + (-0.5|x| + 1.5) - \\frac{3\\cdot\\sqrt{10}}{7}\\cdot\\sqrt{4 - (|x|-1)^2}, |x| \u0026gt; 1\\)\n\\(y = 9 -8|x|, 0.75 \u0026lt; |x| \u0026lt; 1\\)\n\\(y = 3|x| + 0.75, 0.5 \u0026lt; |x| \u0026lt; 0.75\\)\n\\(y = 2.25, |x| \u0026lt; 0.5\\)\nLa ecuaci√≥n de batman puede ser f√°cilmente graficada utilizando Matplotlib del siguiente modo:\n# graficos embebidos %matplotlib inline # Importando lo necesario para los c√°lculos import matplotlib.pyplot as plt from numpy import sqrt from numpy import meshgrid from numpy import arange # Graficando la ecuaci√≥n de Batman. xs = arange(-7.25, 7.25, 0.01) ys = arange(-5, 5, 0.01) x, y = meshgrid(xs, ys) eq1 = ((x/7)**2*sqrt(abs(abs(x)-3)/(abs(x)-3))+(y/3)**2*sqrt(abs(y+3/7*sqrt(33))/(y+3/7*sqrt(33)))-1) eq2 = (abs(x/2)-((3*sqrt(33)-7)/112)*x**2-3+sqrt(1-(abs(abs(x)-2)-1)**2)-y) eq3 = (9*sqrt(abs((abs(x)-1)*(abs(x)-.75))/((1-abs(x))*(abs(x)-.75)))-8*abs(x)-y) eq4 = (3*abs(x)+.75*sqrt(abs((abs(x)-.75)*(abs(x)-.5))/((.75-abs(x))*(abs(x)-.5)))-y) eq5 = (2.25*sqrt(abs((x-.5)*(x+.5))/((.5-x)*(.5+x)))-y) eq6 = (6*sqrt(10)/7+(1.5-.5*abs(x))*sqrt(abs(abs(x)-1)/(abs(x)-1))-(6*sqrt(10)/14)*sqrt(4-(abs(x)-1)**2)-y) for f, c in [(eq1, \u0026#34;red\u0026#34;), (eq2, \u0026#34;purple\u0026#34;), (eq3, \u0026#34;green\u0026#34;), (eq4, \u0026#34;blue\u0026#34;), (eq5, \u0026#34;orange\u0026#34;), (eq6, \u0026#34;black\u0026#34;)]: plt.contour(x, y, f, [0], colors=c) plt.show() Ahora ya saben\u0026hellip;si est√°n en alg√∫n apuro y necesitan la ayuda del bati-h√©roe, solo necesitan graficar una ecuaci√≥n para llamarlo con la bati-se√±al!\nSaludos!\nEste post fue escrito utilizando IPython notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2015-08-20","id":37,"permalink":"/blog/2015/08/20/batman-ecuaciones-y-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nBatman siempre fue mi superh√©roe favorito porque es uno de los pocos h√©roes que no posee ning√∫n superpoder, sino que debe recurrir a su intelecto y a la ciencia para construir las bati-herramientas que utiliza para combatir al crimen. Adem√°s posee ese toque de oscuridad producto de la dualidad entre realizar el bien, protegiendo a la gente de ciudad g√≥tica, y su sed de venganza contra el crimen y la corrupci√≥n que acab√≥ con la vida de su familia.","tags":["python","matematica","batman"],"title":"Batman, ecuaciones y python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Uno de los problemas m√°s comunes con que nos solemos encontrar al desarrollar cualquier programa inform√°tico, es el de procesamiento de texto. Esta tarea puede resultar bastante trivial para el cerebro humano, ya que nosotros podemos detectar con facilidad que es un n√∫mero y que una letra, o cuales son palabras que cumplen con un determinado patr√≥n y cuales no; pero estas mismas tareas no son tan f√°ciles para una computadora. Es por esto, que el procesamiento de texto siempre ha sido uno de los temas m√°s relevantes en las ciencias de la computaci√≥n. Luego de varias d√©cadas de investigaci√≥n se logr√≥ desarrollar un poderoso y vers√°til lenguaje que cualquier computadora puede utilizar para reconocer patrones de texto; este lenguale es lo que hoy en d√≠a se conoce con el nombre de expresiones regulares; las operaciones de validaci√≥n, b√∫squeda, extracci√≥n y sustituci√≥n de texto ahora son tareas mucho m√°s sencillas para las computadoras gracias a las expresiones regulares.\n¬øQu√© son las Expresiones Regulares?# Las expresiones regulares, a menudo llamada tambi√©n regex, son unas secuencias de caracteres que forma un patr√≥n de b√∫squeda, las cuales son formalizadas por medio de una sintaxis espec√≠fica. Los patrones se interpretan como un conjunto de instrucciones, que luego se ejecutan sobre un texto de entrada para producir un subconjunto o una versi√≥n modificada del texto original. Las expresiones regulares pueden incluir patrones de coincidencia literal, de repetici√≥n, de composici√≥n, de ramificaci√≥n, y otras sofisticadas reglas de reconocimiento de texto . Las expresiones regulares deber√≠an formar parte del arsenal de cualquier buen programador ya que un gran n√∫mero de problemas de procesamiento de texto pueden ser f√°cilmente resueltos con ellas.\nComponentes de las Expresiones Regulares# Las expresiones regulares son un mini lenguaje en s√≠ mismo, por lo que para poder utilizarlas eficientemente primero debemos entender los componentes de su sintaxis; ellos son:\nLiterales: Cualquier caracter se encuentra a s√≠ mismo, a menos que se trate de un metacaracter con significado especial. Una serie de caracteres encuentra esa misma serie en el texto de entrada, por lo tanto la plantilla \u0026ldquo;raul\u0026rdquo; encontrar√° todas las apariciones de \u0026ldquo;raul\u0026rdquo; en el texto que procesamos.\nSecuencias de escape: La sintaxis de las expresiones regulares nos permite utilizar las secuencias de escape que ya conocemos de otros lenguajes de programaci√≥n para esos casos especiales como ser finales de l√≠nea, tabs, barras diagonales, etc. Las principales secuencias de escape que podemos encontrar, son:\nSecuencia de escape Significado \\n Nueva l√≠nea (new line). El cursor pasa a la primera posici√≥n de la l√≠nea siguiente. \\t Tabulador. El cursor pasa a la siguiente posici√≥n de tabulaci√≥n. \\\\ Barra diagonal inversa \\v Tabulaci√≥n vertical. \\ooo Car√°cter ASCII en notaci√≥n octal. \\xhh Car√°cter ASCII en notaci√≥n hexadecimal. \\xhhhh Car√°cter Unicode en notaci√≥n hexadecimal. Clases de caracteres: Se pueden especificar clases de caracteres encerrando una lista de caracteres entre corchetes [], la que que encontrar√° uno cualquiera de los caracteres de la lista. Si el primer s√≠mbolo despu√©s del \u0026ldquo;[\u0026rdquo; es \u0026ldquo;^\u0026rdquo;, la clase encuentra cualquier caracter que no est√° en la lista.\nMetacaracteres: Los metacaracteres son caracteres especiales que son la esencia de las expresiones regulares. Como son sumamente importantes para entender la sintaxis de las expresiones regulares y existen diferentes tipos, voy a dedicar una secci√≥n a explicarlos un poco m√°s en detalle.\nMetacaracteres# Metacaracteres - delimitadores# Esta clase de metacaracteres nos permite delimitar d√≥nde queremos buscar los patrones de b√∫squeda. Ellos son:\nMetacaracter Descripci√≥n ^ inicio de l√≠nea. $ fin de l√≠nea. \\A inicio de texto. \\Z fin de texto. . cualquier caracter en la l√≠nea. \\b encuentra l√≠mite de palabra. \\B encuentra distinto a l√≠mite de palabra. Metacaracteres - clases predefinidas# Estas son clases predefinidas que nos facilitan la utilizaci√≥n de las expresiones regulares. Ellos son:\nMetacaracter Descripci√≥n \\w un caracter alfanum√©rico (incluye \u0026ldquo;_\u0026rdquo;). \\W un caracter no alfanum√©rico. \\d un caracter num√©rico. \\D un caracter no num√©rico. \\s cualquier espacio (lo mismo que [ \\t\\n\\r\\f]). \\S un no espacio. Metacaracteres - iteradores# Cualquier elemento de una expresion regular puede ser seguido por otro tipo de metacaracteres, los iteradores. Usando estos metacaracteres se puede especificar el n√∫mero de ocurrencias del caracter previo, de un metacaracter o de una subexpresi√≥n. Ellos son:\nMetacaracter Descripci√≥n * cero o m√°s, similar a {0,}. + una o m√°s, similar a {1,}. ? cero o una, similar a {0,1}. {n} exactamente n veces. {n,} por lo menos n veces. {n,m} por lo menos n pero no m√°s de m veces. *? cero o m√°s, similar a {0,}?. +? una o m√°s, similar a {1,}?. ?? cero o una, similar a {0,1}?. {n}? exactamente n veces. {n,}? por lo menos n veces. {n,m}? por lo menos n pero no m√°s de m veces. En estos metacaracteres, los d√≠gitos entre llaves de la forma {n,m}, especifican el m√≠nimo n√∫mero de ocurrencias en n y el m√°ximo en m.\nMetacaracteres - alternativas# Se puede especificar una serie de alternativas para una plantilla usando \u0026ldquo;|\u0026rdquo; para separarlas, entonces do|re|mi encontrar√° cualquier \u0026ldquo;do\u0026rdquo;, \u0026ldquo;re\u0026rdquo;, o \u0026ldquo;mi\u0026rdquo; en el texto de entrada.Las alternativas son evaluadas de izquierda a derecha, por lo tanto la primera alternativa que coincide plenamente con la expresi√≥n analizada es la que se selecciona. Por ejemplo: si se buscan foo|foot en \u0026ldquo;barefoot\u0026rsquo;\u0026rsquo;, s√≥lo la parte \u0026ldquo;foo\u0026rdquo; da resultado positivo, porque es la primera alternativa probada, y porque tiene √©xito en la b√∫squeda de la cadena analizada.\nEjemplo:\nfoo(bar|foo) \u0026ndash;\u0026gt; encuentra las cadenas \u0026lsquo;foobar\u0026rsquo; o \u0026lsquo;foofoo\u0026rsquo;.\nMetacaracteres - subexpresiones# La construcci√≥n ( \u0026hellip; ) tambi√©n puede ser empleada para definir subexpresiones de expresiones regulares.\nEjemplos:\n(foobar){10} \u0026ndash;\u0026gt; encuentra cadenas que contienen 8, 9 o 10 instancias de \u0026lsquo;foobar\u0026rsquo;\nfoob([0-9]|a+)r \u0026ndash;\u0026gt; encuentra \u0026lsquo;foob0r\u0026rsquo;, \u0026lsquo;foob1r\u0026rsquo; , \u0026lsquo;foobar\u0026rsquo;, \u0026lsquo;foobaar\u0026rsquo;, \u0026lsquo;foobaar\u0026rsquo; etc.\nMetacaracteres - memorias (backreferences)# Los metacaracteres \\1 a \\9 son interpretados como memorias. \u0026lt;n\u0026gt; encuentra la subexpresi√≥n previamente encontrada #.\nEjemplos:\n(.)\\1+ \u0026ndash;\u0026gt; encuentra \u0026lsquo;aaaa\u0026rsquo; y \u0026lsquo;cc\u0026rsquo;.\n(.+)\\1+ \u0026ndash;\u0026gt; tambi√©n encuentra \u0026lsquo;abab\u0026rsquo; y \u0026lsquo;123123\u0026rsquo;\n([\u0026rsquo;\u0026rdquo;]?)(\\d+)\\1 \u0026ndash;\u0026gt; encuentra \u0026lsquo;\u0026ldquo;13\u0026rdquo; (entre comillas dobles), o \u0026lsquo;4\u0026rsquo; (entre comillas simples) o 77 (sin comillas) etc.\nExpresiones Regulares con Python# Luego de esta introducci√≥n, lleg√≥ el tiempo de empezar a jugar con las expresiones regulares y Python.\nComo no podr√≠a ser de otra forma tratandose de Python y su filosof√≠a de todas las bater√≠as inclu√≠das; en la librer√≠a estandar de Python podemos encontrar el m√≥dulo re, el cual nos proporciona todas las operaciones necesarias para trabajar con las expresiones regulares.\nPor lo tanto, en primer lugar lo que debemos hacer es importar el modulo re.\n# importando el modulo de regex de python import re Buscando coincidencias# Una vez que hemos importado el m√≥dulo, podemos empezar a tratar de buscar coincidencias con un determinado patr√≥n de b√∫squeda. Para hacer esto, primero debemos compilar nuestra expresion regular en un objeto de patrones de Python, el cual posee m√©todos para diversas operaciones, tales como la b√∫squeda de coincidencias de patrones o realizar sustituciones de texto.\n# compilando la regex patron = re.compile(r\u0026#39;\\bfoo\\b\u0026#39;) # busca la palabra foo Ahora que ya tenemos el objeto de expresi√≥n regular compilado podemos utilizar alguno de los siguientes m√©todos para buscar coincidencias con nuestro texto.\nmatch(): El cual determinada si la regex tiene coincidencias en el comienzo del texto. search(): El cual escanea todo el texto buscando cualquier ubicaci√≥n donde haya una coincidencia. findall(): El cual encuentra todos los subtextos donde haya una coincidencia y nos devuelve estas coincidencias como una lista. finditer(): El cual es similar al anterior pero en lugar de devolvernos una lista nos devuelve un iterador. Veamoslos en acci√≥n.\n# texto de entrada texto = \u0026#34;\u0026#34;\u0026#34; bar foo bar foo barbarfoo foofoo foo bar \u0026#34;\u0026#34;\u0026#34; # match nos devuelve None porque no hubo coincidencia al comienzo del texto print(patron.match(texto)) None # match encuentra una coindencia en el comienzo del texto m = patron.match(\u0026#39;foo bar\u0026#39;) m \u0026lt;_sre.SRE_Match object; span=(0, 3), match='foo'\u0026gt; # search nos devuelve la coincidencia en cualquier ubicacion. s = patron.search(texto) s \u0026lt;_sre.SRE_Match object; span=(5, 8), match='foo'\u0026gt; # findall nos devuelve una lista con todas las coincidencias fa = patron.findall(texto) fa ['foo', 'foo', 'foo'] # finditer nos devuelve un iterador fi = patron.finditer(texto) fi \u0026lt;callable_iterator at 0x7f413db74240\u0026gt; # iterando por las distintas coincidencias next(fi) \u0026lt;_sre.SRE_Match object; span=(5, 8), match='foo'\u0026gt; next(fi) \u0026lt;_sre.SRE_Match object; span=(13, 16), match='foo'\u0026gt; Como podemos ver en estos ejemplos, cuando hay coincidencias, Python nos devuelve un Objeto de coincidencia (salvo por el m√©todo findall() que devuelve una lista). Este Objeto de coincidencia tambi√©n tiene sus propios m√©todos que nos proporcionan informaci√≥n adicional sobre la coincidencia; √©stos m√©todos son:\ngroup(): El cual devuelve el texto que coincide con la expresion regular. start(): El cual devuelve la posici√≥n inicial de la coincidencia. end(): El cual devuelve la posici√≥n final de la coincidencia. span(): El cual devuelve una tupla con la posici√≥n inicial y final de la coincidencia. # M√©todos del objeto de coincidencia m.group(), m.start(), m.end(), m.span() ('foo', 0, 3, (0, 3)) s.group(), s.start(), s.end(), s.span() ('foo', 5, 8, (5, 8)) Modificando el texto de entrada# Adem√°s de buscar coincidencias de nuestro patr√≥n de b√∫squeda en un texto, podemos utilizar ese mismo patr√≥n para realizar modificaciones al texto de entrada. Para estos casos podemos utilizar los siguientes m√©todos:\nsplit():\tEl cual divide el texto en una lista, realizando las divisiones del texto en cada lugar donde se cumple con la expresion regular. sub(): El cual encuentra todos los subtextos donde existe una coincidencia con la expresion regular y luego los reemplaza con un nuevo texto. subn(): El cual es similar al anterior pero adem√°s de devolver el nuevo texto, tambi√©n devuelve el numero de reemplazos que realiz√≥. Veamoslos en acci√≥n.\n# texto de entrada becquer = \u0026#34;\u0026#34;\u0026#34;Podr√° nublarse el sol eternamente; Podr√° secarse en un instante el mar; Podr√° romperse el eje de la tierra como un d√©bil cristal. ¬°todo suceder√°! Podr√° la muerte cubrirme con su f√∫nebre cresp√≥n; Pero jam√°s en m√≠ podr√° apagarse la llama de tu amor.\u0026#34;\u0026#34;\u0026#34; # patron para dividir donde no encuentre un caracter alfanumerico patron = re.compile(r\u0026#39;\\W+\u0026#39;) palabras = patron.split(becquer) palabras[:10] # 10 primeras palabras ['Podr√°', 'nublarse', 'el', 'sol', 'eternamente', 'Podr√°', 'secarse', 'en', 'un', 'instante'] # Utilizando la version no compilada de split. re.split(r\u0026#39;\\n\u0026#39;, becquer) # Dividiendo por linea. ['Podr√° nublarse el sol eternamente; ', 'Podr√° secarse en un instante el mar; ', 'Podr√° romperse el eje de la tierra ', 'como un d√©bil cristal. ', '¬°todo suceder√°! Podr√° la muerte ', 'cubrirme con su f√∫nebre cresp√≥n; ', 'Pero jam√°s en m√≠ podr√° apagarse ', 'la llama de tu amor.'] # Utilizando el tope de divisiones patron.split(becquer, 5) ['Podr√°', 'nublarse', 'el', 'sol', 'eternamente', 'Podr√° secarse en un instante el mar; \\nPodr√° romperse el eje de la tierra \\ncomo un d√©bil cristal. \\n¬°todo suceder√°! Podr√° la muerte \\ncubrirme con su f√∫nebre cresp√≥n; \\nPero jam√°s en m√≠ podr√° apagarse \\nla llama de tu amor.'] # Cambiando \u0026#34;Podr√°\u0026#34; o \u0026#34;podra\u0026#34; por \u0026#34;Puede\u0026#34; podra = re.compile(r\u0026#39;\\b(P|p)odr√°\\b\u0026#39;) puede = podra.sub(\u0026#34;Puede\u0026#34;, becquer) print(puede) Puede nublarse el sol eternamente; Puede secarse en un instante el mar; Puede romperse el eje de la tierra como un d√©bil cristal. ¬°todo suceder√°! Puede la muerte cubrirme con su f√∫nebre cresp√≥n; Pero jam√°s en m√≠ Puede apagarse la llama de tu amor. # Limitando el n√∫mero de reemplazos puede = podra.sub(\u0026#34;Puede\u0026#34;, becquer, 2) print(puede) Puede nublarse el sol eternamente; Puede secarse en un instante el mar; Podr√° romperse el eje de la tierra como un d√©bil cristal. ¬°todo suceder√°! Podr√° la muerte cubrirme con su f√∫nebre cresp√≥n; Pero jam√°s en m√≠ podr√° apagarse la llama de tu amor. # Utilizando la version no compilada de subn re.subn(r\u0026#39;\\b(P|p)odr√°\\b\u0026#39;, \u0026#34;Puede\u0026#34;, becquer) # se realizaron 5 reemplazos ('Puede nublarse el sol eternamente; \\nPuede secarse en un instante el mar; \\nPuede romperse el eje de la tierra \\ncomo un d√©bil cristal. \\n¬°todo suceder√°! Puede la muerte \\ncubrirme con su f√∫nebre cresp√≥n; \\nPero jam√°s en m√≠ Puede apagarse \\nla llama de tu amor.', 5) Funciones no compiladas# En estos √∫ltimos ejemplos, pudimos ver casos donde utilizamos las funciones al nivel del m√≥dulo split() y subn(). Para cada uno de los ejemplos que vimos (match, search, findall, finditer, split, sub y subn) existe una versi√≥n al nivel del m√≥dulo que se puede utilizar sin necesidad de compilar primero el patr√≥n de b√∫squeda; simplemente le pasamos como primer argumento la expresion regular y el resultado ser√° el mismo. La ventaja que tiene la versi√≥n compila sobre las funciones no compiladas es que si vamos a utilizar la expresion regular dentro de un bucle nos vamos a ahorrar varias llamadas de funciones y por lo tanto mejorar la performance de nuestro programa.\n# Ejemplo de findall con la funcion a nivel del modulo # findall nos devuelve una lista con todas las coincidencias re.findall(r\u0026#39;\\bfoo\\b\u0026#39;, texto) ['foo', 'foo', 'foo'] Banderas de compilaci√≥n# Las banderas de compilaci√≥n permiten modificar algunos aspectos de c√≥mo funcionan las expresiones regulares. Todas ellas est√°n disponibles en el m√≥dulo re bajo dos nombres, un nombre largo como IGNORECASE y una forma abreviada de una sola letra como I. M√∫ltiples banderas pueden ser especificadas utilizando el operador \u0026ldquo;|\u0026rdquo; OR; Por ejemplo, re.I | RE.M establece las banderas de E y M.\nAlgunas de las banderas de compilaci√≥n que podemos encontrar son:\nIGNORECASE, I: Para realizar b√∫squedas sin tener en cuenta las min√∫sculas o may√∫sculas. VERBOSE, X: Que habilita la modo verborr√°gico, el cual permite organizar el patr√≥n de b√∫squeda de una forma que sea m√°s sencilla de entender y leer. ASCII, A: Que hace que las secuencias de escape \\w, \\b, \\s and \\d funciones para coincidencias con los caracteres ASCII. DOTALL, S: La cual hace que el metacaracter . funcione para cualquier caracter, incluyendo el las l√≠neas nuevas. LOCALE, L: Esta opci√≥n hace que \\w, \\W, \\b, \\B, \\s, y \\S dependientes de la localizaci√≥n actual. MULTILINE, M: Que habilita la coincidencia en m√∫ltiples l√≠neas, afectando el funcionamiento de los metacaracteres ^ and $. # Ejemplo de IGNORECASE # Cambiando \u0026#34;Podr√°\u0026#34; o \u0026#34;podra\u0026#34; por \u0026#34;Puede\u0026#34; podra = re.compile(r\u0026#39;podr√°\\b\u0026#39;, re.I) # el patr√≥n se vuelve m√°s sencillo puede = podra.sub(\u0026#34;puede\u0026#34;, becquer) print(puede) puede nublarse el sol eternamente; puede secarse en un instante el mar; puede romperse el eje de la tierra como un d√©bil cristal. ¬°todo suceder√°! puede la muerte cubrirme con su f√∫nebre cresp√≥n; Pero jam√°s en m√≠ puede apagarse la llama de tu amor. # Ejemplo de VERBOSE mail = re.compile(r\u0026#34;\u0026#34;\u0026#34; \\b # comienzo de delimitador de palabra [\\w.%+-] # usuario: Cualquier caracter alfanumerico mas los signos (.%+-) +@ # seguido de @ [\\w.-] # dominio: Cualquier caracter alfanumerico mas los signos (.-) +\\. # seguido de . [a-zA-Z]{2,6} # dominio de alto nivel: 2 a 6 letras en min√∫sculas o may√∫sculas. \\b # fin de delimitador de palabra \u0026#34;\u0026#34;\u0026#34;, re.X) mails = \u0026#34;\u0026#34;\u0026#34;raul.lopez@relopezbriega.com, Raul Lopez Briega, foo bar, relopezbriega@relopezbriega.com.ar, raul@github.io, https://relopezbriega.com.ar, https://relopezbriega.github.io, python@python, river@riverplate.com.ar, pythonAR@python.pythonAR \u0026#34;\u0026#34;\u0026#34; # filtrando los mails con estructura v√°lida mail.findall(mails) ['raul.lopez@relopezbriega.com', 'relopezbriega@relopezbriega.com.ar', 'raul@github.io', 'river@riverplate.com.ar'] Como podemos ver en este √∫ltimo ejemplo, la opci√≥n VERBOSE puede ser muy util para que cualquier persona que lea nuestra expresion regular pueda entenderla m√°s f√°cilmente.\nNombrando los grupos# Otra de las funciones interesantes que nos ofrece el m√≥dulo re de Python; es la posibilidad de ponerle nombres a los grupos de nuestras expresiones regulares. As√≠ por ejemplo, en lugar de acceder a los grupos por sus √≠ndices, como en este caso\u0026hellip;\n# Accediendo a los grupos por sus indices patron = re.compile(r\u0026#34;(\\w+) (\\w+)\u0026#34;) s = patron.search(\u0026#34;Raul Lopez\u0026#34;) # grupo 1 s.group(1) 'Raul' # grupo 2 s.group(2) 'Lopez' Podemos utilizar la sintaxis especial (?P\u0026lt;nombre\u0026gt;patron) que nos ofrece Python para nombrar estos grupos y que sea m√°s f√°cil identificarlos.\n# Accediendo a los grupos por nombres patron = re.compile(r\u0026#34;(?P\u0026lt;nombre\u0026gt;\\w+) (?P\u0026lt;apellido\u0026gt;\\w+)\u0026#34;) s = patron.search(\u0026#34;Raul Lopez\u0026#34;) # grupo nombre s.group(\u0026#34;nombre\u0026#34;) 'Raul' # grupo apellido s.group(\u0026#34;apellido\u0026#34;) 'Lopez' Otros ejemplos de expresiones regulares# Por √∫ltimo, para ir cerrando esta introducci√≥n a las expresiones regulares, les dejo algunos ejemplos de las expresiones regulares m√°s utilizadas.\nValidando mails# Para validar que un mail tenga la estructura correcta, podemos utilizar la siguiente expresion regular:\nregex: \\b[\\w.%+-]+@[\\w.-]+\\.[a-zA-Z]{2,6}\\b\nEste es el patr√≥n que utilizamos en el ejemplo de la opci√≥n VERBOSE.\nValidando una URL# Para validar que una URL tenga una estructura correcta, podemos utilizar esta expresion regular:\nregex: ^(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)*\\/?$\n# Validando una URL url = re.compile(r\u0026#34;^(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)*\\/?$\u0026#34;) # vemos que https://relopezbriega.com.ar lo acepta como una url v√°lida. url.search(\u0026#34;https://relopezbriega.com.ar\u0026#34;) \u0026lt;_sre.SRE_Match object; span=(0, 27), match='https://relopezbriega.com.ar'\u0026gt; # pero https://google.com/un/archivo!.html no la acepta por el carcter ! print(url.search(\u0026#34;https://google.com/un/archivo!.html\u0026#34;)) None Validando una direcci√≥n IP# Para validar que una direcci√≥n IP tenga una estructura correcta, podemos utilizar esta expresi√≥n regular:\nregex: ^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$\n# Validando una direcci√≥n IP patron = (\u0026#39;^(?:(?:25[0-5]|2[0-4][0-9]|\u0026#39; \u0026#39;[01]?[0-9][0-9]?)\\.){3}\u0026#39; \u0026#39;(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$\u0026#39;) ip = re.compile(patron) # la ip 73.60.124.136 es valida ip.search(\u0026#34;73.60.124.136\u0026#34;) \u0026lt;_sre.SRE_Match object; span=(0, 13), match='73.60.124.136'\u0026gt; # pero la ip 256.60.124.136 no es valida print(ip.search(\u0026#34;256.60.124.136\u0026#34;)) None Validando una fecha# Para validar que una fecha tenga una estructura dd/mm/yyyy, podemos utilizar esta expresi√≥n regular:\nregex: ^(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[012])/((19|20)\\d\\d)$\n# Validando una fecha fecha = re.compile(r\u0026#39;^(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[012])/((19|20)\\d\\d)$\u0026#39;) # validando 13/02/1982 fecha.search(\u0026#34;13/02/1982\u0026#34;) \u0026lt;_sre.SRE_Match object; span=(0, 10), match='13/02/1982'\u0026gt; # no valida 13-02-1982 print(fecha.search(\u0026#34;13-02-1982\u0026#34;)) None # no valida 32/12/2015 print(fecha.search(\u0026#34;32/12/2015\u0026#34;)) None # no valida 30/14/2015 print(fecha.search(\u0026#34;30/14/2015\u0026#34;)) None Y con estos ejemplos termino este tutorial, espero que les haya sido de utilidad.\nSaludos!\nEste post fue escrito utilizando IPython notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2015-07-19","id":38,"permalink":"/blog/2015/07/19/expresiones-regulares-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Uno de los problemas m√°s comunes con que nos solemos encontrar al desarrollar cualquier programa inform√°tico, es el de procesamiento de texto. Esta tarea puede resultar bastante trivial para el cerebro humano, ya que nosotros podemos detectar con facilidad que es un n√∫mero y que una letra, o cuales son palabras que cumplen con un determinado patr√≥n y cuales no; pero estas mismas tareas no son tan f√°ciles para una computadora.","tags":["python","regex","programacion","analisis de datos"],"title":"Expresiones Regulares con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nActualmente con el boom de la Big Data, tener nociones de probabilidad y estad√≠stica se ha hecho fundamental. En los √∫ltimos a√±os ha habido un resurgimiento de todo lo relacionado con estad√≠stica , data mining y machine learning empujados principalmente por la explosi√≥n de datos con que contamos, estos conceptos combinados forman la base de lo que actualmente se conoce como la Ciencia de Datos. Dentro de este contexto, Python es uno de los lenguajes que m√°s nos facilita trabajar con datos. Realizar complejos an√°lisis estad√≠sticos nunca fue tan f√°cil como con Python!\n¬øQu√© es la Estad√≠stica?# La estad√≠stica suele ser definida como la ciencia de aprender de los datos o como la ciencia de obtener conclusiones en la presencia de incertidumbre. Se relaciona principalmente con la recolecci√≥n, an√°lisis e interpretaci√≥n de datos, as√≠ como tambi√©n con la efectiva comunicaci√≥n y presentaci√≥n de los resultados basados en esos datos. Como por datos entendemos a cualquier clase de informaci√≥n grabada, la estad√≠stica juego un rol importante en muchas disciplinas cient√≠ficas.\nLa estad√≠stica puede ser muy importante para una efectiva toma de decisiones. Existe una gran cantidad de valiosa informaci√≥n escondida entre los datos, pero esta informaci√≥n no suele ser f√°cilmente accesible, la estad√≠stica nos brinda los principios fundamentales que nos permiten extraer y entender esa informaci√≥n; tambien nos proporciona las herramientas necesarias para verificar la calidad de nuestros datos y nuestra informaci√≥n.\nLa estad√≠stica suele ser dividida en dos grandes ramas:\nLa estad√≠stica descriptiva: La cual se dedica a recolectar, ordenar, analizar y representar a un conjunto de datos, con el fin de describir apropiadamente las caracter√≠sticas de este. Calcula los par√°metros estad√≠sticos que describen el conjunto estudiado. Algunas de las herramientas que utiliza son gr√°ficos, medidas de frecuencias, medidas de centralizaci√≥n, medidas de posici√≥n, medidas de dispersi√≥n, entre otras.\nLa estadistica inferencial: La cual estudia c√≥mo sacar conclusiones generales para toda la poblaci√≥n a partir del estudio de una muestra, y el grado de fiabilidad o significaci√≥n de los resultados obtenidos. Sus principales herramientas son el muestra, la estimaci√≥n de par√°metros y el contraste de hip√≥tesis.\n¬øQu√© es la Probabilidad?# La probabilidad mide la mayor o menor posibilidad de que se d√© un determinado resultado (suceso o evento) cuando se realiza un experimento aleatorio. Para calcular la probabilidad de un evento se toma en cuenta todos los casos posibles de ocurrencia del mismo; es decir, de cu√°ntas formas puede ocurrir determinada situaci√≥n.Los casos favorables de ocurrencia de un evento ser√°n los que cumplan con la condici√≥n que estamos buscando. La probabilidad toma valores entre 0 y 1 (o expresados en tanto por ciento, entre 0% y 100%).\nLa probabilidad es a la vez el inverso y complemento para la estad√≠stica. D√≥nde la estad√≠stica nos ayuda a ir desde los datos observados hasta hacer generalizaciones sobre como funcionan las cosas; la probabilidad funciona en la direcci√≥n inversa: si asumimos que sabemos como las cosas funcionan, entonces podemos averiguar la clase de datos que vamos a ver y cuan probable es que los veamos.\nLa probabilidad tambi√©n funciona como complemento de la estad√≠stica cuando nos proporciona una s√≥lida base para la estadistica inferencial. Cuando hay incertidumbre, no sabemos que puede pasar y hay alguna posibilidad de errores, utilizando probabilidades podemos aprender formas de controlar la tasa de errores para reducirlos.\nActividades b√°sicas del analisis estad√≠stico# Las t√©cnicas estad√≠sticas deber√≠an ser vistas como una parte importante de cualquier proceso de toma de dicisiones, permitiendo tomar decisiones estrat√©gicamente informadas que combinen intuici√≥n con experiencia y un entendimiento estad√≠stico de los datos que tenemos disponibles.\nUn an√°lisis estad√≠stico suele contener 5 actividades b√°sicas:\nDise√±o del an√°lisis: Esta actividad involucra el planeamiento de los detalles para obtener los datos que necesitamos y la generaci√≥n de la hip√≥tesis a ser evaluada.\nExploraci√≥n de datos: En esta actividad nos dedicamos a jugar con nuestros datos, los describimos, los resumimos, realizamos gr√°ficos para mirarlos desde distintos √°ngulos. Esta exploraci√≥n nos ayuda a asegurarnos que los datos que obtuvimos son completos y que la etapa de dise√±o fue correcta.\nArmado del modelo: En esta actividad intentamos armar un modelo que explique el comportamiento de nuestros datos y pueda llegar a hacer predicciones sobre los mismos. La idea es que el modelo pueda describir las propiedades fundamentales de nuestros datos.\nRealizar estimaciones: Aqu√≠ vamos a intentar realizar estimaciones basadas en el modelo que armamos anteriormente. Tambi√©n vamos a intentar estimar el tama√±o del error que nuestro modelo puede tener en sus predicciones.\nContraste de la hip√≥tesis: Esta actividad es la que va a producir la decisi√≥n final sobre si las predicciones del modelo son correctas y ayudarnos a concluir si los datos que poseemos confirman o rechazan la hip√≥tesis que generamos en la actividad 1.\nConceptos b√°sicos de la estad√≠stica descriptiva# En estad√≠stica descriptiva se utilizan distintas medidas para intentar describir las propiedades de nuestros datos, algunos de los conceptos b√°sicos, son:\nMedia aritm√©tica: La media aritm√©tica es el valor obtenido al sumar todos los datos y dividir el resultado entre el n√∫mero total elementos. Se suele representar con la letra griega \\(\\mu\\). Si tenemos una muestra de \\(n\\) valores, $x_i$, la media aritm√©tica, \\(\\mu\\), es la suma de los valores divididos por el numero de elementos; en otras palabras: $$\\mu = \\frac{1}{n} \\sum_{i}x_i$$ Desviaci√≥n respecto a la media: La desviaci√≥n respecto a la media es la diferencia en valor absoluto entre cada valor de la variable estad√≠stica y la media aritm√©tica. $$D_i = |x_i - \\mu|$$ Varianza: La varianza es la media aritm√©tica del cuadrado de las desviaciones respecto a la media de una distribuci√≥n estad√≠stica. La varianza intenta describir la dispersi√≥n de los datos. Se representa como \\(\\sigma^2\\). $$\\sigma^2 = \\frac{\\sum\\limits_{i=1}^n(x_i - \\mu)^2}{n} $$ Desviaci√≥n t√≠pica: La desviaci√≥n t√≠pica es la ra√≠z cuadrada de la varianza. Se representa con la letra griega \\(\\sigma\\). $$\\sigma = \\sqrt{\\frac{\\sum\\limits_{i=1}^n(x_i - \\mu)^2}{n}} $$ Moda: La moda es el valor que tiene mayor frecuencia absoluta. Se representa con \\(M_0\\)\nMediana: La mediana es el valor que ocupa el lugar central de todos los datos cuando √©stos est√°n ordenados de menor a mayor. Se representa con \\(\\widetilde{x}\\).\nCorrelaci√≥n: La correlaci√≥n trata de establecer la relaci√≥n o dependencia que existe entre las dos variables que intervienen en una distribuci√≥n bidimensional. Es decir, determinar si los cambios en una de las variables influyen en los cambios de la otra. En caso de que suceda, diremos que las variables est√°n correlacionadas o que hay correlaci√≥n entre ellas. La correlaci√≥n es positiva cuando los valores de las variables aumenta juntos; y es negativa cuando un valor de una variable se reduce cuando el valor de la otra variable aumenta.\nCovarianza: La covarianza es el equivalente de la varianza aplicado a una variable bidimensional. Es la media aritm√©tica de los productos de las desviaciones de cada una de las variables respecto a sus medias respectivas.La covarianza indica el sentido de la correlaci√≥n entre las variables; Si \\(\\sigma_{xy} \u0026gt; 0\\) la correlaci√≥n es directa; Si \\(\\sigma_{xy} \u0026lt; 0\\) la correlaci√≥n es inversa.\n$$\\sigma_{xy} = \\frac{\\sum\\limits_{i=1}^n(x_i - \\mu_x)(y_i -\\mu_y)}{n}$$ Valor at√≠pico: Un valor at√≠pico es una observaci√≥n que se aleja demasiado de la moda; esta muy lejos de la tendencia principal del resto de los datos. Pueden ser causados por errores en la recolecci√≥n de datos o medidas inusuales. Generalmente se recomienda eliminarlos del conjunto de datos. Librer√≠as de Python para probabilidad y estad√≠stica# Como ya les vengo mostrando en mis anteriores art√≠culos, Python se lleva muy bien con las matem√°ticas. Adem√°s, la comunidad python es tan amplia que solemos encontrar una librer√≠a para cualquier problema al que nos enfrentemos. En este caso, los principales m√≥dulos que Python nos ofrece para trabajar con probabilidad y estad√≠stica, son:\nnumpy: El popular paquete matem√°tico de Python, se utiliza tanto que mucha gente ya lo considera parte integral del lenguaje. Nos proporciona algunas funciones estad√≠sticas que podemos aplicar f√°cilmente sobre los arrays de Numpy.\nscipy.stats: Este submodulo del paquete cient√≠fico Scipy es el complemento perfecto para Numpy, las funciones estadisticas que no encontremos en uno, las podemos encontrar en el otro.\nstatsmodels: Esta librer√≠a nos brinda un gran n√∫mero de herramientas para explorar datos, estimar modelos estad√≠sticos, realizar pruebas estad√≠sticas y muchas cosas m√°s.\nmatplotlib: Es la librer√≠a m√°s popular en Python para visualizaciones y gr√°ficos. Ella nos va a permitir realizar los gr√°ficos de las distintas distribuciones de datos.\nseaborn: Esta librer√≠a es un complemento ideal de matplotlib para realizar gr√°ficos estad√≠sticos.\npandas: Esta es la librer√≠a m√°s popular para an√°lisis de datos y financieros. Posee algunas funciones muy √∫tiles para realizar estad√≠stica descriptiva sobre nuestros datos y nos facilita sobremanera el trabajar con series de tiempo.\npyMC: pyMC es un m√≥dulo de Python que implementa modelos estad√≠sticos bayesianos, incluyendo la cadena de Markov Monte Carlo(MCMC). pyMC ofrece funcionalidades para hacer el an√°lisis bayesiano lo mas simple posible.\nEjemplos en Python# Calcular los principales indicadores de la estad√≠stica descriptiva con Python es muy f√°cil!.\n# Ejemplos de estadistica descriptiva con python import numpy as np # importando numpy from scipy import stats # importando scipy.stats import pandas as pd # importando pandas np.random.seed(2131982) # para poder replicar el random datos = np.random.randn(5, 4) # datos normalmente distribuidos datos array([[ 0.46038022, -1.08942528, -0.62681496, -0.63329028], [-0.1074033 , -0.88138082, -0.34466623, -0.28320214], [ 0.94051171, 0.86693793, 1.20947882, -0.16894118], [-0.12790177, -0.58099931, -0.46188426, -0.18148302], [-0.76959435, -1.37414587, 1.37696874, -0.18040537]]) # media ar√≠tmetica datos.mean() # Calcula la media aritmetica de -0.14786303590303568 np.mean(datos) # Mismo resultado desde la funcion de numpy -0.14786303590303568 datos.mean(axis=1) # media aritmetica de cada fila array([-0.47228757, -0.40416312, 0.71199682, -0.33806709, -0.23679421]) datos.mean(axis=0) # media aritmetica de cada columna array([ 0.0791985 , -0.61180267, 0.23061642, -0.2894644 ]) # mediana np.median(datos) -0.23234258265023794 np.median(datos, 0) # media aritmetica de cada columna array([-0.1074033 , -0.88138082, -0.34466623, -0.18148302]) # Desviaci√≥n t√≠pica np.std(datos) 0.73755354584071608 np.std(datos, 0) # Desviaci√≥n t√≠pica de cada columna array([ 0.58057213, 0.78352862, 0.87384108, 0.17682485]) # varianza np.var(datos) 0.54398523298221324 np.var(datos, 0) # varianza de cada columna array([ 0.337064 , 0.6139171 , 0.76359823, 0.03126703]) # moda stats.mode(datos) # Calcula la moda de cada columna # el 2do array devuelve la frecuencia. (array([[-0.76959435, -1.37414587, -0.62681496, -0.63329028]]), array([[ 1., 1., 1., 1.]])) datos2 = np.array([1, 2, 3, 6, 6, 1, 2, 4, 2, 2, 6, 6, 8, 10, 6]) stats.mode(datos2) # aqui la moda es el 6 porque aparece 5 veces en el vector. (array([6]), array([ 5.])) # correlacion np.corrcoef(datos) # Crea matriz de correlaci√≥n. array([[ 1. , 0.82333743, 0.15257202, 0.78798675, -0.02292073], [ 0.82333743, 1. , -0.13709662, 0.86873632, 0.41234875], [ 0.15257202, -0.13709662, 1. , -0.47691376, 0.21216856], [ 0.78798675, 0.86873632, -0.47691376, 1. , -0.03445705], [-0.02292073, 0.41234875, 0.21216856, -0.03445705, 1. ]]) # calculando la correlaci√≥n entre dos vectores. np.corrcoef(datos[0], datos[1]) array([[ 1. , 0.82333743], [ 0.82333743, 1. ]]) # covarianza np.cov(datos) # calcula matriz de covarianza array([[ 0.43350958, 0.18087281, 0.06082243, 0.11328658, -0.01782409], [ 0.18087281, 0.11132485, -0.0276957 , 0.06329134, 0.16249513], [ 0.06082243, -0.0276957 , 0.36658864, -0.06305065, 0.15172255], [ 0.11328658, 0.06329134, -0.06305065, 0.04767826, -0.00888624], [-0.01782409, 0.16249513, 0.15172255, -0.00888624, 1.39495179]]) # covarianza de dos vectores np.cov(datos[0], datos[1]) array([[ 0.43350958, 0.18087281], [ 0.18087281, 0.11132485]]) # usando pandas dataframe = pd.DataFrame(datos, index=[\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;e\u0026#39;], columns=[\u0026#39;col1\u0026#39;, \u0026#39;col2\u0026#39;, \u0026#39;col3\u0026#39;, \u0026#39;col4\u0026#39;]) dataframe col1 col2 col3 col4 a 0.460380 -1.089425 -0.626815 -0.633290 b -0.107403 -0.881381 -0.344666 -0.283202 c 0.940512 0.866938 1.209479 -0.168941 d -0.127902 -0.580999 -0.461884 -0.181483 e -0.769594 -1.374146 1.376969 -0.180405 # resumen estadistadistico con pandas dataframe.describe() col1 col2 col3 col4 count 5.000000 5.000000 5.000000 5.000000 mean 0.079199 -0.611803 0.230616 -0.289464 std 0.649099 0.876012 0.976984 0.197696 min -0.769594 -1.374146 -0.626815 -0.633290 25% -0.127902 -1.089425 -0.461884 -0.283202 50% -0.107403 -0.881381 -0.344666 -0.181483 75% 0.460380 -0.580999 1.209479 -0.180405 max 0.940512 0.866938 1.376969 -0.168941 # sumando las columnas dataframe.sum() col1 0.395993 col2 -3.059013 col3 1.153082 col4 -1.447322 dtype: float64 # sumando filas dataframe.sum(axis=1) a -1.889150 b -1.616652 c 2.847987 d -1.352268 e -0.947177 dtype: float64 dataframe.cumsum() # acumulados col1 col2 col3 col4 a 0.460380 -1.089425 -0.626815 -0.633290 b 0.352977 -1.970806 -0.971481 -0.916492 c 1.293489 -1.103868 0.237998 -1.085434 d 1.165587 -1.684867 -0.223887 -1.266917 e 0.395993 -3.059013 1.153082 -1.447322 # media aritmetica de cada columna con pandas dataframe.mean() col1 0.079199 col2 -0.611803 col3 0.230616 col4 -0.289464 dtype: float64 # media aritmetica de cada fila con pandas dataframe.mean(axis=1) a -0.472288 b -0.404163 c 0.711997 d -0.338067 e -0.236794 dtype: float64 Histogramas y Distribuciones# Muchas veces los indicadores de la estad√≠stica descriptiva no nos proporcionan una imagen clara de nuestros datos. Por esta raz√≥n, siempre es √∫til complementarlos con gr√°ficos de las distribuciones de los datos, que describan con qu√© frecuencia aparece cada valor. La representaci√≥n m√°s com√∫n de una distribuci√≥n es un histograma, que es un gr√°fico que muestra la frecuencia o probabilidad de cada valor. El histograma muestra las frecuencias como un gr√°fico de barras que indica cuan frecuente un determinado valor ocurre en el conjunto de datos. El eje horizontal representa los valores del conjunto de datos y el eje vertical representa la frecuencia con que esos valores ocurren.\nLas distribuciones se pueden clasificar en dos grandes grupos:\nLas distribuciones continuas, que son aquellas que presentan un n√∫mero infinito de posibles soluciones. Dentro de este grupo vamos a encontrar a las distribuciones:\nnormal, gamma, chi cuadrado, t de Student, pareto, entre otras Las distribuciones discretas, que son aquellas en las que la variable puede tomar un n√∫mero determinado de valores. Los principales exponentes de este grupo son las distribuciones:\npoisson, binomial, hipergeom√©trica, bernoulli entre otras Veamos algunos ejemplos graficados con la ayuda de Python.\nDistribuci√≥n normal# La distribuci√≥n normal es una de las principales distribuciones, ya que es la que con m√°s frecuencia aparece aproximada en los fen√≥menos reales. Tiene una forma acampanada y es sim√©trica respecto de un determinado par√°metro estad√≠stico. Con la ayuda de Python la podemos graficar de la siguiente manera:\n# Graficos embebidos. %matplotlib inline import matplotlib.pyplot as plt # importando matplotlib import seaborn as sns # importando seaborn # parametros esteticos de seaborn sns.set_palette(\u0026#34;deep\u0026#34;, desat=.6) sns.set_context(rc={\u0026#34;figure.figsize\u0026#34;: (8, 4)}) mu, sigma = 0, 0.1 # media y desvio estandar s = np.random.normal(mu, sigma, 1000) #creando muestra de datos # histograma de distribuci√≥n normal. cuenta, cajas, ignorar = plt.hist(s, 30, normed=True) normal = plt.plot(cajas, 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (cajas - mu)**2 / (2 * sigma**2) ), linewidth=2, color=\u0026#39;r\u0026#39;) Distribuciones simetricas y asimetricas# Una distribuci√≥n es sim√©trica cuando moda, mediana y media coinciden aproximadamente en sus valores. Si una distribuci√≥n es sim√©trica, existe el mismo n√∫mero de valores a la derecha que a la izquierda de la media, por tanto, el mismo n√∫mero de desviaciones con signo positivo que con signo negativo.\nUna distribuci√≥n tiene asimetria positiva (o a la derecha) si la \u0026ldquo;cola\u0026rdquo; a la derecha de la media es m√°s larga que la de la izquierda, es decir, si hay valores m√°s separados de la media a la derecha. De la misma forma una distribuci√≥n tiene asimetria negativa (o a la izquierda) si la \u0026ldquo;cola\u0026rdquo; a la izquierda de la media es m√°s larga que la de la derecha, es decir, si hay valores m√°s separados de la media a la izquierda.\nLas distribuciones asim√©tricas suelen ser problem√°ticas, ya que la mayor√≠a de los m√©todos estad√≠sticos suelen estar desarrollados para distribuciones del tipo normal. Para salvar estos problemas se suelen realizar transformaciones a los datos para hacer a estas distribuciones m√°s sim√©tricas y acercarse a la distribuci√≥n normal.\n# Dibujando la distribucion Gamma x = stats.gamma(3).rvs(5000) gamma = plt.hist(x, 70, histtype=\u0026#34;stepfilled\u0026#34;, alpha=.7) En este ejemplo podemos ver que la distribuci√≥n gamma que dibujamos tiene una asimetria positiva.\n# Calculando la simetria con scipy stats.skew(x) 1.1437199125547868 Cuartiles y diagramas de cajas# Los cuartiles son los tres valores de la variable estad√≠stica que dividen a un conjunto de datos ordenados en cuatro partes iguales. Q1, Q2 y Q3 determinan los valores correspondientes al 25%, al 50% y al 75% de los datos. Q2 coincide con la mediana.\nLos diagramas de cajas son una presentaci√≥n visual que describe varias caracter√≠sticas importantes al mismo tiempo, tales como la dispersi√≥n y simetr√≠a. Para su realizaci√≥n se representan los tres cuartiles y los valores m√≠nimo y m√°ximo de los datos, sobre un rect√°ngulo, alineado horizontal o verticalmente. Estos gr√°ficos nos proporcionan abundante informaci√≥n y son sumamente √∫tiles para encontrar valores at√≠picos y comparar dos conjunto de datos.\n# Ejemplo de grafico de cajas en python datos_1 = np.random.normal(100, 10, 200) datos_2 = np.random.normal(80, 30, 200) datos_3 = np.random.normal(90, 20, 200) datos_4 = np.random.normal(70, 25, 200) datos_graf = [datos_1, datos_2, datos_3, datos_4] # Creando el objeto figura fig = plt.figure(1, figsize=(9, 6)) # Creando el subgrafico ax = fig.add_subplot(111) # creando el grafico de cajas bp = ax.boxplot(datos_graf) # visualizar mas facile los at√≠picos for flier in bp[\u0026#39;fliers\u0026#39;]: flier.set(marker=\u0026#39;o\u0026#39;, color=\u0026#39;red\u0026#39;, alpha=0.5) # los puntos aislados son valores at√≠picos # usando seaborn sns.boxplot(datos_graf, names=[\u0026#34;grupo1\u0026#34;, \u0026#34;grupo2\u0026#34;, \u0026#34;grupo3\u0026#34;, \u0026#34;grupo 4\u0026#34;], color=\u0026#34;PaleGreen\u0026#34;); Regresiones# Las regresiones es una de las herramientas principales de la estadistica inferencial. El objetivo del an√°lisis de regresi√≥n es describir la relaci√≥n entre un conjunto de variables, llamadas variables dependientes, y otro conjunto de variables, llamadas variables independientes o explicativas. M√°s espec√≠ficamente, el an√°lisis de regresi√≥n ayuda a entender c√≥mo el valor t√≠pico de la variable dependiente cambia cuando cualquiera de las variables independientes es cambiada, mientras que se mantienen las otras variables independientes fijas. El producto final del an√°lisis de regresi√≥n es la estimaci√≥n de una funci√≥n de las variables independientes llamada la funci√≥n de regresi√≥n. La idea es que en base a esta funci√≥n de regresi√≥n podamos hacer estimaciones sobre eventos futuros.\nLa regresi√≥n lineal es una de las t√©cnicas m√°s simples y mayormente utilizadas en los an√°lisis de regresiones. Hace suposiciones muy r√≠gidas sobre la relaci√≥n entre la variable dependiente \\(y\\) y variable independiente \\(x\\). Asume que la relaci√≥n va a tomar la forma:\n$$ y = \\beta_0 + \\beta_1 * x$$ Uno de los m√©todos m√°s populares para realizar regresiones lineales es el de m√≠nimos cuadrados ordinarios (OLS, por sus siglas en ingl√©s), este m√©todo es el estimador m√°s simple y com√∫n en la que los dos \\(\\beta\\)s se eligen para minimizar el cuadrado de la distancia entre los valores estimados y los valores reales.\nRealizar an√°lisis de regresiones en Python es sumamente f√°cil gracias a statsmodels.\nVeamos un peque√±o ejemplo utilizando el dataset longley, el cual es ideal para realizar regresiones:\n# importanto la api de statsmodels import statsmodels.formula.api as smf import statsmodels.api as sm # Creando un DataFrame de pandas. df = pd.read_csv(\u0026#39;https://vincentarelbundock.github.io/Rdatasets/csv/datasets/longley.csv\u0026#39;, index_col=0) df.head() # longley dataset GNP.deflator GNP Unemployed Armed.Forces Population Year Employed 1947 83.0 234.289 235.6 159.0 107.608 1947 60.323 1948 88.5 259.426 232.5 145.6 108.632 1948 61.122 1949 88.2 258.054 368.2 161.6 109.773 1949 60.171 1950 89.5 284.599 335.1 165.0 110.929 1950 61.187 1951 96.2 328.975 209.9 309.9 112.075 1951 63.221 # utilizando la api de formula de statsmodels est = smf.ols(formula=\u0026#39;Employed ~ GNP\u0026#39;, data=df).fit() est.summary() # Employed se estima en base a GNP. OLS Regression Results Dep. Variable: Employed R-squared: 0.967 Model: OLS Adj. R-squared: 0.965 Method: Least Squares F-statistic: 415.1 Date: Sat, 27 Jun 2015 Prob (F-statistic): 8.36e-12 Time: 15:30:24 Log-Likelihood: -14.904 No. Observations: 16 AIC: 33.81 Df Residuals: 14 BIC: 35.35 Df Model: 1 Covariance Type: nonrobust coef std err t P\u003e|t| [95.0% Conf. Int.] Intercept 51.8436 0.681 76.087 0.000 50.382 53.305 GNP 0.0348 0.002 20.374 0.000 0.031 0.038 Omnibus: 1.925 Durbin-Watson: 1.619 Prob(Omnibus): 0.382 Jarque-Bera (JB): 1.215 Skew: 0.664 Prob(JB): 0.545 Kurtosis: 2.759 Cond. No. 1.66e+03 Como podemos ver, el resumen que nos brinda statsmodels sobre nuestro modelo de regresi√≥n contiene bastante informaci√≥n sobre como se ajuste el modelo a los datos. Pasemos a explicar algunos de estos valores:\nDep. Variable: es la variable que estamos estimasdo. Model: es el modelo que estamos utilizando. R-squared: es el coeficiente de determinaci√≥n, el cual mide cuan bien nuestra recta de regresion se aproxima a los datos reales. Adj. R-squared: es el coeficiente anterior ajustado seg√∫n el n√∫mero de observaciones. [95.0% Conf. Int.]: Los valores inferior y superior del intervalo de confianza del 95%. coef: el valor estimado del coeficiente. std err: el error est√°ndar de la estimaci√≥n del coeficiente. Skew: una medida de la asimetria de los datos sobre la media. Kurtosis: Una medida de la forma de la distribuci√≥n. La curtosis compara la cantidad de datos cerca de la media con los que est√°n m√°s lejos de la media(en las colas). # grafico de regresion. que tanto se ajusta el modelo a los datos. y = df.Employed # Respuesta X = df.GNP # Predictor X = sm.add_constant(X) # agrega constante X_1 = pd.DataFrame({\u0026#39;GNP\u0026#39;: np.linspace(X.GNP.min(), X.GNP.max(), 100)}) X_1 = sm.add_constant(X_1) y_reg = est.predict(X_1) # estimacion plt.scatter(X.GNP, y, alpha=0.3) # grafica los puntos de datos plt.ylim(30, 100) # limite de eje y plt.xlabel(\u0026#34;Producto bruto\u0026#34;) # leyenda eje x plt.ylabel(\u0026#34;Empleo\u0026#34;) # leyenda eje y plt.title(\u0026#34;Ajuste de regresion\u0026#34;) # titulo del grafico reg = plt.plot(X_1.GNP, y_reg, \u0026#39;r\u0026#39;, alpha=0.9) # linea de regresion # grafico de influencia from statsmodels.graphics.regressionplots import influence_plot inf =influence_plot(est) Este √∫ltimo gr√°fico nos muestra el apalancamiento y la influencia de cada caso.\nLa estad√≠stica bayesiana# La estad√≠stica bayesiana es un subconjunto del campo de la estad√≠stica en la que la evidencia sobre el verdadero estado de las cosas se expresa en t√©rminos de grados de creencia. Esta filosof√≠a de tratar a las creencias como probabilidad es algo natural para los seres humanos. Nosotros la utilizamos constantemente a medida que interactuamos con el mundo y s√≥lo vemos verdades parciales; necesitando reunir pruebas para formar nuestras creencias.\nLa diferencia fundamental entre la estad√≠stica cl√°sica (frecuentista) y la bayesiana es el concepto de probabilidad. Para la estad√≠stica cl√°sica es un concepto objetivo, que se encuentra en la naturaleza, mientras que para la estad√≠stica bayesiana se encuentra en el observador, siendo as√≠ un concepto subjetivo. De este modo, en estad√≠stica cl√°sica solo se toma como fuente de informaci√≥n las muestras obtenidas. En el caso bayesiano, sin embargo, adem√°s de la muestra tambi√©n juega un papel fundamental la informaci√≥n previa o externa que se posee en relaci√≥n a los fen√≥menos que se tratan de modelar.\nLa estad√≠stica bayesiana est√° demostrando su utilidad en ciertas estimaciones basadas en el conocimiento subjetivo a priori y el hecho de permitir revisar esas estimaciones en funci√≥n de la evidencia emp√≠rica es lo que est√° abriendo nuevas formas de hacer conocimiento. Una aplicaci√≥n de esto son los clasificadores bayesianos que son frecuentemente usados en implementaciones de filtros de correo basura, que se adaptan con el uso. La estad√≠stica bayesiana es un tema muy interesante que merece un art√≠culo en s√≠ mismo.\nPara entender m√°s f√°cilmente como funciona la estad√≠stica bayesiana veamos un simple ejemplo del lanzamiento de una moneda. La idea principal de la inferencia bayesiana es que la noci√≥n de probabilidad cambia mientras m√°s datos tengamos.\nsns.set_context(rc={\u0026#34;figure.figsize\u0026#34;: (11, 9)}) dist = stats.beta n_trials = [0, 1, 2, 3, 4, 5, 8, 15, 50, 500] data = stats.bernoulli.rvs(0.5, size=n_trials[-1]) x = np.linspace(0, 1, 100) for k, N in enumerate(n_trials): sx = plt.subplot(len(n_trials) / 2, 2, k + 1) plt.xlabel(\u0026#34;$p$, probabilidad de cara\u0026#34;) \\ if k in [0, len(n_trials) - 1] else None plt.setp(sx.get_yticklabels(), visible=False) heads = data[:N].sum() y = dist.pdf(x, 1 + heads, 1 + N - heads) plt.plot(x, y, label=\u0026#34;lanzamientos observados %d,\\n %d caras\u0026#34; % (N, heads)) plt.fill_between(x, 0, y, color=\u0026#34;#348ABD\u0026#34;, alpha=0.4) plt.vlines(0.5, 0, 4, color=\u0026#34;k\u0026#34;, linestyles=\u0026#34;--\u0026#34;, lw=1) leg = plt.legend() leg.get_frame().set_alpha(0.4) plt.autoscale(tight=True) plt.suptitle(\u0026#34;Actualizacion Bayesiana de probabilidades posterios\u0026#34;, y=1.02, fontsize=14) plt.tight_layout() Como el gr√°fico de arriba muestra, cuando empezamos a observar nuevos datos nuestras probabilidades posteriores comienzan a cambiar y moverse. Eventualmente, a medida que observamos m√°s y m√°s datos (lanzamientos de monedas), nuestras probabilidades se acercan m√°s y m√°s hacia el verdadero valor de p = 0.5 (marcado por una l√≠nea discontinua).\nAqu√≠ termina este tutorial, espero que les haya sido util.\nSaludos!\nEste post fue escrito utilizando IPython notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2015-06-27","id":39,"permalink":"/blog/2015/06/27/probabilidad-y-estadistica-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nActualmente con el boom de la Big Data, tener nociones de probabilidad y estad√≠stica se ha hecho fundamental. En los √∫ltimos a√±os ha habido un resurgimiento de todo lo relacionado con estad√≠stica , data mining y machine learning empujados principalmente por la explosi√≥n de datos con que contamos, estos conceptos combinados forman la base de lo que actualmente se conoce como la Ciencia de Datos.","tags":["python","estadistica","probabilidad","distribuciones"],"title":"Probabilidad y Estad√≠stica con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Una de las herramientas matem√°ticas m√°s utilizadas en machine learning y data mining es el √Ålgebra lineal; por tanto, si queremos incursionar en el fascinante mundo del aprendizaje autom√°tico y el an√°lisis de datos es importante reforzar los conceptos que forman parte de sus cimientos.\nEl √Ålgebra lineal es una rama de las matem√°ticas que es sumamente utilizada en el estudio de una gran variedad de ciencias, como ser, ingenier√≠a, finanzas, investigaci√≥n operativa, entre otras. Es una extensi√≥n del √°lgebra que aprendemos en la escuela secundaria, hacia un mayor n√∫mero de dimensiones; en lugar de trabajar con inc√≥gnitas a nivel de escalares comenzamos a trabajar con matrices y vectores.\nEl estudio del √Ålgebra lineal implica trabajar con varios objetos matem√°ticos, como ser:\nLos Escalares: Un escalar es un solo n√∫mero, en contraste con la mayor√≠a de los otros objetos estudiados en √Ålgebra lineal, que son generalmente una colecci√≥n de m√∫ltiples n√∫meros.\nLos Vectores:Un vector es una serie de n√∫meros. Los n√∫meros tienen una orden preestablecido, y podemos identificar cada n√∫mero individual por su √≠ndice en ese orden. Podemos pensar en los vectores como la identificaci√≥n de puntos en el espacio, con cada elemento que da la coordenada a lo largo de un eje diferente. Existen dos tipos de vectores, los vectores de fila y los vectores de columna. Podemos representarlos de la siguiente manera, d√≥nde f es un vector de fila y c es un vector de columna: $$f=\\begin{bmatrix}0\u00261\u0026-1\\end{bmatrix} ; c=\\begin{bmatrix}0\\\\1\\\\-1\\end{bmatrix}$$ Las Matrices: Una matriz es un arreglo bidimensional de n√∫meros (llamados entradas de la matriz) ordenados en filas (o renglones) y columnas, donde una fila es cada una de las l√≠neas horizontales de la matriz y una columna es cada una de las l√≠neas verticales. En una matriz cada elemento puede ser identificado utilizando dos √≠ndices, uno para la fila y otro para la columna en que se encuentra. Las podemos representar de la siguiente manera, A es una matriz de 3x2. $$A=\\begin{bmatrix}0 \u0026 1\u0026 \\\\-1 \u0026 2 \\\\ -2 \u0026 3\\end{bmatrix}$$ Los Tensores:En algunos casos necesitaremos una matriz con m√°s de dos ejes. En general, una serie de n√∫meros dispuestos en una cuadr√≠cula regular con un n√∫mero variable de ejes es conocido como un tensor.\nSobre estos objetos podemos realizar las operaciones matem√°ticas b√°sicas, como ser adici√≥n, multiplicaci√≥n, sustracci√≥n y divisi√≥n, es decir que vamos a poder sumar vectores con matrices, multiplicar escalares a vectores y dem√°s.\nLibrer√≠as de Python para √°lgebra lineal# Los principales m√≥dulos que Python nos ofrece para realizar operaciones de √Ålgebra lineal son los siguientes:\nNumpy: El popular paquete matem√°tico de Python, nos va a permitir crear vectores, matrices y tensores con suma facilidad.\nnumpy.linalg: Este es un submodulo dentro de Numpy con un gran n√∫mero de funciones para resolver ecuaciones de √Ålgebra lineal.\nscipy.linalg: Este submodulo del paquete cient√≠fico Scipy es muy similar al anterior, pero con algunas m√°s funciones y optimaciones.\nSympy: Esta librer√≠a nos permite trabajar con matem√°tica simb√≥lica, convierte a Python en un sistema algebraico computacional. Nos va a permitir trabajar con ecuaciones y f√≥rmulas simb√≥licamente, en lugar de num√©ricamente.\nCVXOPT: Este m√≥dulo nos permite resolver problemas de optimizaciones de programaci√≥n lineal.\nPuLP: Esta librer√≠a nos permite crear modelos de programaci√≥n lineal en forma muy sencilla con Python.\nOperaciones b√°sicas# Vectores# Un vector de largo n es una secuencia (o array, o tupla) de n n√∫meros. La solemos escribir como \\(x=(x1,\u0026hellip;,xn)\\) o \\(x=[x1,\u0026hellip;,xn]\\)\nEn Python, un vector puede ser representado con una simple lista, o con un array de Numpy; siendo preferible utilizar esta √∫ltima opci√≥n.\n# Vector como lista de Python v1 = [2, 4, 6] v1 [2, 4, 6] # Vectores con numpy import numpy as np v2 = np.ones(3) # vector de solo unos. v2 array([1., 1., 1.]) v3 = np.array([1, 3, 5]) # pasando una lista a las arrays de numpy v3 array([1, 3, 5]) v4 = np.arange(1, 8) # utilizando la funcion arange de numpy v4 array([1, 2, 3, 4, 5, 6, 7]) Representaci√≥n gr√°fica# Tradicionalmente, los vectores son representados visualmente como flechas que parten desde el origen hacia un punto.\nPor ejemplo, si quisi√©ramos representar graficamente a los vectores \\(v1=[2, 4]\\), \\(v2=[-3, 3]\\) y \\(v3=[-4, -3.5]\\), podr√≠amos hacerlo de la siguiente manera.\nimport matplotlib.pyplot as plt from warnings import filterwarnings %matplotlib inline filterwarnings(\u0026#39;ignore\u0026#39;) # Ignorar warnings def move_spines(): \u0026#34;\u0026#34;\u0026#34;Crea la figura de pyplot y los ejes. Mueve las lineas de la izquierda y de abajo para que se intersecten con el origen. Elimina las lineas de la derecha y la de arriba. Devuelve los ejes.\u0026#34;\u0026#34;\u0026#34; fix, ax = plt.subplots() for spine in [\u0026#34;left\u0026#34;, \u0026#34;bottom\u0026#34;]: ax.spines[spine].set_position(\u0026#34;zero\u0026#34;) for spine in [\u0026#34;right\u0026#34;, \u0026#34;top\u0026#34;]: ax.spines[spine].set_color(\u0026#34;none\u0026#34;) return ax def vect_fig(): \u0026#34;\u0026#34;\u0026#34;Genera el grafico de los vectores en el plano\u0026#34;\u0026#34;\u0026#34; ax = move_spines() ax.set_xlim(-5, 5) ax.set_ylim(-5, 5) ax.grid() vecs = [[2, 4], [-3, 3], [-4, -3.5]] # lista de vectores for v in vecs: ax.annotate(\u0026#34; \u0026#34;, xy=v, xytext=[0, 0], arrowprops=dict(facecolor=\u0026#34;blue\u0026#34;, shrink=0, alpha=0.7, width=0.5)) ax.text(1.1 * v[0], 1.1 * v[1], v) vect_fig() # crea el gr√°fico Operaciones con vectores# Las operaciones m√°s comunes que utilizamos cuando trabajamos con vectores son la suma, la resta y la multiplicaci√≥n por escalares.\nCuando sumamos dos vectores, vamos sumando elemento por elemento de cada vector.\n$$ \\begin{split}x + y = \\left[ \\begin{array}{c} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{array} \\right] + \\left[ \\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{array} \\right] := \\left[ \\begin{array}{c} x_1 + y_1 \\\\ x_2 + y_2 \\\\ \\vdots \\\\ x_n + y_n \\end{array} \\right]\\end{split}$$ De forma similar funciona la operaci√≥n de resta.\n$$ \\begin{split}x - y = \\left[ \\begin{array}{c} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{array} \\right] - \\left[ \\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{array} \\right] := \\left[ \\begin{array}{c} x_1 - y_1 \\\\ x_2 - y_2 \\\\ \\vdots \\\\ x_n - y_n \\end{array} \\right]\\end{split}$$ La multiplicaci√≥n por escalares es una operaci√≥n que toma a un n√∫mero \\(\\gamma\\), y a un vector \\(x\\) y produce un nuevo vector donde cada elemento del vector \\(x\\) es multiplicado por el n√∫mero \\(\\gamma\\).\n$$\\begin{split}\\gamma x := \\left[ \\begin{array}{c} \\gamma x_1 \\\\ \\gamma x_2 \\\\ \\vdots \\\\ \\gamma x_n \\end{array} \\right]\\end{split}$$ En Python podr√≠amos realizar estas operaciones en forma muy sencilla:\n# Ejemplo en Python x = np.arange(1, 5) y = np.array([2, 4, 6, 8]) x, y (array([1, 2, 3, 4]), array([2, 4, 6, 8])) # sumando dos vectores numpy x + y array([ 3, 6, 9, 12]) # restando dos vectores x - y array([-1, -2, -3, -4]) # multiplicando por un escalar x * 2 array([2, 4, 6, 8]) y * 3 array([ 6, 12, 18, 24]) Producto escalar o interior# El producto escalar de dos vectores se define como la suma de los productos de sus elementos, suele representarse matem√°ticamente como \u0026lt; x, y \u0026gt; o x\u0026rsquo;y, donde x e y son dos vectores.\n$$\u003c x, y \u003e := \\sum_{i=1}^n x_i y_i$$ Dos vectores son ortogonales o perpendiculares cuando forman √°ngulo recto entre s√≠. Si el producto escalar de dos vectores es cero, ambos vectores son ortogonales.\nAdicionalmente, todo producto escalar induce una norma sobre el espacio en el que est√° definido, de la siguiente manera:\n$$\\| x \\| := \\sqrt{\u003c x, x\u003e} := \\left( \\sum_{i=1}^n x_i^2 \\right)^{1/2}$$ En Python lo podemos calcular de la siguiente forma:\n# Calculando el producto escalar de los vectores x e y x @ y 60 # o lo que es lo mismo, que: sum(x * y), np.dot(x, y) (60, 60) # Calculando la norma del vector X np.linalg.norm(x) 5.477225575051661 # otra forma de calcular la norma de x np.sqrt(x @ x) 5.477225575051661 # vectores ortogonales v1 = np.array([3, 4]) v2 = np.array([4, -3]) v1 @ v2 0 Matrices# Las matrices son una forma clara y sencilla de organizar los datos para su uso en operaciones lineales.\nUna matriz n √ó k es una agrupaci√≥n rectangular de n√∫meros con n filas y k columnas; se representa de la siguiente forma:\n$$\\begin{split}A = \\left[ \\begin{array}{cccc} a_{11} \u0026 a_{12} \u0026 \\cdots \u0026 a_{1k} \\\\ a_{21} \u0026 a_{22} \u0026 \\cdots \u0026 a_{2k} \\\\ \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\\\ a_{n1} \u0026 a_{n2} \u0026 \\cdots \u0026 a_{nk} \\end{array} \\right]\\end{split}$$ En la matriz A, el s√≠mbolo \\(a_{nk}\\) representa el elemento n-√©simo de la fila en la k-√©sima columna. La matriz A tambi√©n puede ser llamada un vector si cualquiera de n o k son iguales a 1. En el caso de n=1, A se llama un vector fila, mientras que en el caso de k=1 se denomina un vector columna.\nLas matrices se utilizan para m√∫ltiples aplicaciones y sirven, en particular, para representar los coeficientes de los sistemas de ecuaciones lineales o para representar transformaciones lineales dada una base. Pueden sumarse, multiplicarse y descomponerse de varias formas.\nOperaciones con matrices# Al igual que con los vectores, que no son m√°s que un caso particular, las matrices se pueden sumar, restar y la multiplicar por escalares.\nMultiplicacion por escalares: $$\\begin{split}\\gamma A \\left[ \\begin{array}{ccc} a_{11} \u0026 \\cdots \u0026 a_{1k} \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \\\\ a_{n1} \u0026 \\cdots \u0026 a_{nk} \\\\ \\end{array} \\right] := \\left[ \\begin{array}{ccc} \\gamma a_{11} \u0026 \\cdots \u0026 \\gamma a_{1k} \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \\\\ \\gamma a_{n1} \u0026 \\cdots \u0026 \\gamma a_{nk} \\\\ \\end{array} \\right]\\end{split}$$ Suma de matrices: $$\\begin{split}A + B = \\left[ \\begin{array}{ccc} a_{11} \u0026 \\cdots \u0026 a_{1k} \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \\\\ a_{n1} \u0026 \\cdots \u0026 a_{nk} \\\\ \\end{array} \\right] + \\left[ \\begin{array}{ccc} b_{11} \u0026 \\cdots \u0026 b_{1k} \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \\\\ b_{n1} \u0026 \\cdots \u0026 b_{nk} \\\\ \\end{array} \\right] := \\left[ \\begin{array}{ccc} a_{11} + b_{11} \u0026 \\cdots \u0026 a_{1k} + b_{1k} \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \\\\ a_{n1} + b_{n1} \u0026 \\cdots \u0026 a_{nk} + b_{nk} \\\\ \\end{array} \\right]\\end{split}$$ Resta de matrices: $$\\begin{split}A - B = \\left[ \\begin{array}{ccc} a_{11} \u0026 \\cdots \u0026 a_{1k} \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \\\\ a_{n1} \u0026 \\cdots \u0026 a_{nk} \\\\ \\end{array} \\right]- \\left[ \\begin{array}{ccc} b_{11} \u0026 \\cdots \u0026 b_{1k} \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \\\\ b_{n1} \u0026 \\cdots \u0026 b_{nk} \\\\ \\end{array} \\right] := \\left[ \\begin{array}{ccc} a_{11} - b_{11} \u0026 \\cdots \u0026 a_{1k} - b_{1k} \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \\\\ a_{n1} - b_{n1} \u0026 \\cdots \u0026 a_{nk} - b_{nk} \\\\ \\end{array} \\right]\\end{split}$$ Para los casos de suma y resta, hay que tener en cuenta que solo se pueden sumar o restar matrices que tengan las mismas dimensiones, es decir que si tengo una matriz A de dimensi√≥n 3x2 (3 filas y 2 columnas) solo voy a poder sumar o restar la matriz B si esta tambi√©n tiene 3 filas y 2 columnas.\n# Ejemplo en Python A = np.array([[1, 3, 2], [1, 0, 0], [1, 2, 2]]) B = np.array([[1, 0, 5], [7, 5, 0], [2, 1, 1]]) # suma de las matrices A y B A + B array([[2, 3, 7], [8, 5, 0], [3, 3, 3]]) # resta de matrices A - B array([[ 0, 3, -3], [-6, -5, 0], [-1, 1, 1]]) # multiplicando matrices por escalares A * 2 array([[2, 6, 4], [2, 0, 0], [2, 4, 4]]) B * 3 array([[ 3, 0, 15], [21, 15, 0], [ 6, 3, 3]]) # ver la dimension de una matriz A.shape (3, 3) # ver cantidad de elementos de una matriz A.size 9 Multiplicacion o Producto de matrices# La regla para la multiplicaci√≥n de matrices generaliza la idea del producto interior que vimos con los vectores; y esta dise√±ada para facilitar las operaciones lineales b√°sicas. Cuando multiplicamos matrices, el n√∫mero de columnas de la primera matriz debe ser igual al n√∫mero de filas de la segunda matriz; y el resultado de esta multiplicaci√≥n va a tener el mismo n√∫mero de filas que la primer matriz y el n√∫mero de la columnas de la segunda matriz. Es decir, que si yo tengo una matriz A de dimensi√≥n 3x4 y la multiplico por una matriz B de dimensi√≥n 4x2, el resultado va a ser una matriz C de dimensi√≥n 3x2.\nAlgo a tener en cuenta a la hora de multiplicar matrices es que la propiedad connmutativa no se cumple. AxB no es lo mismo que BxA.\nVeamos los ejemplos en Python.\n# Ejemplo multiplicaci√≥n de matrices A = np.arange(1, 13).reshape(3, 4) #matriz de dimension 3x4 A array([[ 1, 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12]]) B = np.arange(8).reshape(4,2) #matriz de dimension 4x2 B array([[0, 1], [2, 3], [4, 5], [6, 7]]) # Multiplicando A x B A @ B #resulta en una matriz de dimension 3x2 array([[ 40, 50], [ 88, 114], [136, 178]]) # Multiplicando B x A B @ A --------------------------------------------------------------------------- ValueError Traceback (most recent call last) \u0026lt;ipython-input-28-b55e34ad9c31\u0026gt; in \u0026lt;module\u0026gt;() 1 # Multiplicando B x A ----\u0026gt; 2 B @ A ValueError: shapes (4,2) and (3,4) not aligned: 2 (dim 1) != 3 (dim 0) Este ultimo ejemplo vemos que la propiedad conmutativa no se cumple, es m√°s, Python nos arroja un error, ya que el n√∫mero de columnas de B no coincide con el n√∫mero de filas de A, por lo que ni siquiera se puede realizar la multiplicaci√≥n de B x A.\nPara una explicaci√≥n m√°s detallada del proceso de multiplicaci√≥n de matrices, pueden consultar el siguiente tutorial.\nLa matriz identidad, la matriz inversa, la matriz transpuesta y el determinante# La matriz identidad es el elemento neutro en la multiplicaci√≥n de matrices, es el equivalente al n√∫mero 1. Cualquier matriz multiplicada por la matriz identidad nos da como resultado la misma matriz. La matriz identidad es una matriz cuadrada (tiene siempre el mismo n√∫mero de filas que de columnas); y su diagonal principal se compone de todos elementos 1 y el resto de los elementos se completan con 0. Suele representase con la letra I\nPor ejemplo la matriz identidad de 3x3 ser√≠a la siguiente:\n$$I=\\begin{bmatrix}1 \u0026 0 \u0026 0 \u0026 \\\\0 \u0026 1 \u0026 0\\\\ 0 \u0026 0 \u0026 1\\end{bmatrix}$$ Ahora que conocemos el concepto de la matriz identidad, podemos llegar al concepto de la matriz inversa. Si tenemos una matriz A, la matriz inversa de A, que se representa como \\(A^{-1}\\) es aquella matriz cuadrada que hace que la multiplicaci√≥n \\(A$x$A^{-1}\\) sea igual a la matriz identidad I. Es decir que es la matriz rec√≠proca de A.\n$$A √ó A^{-1} = A^{-1} √ó A = I$$ Tener en cuenta que esta matriz inversa en muchos casos puede no existir.En este caso se dice que la matriz es singular o degenerada. Una matriz es singular si y solo si su determinante es nulo.\nEl determinante es un n√∫mero especial que puede calcularse sobre las matrices cuadradas. Se calcula como la suma de los productos de las diagonales de la matriz en una direcci√≥n menos la suma de los productos de las diagonales en la otra direcci√≥n. Se represente con el s√≠mbolo |A|.\n$$A=\\begin{bmatrix}a_{11} \u0026 a_{12} \u0026 a_{13} \u0026 \\\\a_{21} \u0026 a_{22} \u0026 a_{23} \u0026 \\\\ a_{31} \u0026 a_{32} \u0026 a_{33} \u0026 \\end{bmatrix}$$ $$|A| = (a_{11} a_{22} a_{33} + a_{12} a_{23} a_{31} + a_{13} a_{21} a_{32} ) - (a_{31} a_{22} a_{13} + a_{32} a_{23} a_{11} + a_{33} a_{21} a_{12}) $$ Por √∫ltimo, la matriz transpuesta es aquella en que las filas se transforman en columnas y las columnas en filas. Se representa con el s√≠mbolo $A^\\intercal$\n$$\\begin{bmatrix}a \u0026 b \u0026 \\\\c \u0026 d \u0026 \\\\ e \u0026 f \u0026 \\end{bmatrix}^T:=\\begin{bmatrix}a \u0026 c \u0026 e \u0026\\\\b \u0026 d \u0026 f \u0026 \\end{bmatrix}$$ Ejemplos en Python:\n# Creando una matriz identidad de 2x2 I = np.eye(2) I array([[1., 0.], [0., 1.]]) # Multiplicar una matriz por la identidad nos da la misma matriz A = np.array([[4, 7], [2, 6]]) A array([[4, 7], [2, 6]]) A @ I # AxI = A array([[4., 7.], [2., 6.]]) # Calculando el determinante de la matriz A np.linalg.det(A) 10.000000000000002 # Calculando la inversa de A. A_inv = np.linalg.inv(A) A_inv array([[ 0.6, -0.7], [-0.2, 0.4]]) # A x A_inv nos da como resultado I. A @ A_inv array([[1., 0.], [0., 1.]]) # Trasponiendo una matriz A = np.arange(6).reshape(3, 2) A array([[0, 1], [2, 3], [4, 5]]) np.transpose(A) array([[0, 2, 4], [1, 3, 5]]) Sistemas de ecuaciones lineales# Una de las principales aplicaciones del √Ålgebra lineal consiste en resolver problemas de sistemas de ecuaciones lineales.\nUna ecuaci√≥n lineal es una ecuaci√≥n que solo involucra sumas y restas de una variable o mas variables a la primera potencia. Es la ecuaci√≥n de la l√≠nea recta.Cuando nuestro problema esta representado por m√°s de una ecuaci√≥n lineal, hablamos de un sistema de ecuaciones lineales. Por ejemplo, podr√≠amos tener un sistema de dos ecuaciones con dos inc√≥gnitas como el siguiente:\n$$ x - 2y = 1$$ $$3x + 2y = 11$$ La idea es encontrar el valor de \\(x\\) e \\(y\\) que resuelva ambas ecuaciones. Una forma en que podemos hacer esto, puede ser representando graficamente ambas rectas y buscar los puntos en que las rectas se cruzan.\nEn Python esto se puede hacer en forma muy sencilla con la ayuda de matplotlib.\n# graficando el sistema de ecuaciones. x_vals = np.linspace(0, 5, 50) # crea 50 valores entre 0 y 5 plt.plot(x_vals, (1 - x_vals)/-2) # grafica x - 2y = 1 plt.plot(x_vals, (11 - (3*x_vals))/2) # grafica 3x + 2y = 11 plt.axis(ymin = 0) (-0.25, 5.25, 0, 5.875) Luego de haber graficado las funciones, podemos ver que ambas rectas se cruzan en el punto (3, 1), es decir que la soluci√≥n de nuestro sistema ser√≠a \\(x=3\\) e \\(y=1\\). En este caso, al tratarse de un sistema simple y con solo dos inc√≥gnitas, la soluci√≥n gr√°fica puede ser de utilidad, pero para sistemas m√°s complicados se necesita una soluci√≥n num√©rica, es aqu√≠ donde entran a jugar las matrices.\nEse mismo sistema se podr√≠a representar como una ecuaci√≥n matricial de la siguiente forma:\n$$\\begin{bmatrix}1 \u0026 -2 \u0026 \\\\3 \u0026 2 \u0026 \\end{bmatrix} \\begin{bmatrix}x \u0026 \\\\y \u0026 \\end{bmatrix} = \\begin{bmatrix}1 \u0026 \\\\11 \u0026 \\end{bmatrix}$$ Lo que es lo mismo que decir que la matriz A por la matriz $x$ nos da como resultado el vector b.\n$$ Ax = b$$ En este caso, ya sabemos el resultado de $x$, por lo que podemos comprobar que nuestra soluci√≥n es correcta realizando la multiplicaci√≥n de matrices.\n# Comprobando la solucion con la multiplicaci√≥n de matrices. A = np.array([[1., -2.], [3., 2.]]) x = np.array([[3.],[1.]]) A @ x array([[ 1.], [11.]]) Para resolver en forma num√©rica los sistema de ecuaciones, existen varios m√©todos:\nEl m√©todo de sustituci√≥n: El cual consiste en despejar en una de las ecuaciones cualquier inc√≥gnita, preferiblemente la que tenga menor coeficiente y a continuaci√≥n sustituirla en otra ecuaci√≥n por su valor.\nEl m√©todo de igualacion: El cual se puede entender como un caso particular del m√©todo de sustituci√≥n en el que se despeja la misma inc√≥gnita en dos ecuaciones y a continuaci√≥n se igualan entre s√≠ la parte derecha de ambas ecuaciones.\nEl m√©todo de reduccion: El procedimiento de este m√©todo consiste en transformar una de las ecuaciones (generalmente, mediante productos), de manera que obtengamos dos ecuaciones en la que una misma inc√≥gnita aparezca con el mismo coeficiente y distinto signo. A continuaci√≥n, se suman ambas ecuaciones produci√©ndose as√≠ la reducci√≥n o cancelaci√≥n de dicha inc√≥gnita, obteniendo una ecuaci√≥n con una sola inc√≥gnita, donde el m√©todo de resoluci√≥n es simple.\nEl m√©todo gr√°fico: Que consiste en construir el gr√°fica de cada una de las ecuaciones del sistema. Este m√©todo (manualmente aplicado) solo resulta eficiente en el plano cartesiano (solo dos inc√≥gnitas).\nEl m√©todo de Gauss: El m√©todo de eliminaci√≥n de Gauss o simplemente m√©todo de Gauss consiste en convertir un sistema lineal de n ecuaciones con n inc√≥gnitas, en uno escalonado, en el que la primera ecuaci√≥n tiene n inc√≥gnitas, la segunda ecuaci√≥n tiene n - 1 inc√≥gnitas, \u0026hellip;, hasta la √∫ltima ecuaci√≥n, que tiene 1 inc√≥gnita. De esta forma, ser√° f√°cil partir de la √∫ltima ecuaci√≥n e ir subiendo para calcular el valor de las dem√°s inc√≥gnitas.\nEl m√©todo de Eliminaci√≥n de Gauss-Jordan: El cual es una variante del m√©todo anterior, y consistente en triangular la matriz aumentada del sistema mediante transformaciones elementales, hasta obtener ecuaciones de una sola inc√≥gnita.\nEl m√©todo de Cramer: El cual consiste en aplicar la regla de Cramer para resolver el sistema. Este m√©todo solo se puede aplicar cuando la matriz de coeficientes del sistema es cuadrada y de determinante no nulo.\nLa idea no es explicar cada uno de estos m√©todos, sino saber que existen y que Python nos hacer la vida mucho m√°s f√°cil, ya que para resolver un sistema de ecuaciones simplemente debemos llamar a la funci√≥n solve().\nPor ejemplo, para resolver este sistema de 3 ecuaciones y 3 inc√≥gnitas.\n$$ x + 2y + 3z = 6$$ $$ 2x + 5y + 2z = 4$$ $$ 6x - 3y + z = 2$$ Primero armamos la matriz A de coeficientes y la matriz b de resultados y luego utilizamos solve() para resolverla.\n# Creando matriz de coeficientes A = np.array([[1, 2, 3], [2, 5, 2], [6, -3, 1]]) A array([[ 1, 2, 3], [ 2, 5, 2], [ 6, -3, 1]]) # Creando matriz de resultados b = np.array([6, 4, 2]) b array([6, 4, 2]) # Resolviendo sistema de ecuaciones x = np.linalg.solve(A, b) x array([0., 0., 2.]) # Comprobando la solucion A @ x == b array([ True, True, True]) Programaci√≥n lineal# La programaci√≥n lineal estudia las situaciones en las que se exige maximizar o minimizar funciones que se encuentran sujetas a determinadas restricciones.\nConsiste en optimizar (minimizar o maximizar) una funci√≥n lineal, denominada funci√≥n objetivo, de tal forma que las variables de dicha funci√≥n est√©n sujetas a una serie de restricciones que expresamos mediante un sistema de inecuaciones lineales.\nPara resolver un problema de programaci√≥n lineal, debemos seguir los siguientes pasos:\nElegir las inc√≥gnitas.\nEscribir la funci√≥n objetivo en funci√≥n de los datos del problema.\nEscribir las restricciones en forma de sistema de inecuaciones.\nAveriguar el conjunto de soluciones factibles representando gr√°ficamente las restricciones.\nCalcular las coordenadas de los v√©rtices del recinto de soluciones factibles (si son pocos).\nCalcular el valor de la funci√≥n objetivo en cada uno de los v√©rtices para ver en cu√°l de ellos presenta el valor m√°ximo o m√≠nimo seg√∫n nos pida el problema (hay que tener en cuenta aqu√≠ la posible no existencia de soluci√≥n).\nVeamos un ejemplo y como Python nos ayuda a resolverlo en forma sencilla.\nSupongamos que tenemos la siguiente funci√≥n objetivo:\n$$f(x_{1},x_{2})= 50x_{1} + 40x_{2}$$ y las siguientes restricciones:\n$$x_{1} + 1.5x_{2} \\leq 750$$ $$2x_{1} + x_{2} \\leq 1000$$ $$x_{1} \\geq 0$$ $$x_{2} \\geq 0$$ Podemos resolverlo utilizando PuLP, CVXOPT o graficamente (con matplotlib) de la siguiente forma.\n# Resolviendo la optimizacion con pulp from pulp import * # declarando las variables x1 = LpVariable(\u0026#34;x1\u0026#34;, 0, 800) # 0\u0026lt;= x1 \u0026lt;= 40 x2 = LpVariable(\u0026#34;x2\u0026#34;, 0, 1000) # 0\u0026lt;= x2 \u0026lt;= 1000 # definiendo el problema prob = LpProblem(\u0026#34;problem\u0026#34;, LpMaximize) # definiendo las restricciones prob += x1+1.5*x2 \u0026lt;= 750 prob += 2*x1+x2 \u0026lt;= 1000 prob += x1\u0026gt;=0 prob += x2\u0026gt;=0 # definiendo la funcion objetivo a maximizar prob += 50*x1+40*x2 # resolviendo el problema status = prob.solve(GLPK(msg=0)) LpStatus[status] # imprimiendo los resultados (value(x1), value(x2)) (375.0, 250.0) # Resolviendo el problema con cvxopt from cvxopt import matrix, solvers A = matrix([[-1., -2., 1., 0.], # columna de x1 [-1.5, -1., 0., 1.]]) # columna de x2 b = matrix([750., 1000., 0., 0.]) # resultados c = matrix([50., 40.]) # funcion objetivo # resolviendo el problema sol=solvers.lp(c,A,b) pcost dcost gap pres dres k/t 0: -2.5472e+04 -3.6797e+04 5e+03 0e+00 3e-01 1e+00 1: -2.8720e+04 -2.9111e+04 1e+02 2e-16 9e-03 2e+01 2: -2.8750e+04 -2.8754e+04 1e+00 8e-17 9e-05 2e-01 3: -2.8750e+04 -2.8750e+04 1e-02 4e-16 9e-07 2e-03 4: -2.8750e+04 -2.8750e+04 1e-04 9e-17 9e-09 2e-05 Optimal solution found. # imprimiendo la solucion. print(\u0026#39;{0:.2f}, {1:.2f}\u0026#39;.format(sol[\u0026#39;x\u0026#39;][0]*-1, sol[\u0026#39;x\u0026#39;][1]*-1)) 375.00, 250.00 # Resolviendo la optimizacion graficamente. x_vals = np.linspace(0, 800, 10) # 10 valores entre 0 y 800 plt.plot(x_vals, ((750 - x_vals)/1.5)) # grafica x1 + 1.5x2 = 750 plt.plot(x_vals, (1000 - 2*x_vals)) # grafica 2x1 + x2 = 1000 plt.axis(ymin = 0) plt.show() Como podemos ver en el gr√°fico, ambas rectas se cruzan en la soluci√≥n √≥ptima, x1=375 y x2=250.\nCon esto termino esta introducci√≥n al √Ålgebra lineal con Python.\nEspero que hayan disfurtado de este tutorial tanto como yo disfrute escribirlo!\nSaludos!\nEste post fue escrito utilizando IPython notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2015-06-14","id":40,"permalink":"/blog/2015/06/14/algebra-lineal-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Una de las herramientas matem√°ticas m√°s utilizadas en machine learning y data mining es el √Ålgebra lineal; por tanto, si queremos incursionar en el fascinante mundo del aprendizaje autom√°tico y el an√°lisis de datos es importante reforzar los conceptos que forman parte de sus cimientos.","tags":["python","matematica","calculo","matrices","programacion lineal","tensores"],"title":"Algebra Lineal con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nHoy, 14 de Marzo se celebra el d√≠a de Pi(\\(\\pi\\)), esta celebraci√≥n fue una ocurrencia del f√≠sico Larry Shaw, quien eligi√≥ esta fecha por su semejanza con el valor de dos d√≠gitos de Pi. (en el formato de fecha de Estados Unidos, el 14 de Marzo se escribe 3/14). Particularmente este a√±o, se dar√° el fen√≥meno de que ser√° el d√≠a de Pi m√°s preciso del siglo, ya que a las 9:26:53 se formar√≠a el n√∫mero Pi con 9 d√≠gitos de precisi√≥n! (3/14/15 9:26:53). En honor a su d√≠a, voy a dedicar este art√≠culo al n√∫mero \\(\\pi\\).\n¬øQu√© es el n√∫mero \\(\\pi\\)?# El n√∫mero \\(\\pi\\) es uno de los m√°s famosos de la matem√°tica. Mide la relaci√≥n que existe entre la longitud de una circunferencia y su di√°metro. No importa cual sea el tama√±o de la circunferencia, esta relaci√≥n siempre va a ser la misma y va a estar representada por \\(\\pi\\). Este tipo de propiedades, que se mantienen sin cambios cuando otros atributos var√≠an son llamadas constantes. \\(\\pi\\) es una de las constantes utilizadas con mayor frecuencia en matem√°tica, f√≠sica e ingenier√≠a.\nHistoria del n√∫mero \\(\\pi\\)# La primera referencia que se conoce de \\(\\pi\\) data aproximadamente del a√±o 1650 ac en el Papiro de Ahmes, documento que contiene problemas matem√°ticos b√°sicos, fracciones, c√°lculo de √°reas, vol√∫menes, progresiones, repartos proporcionales, reglas de tres, ecuaciones lineales y trigonometr√≠a b√°sica. El valor que se asigna a \\(\\pi\\) en ese documento es el de 28/34 aproximadamente 3,1605.\nUna de las primeras aproximaciones fue la realizada por Arqu√≠medes en el a√±o 250 adC quien calculo que el valor estaba comprendido entre 3 10/71 y 3 1/7 (3,1408 y 3,1452) y utilizo para sus estudios el valor 211875/67441 aproximadamente 3,14163.\nEl matem√°tico Leonhard Euler adopt√≥ el conocido s√≠mbolo \\(\\pi\\) en 1737 en su obra Introducci√≥n al c√°lculo infinitesimal e instant√°neamente se convirti√≥ en una notaci√≥n est√°ndar hasta hoy en d√≠a.\n¬øQu√© hace especial al n√∫mero \\(\\pi\\)?# Lo que convierte a \\(\\pi\\) en un n√∫mero interesante, es que se trata de un n√∫mero irracional, es decir, que el mismo no puede ser expresado como una fraccion de dos n√∫meros enteros. Asimismo, tambi√©n es un n√∫mero trascendental, ya que no es ra√≠z de ninguna ecuaci√≥n algebraica con coeficientes enteros, lo que quiere decir que tampoco puede ser expresado algebraicamente.\nCalculando el valor de \\(\\pi\\)# Si bien el n√∫mero \\(\\pi\\) puede ser observado con facilidad, su c√°lculo es uno de los problemas m√°s dif√≠ciles de la matem√°tica y ha mantenido a los matem√°ticos ocupados por a√±os. Actualmente se conocen hasta 10 billones de decimales del n√∫mero \\(\\pi\\), es decir, 10.000.000.000.000.\nLa aproximaci√≥n de Arqu√≠medes# Uno de los m√©todos m√°s conocidos para la aproximaci√≥n del n√∫mero \\(\\pi\\) es la aproximaci√≥n de Arqu√≠medes; la cual consiste en circunscribir e inscribir pol√≠gonos regulares de n-lados en circunferencias y calcular el per√≠metro de dichos pol√≠gonos. Arqu√≠medes empez√≥ con hex√°gonos circunscritos e inscritos, y fue doblando el n√∫mero de lados hasta llegar a pol√≠gonos de 96 lados.\nLa serie de Leibniz# Otro m√©todo bastante popular para el c√°lculo de \\(\\pi\\), es la utilizaci√≥n de las series infinitas de Gregory-Leibniz. Este m√©todo consiste en ir realizando operaciones matematicas sobre series infinitas de n√∫meros hasta que la serie converge en el n√∫mero \\(\\pi\\). Aunque no es muy eficiente, se acerca cada vez m√°s al valor de Pi en cada repetici√≥n, produciendo con precisi√≥n hasta cinco mil decimales de Pi con 500000 repeticiones. Su formula es muy simple.\n$$\\pi=(4/1) - (4/3) + (4/5) - (4/7) + (4/9) - (4/11) + (4/13) - (4/15) ...$$ Calculando \\(\\pi\\) con Python# Como este blog lo tengo dedicado a Python, obviamente no pod√≠a concluir este art√≠culo sin incluir distintas formas de calcular \\(\\pi\\) utilizando Python; el cual bien es sabido que se adapta m√°s que bien para las matem√°ticas!.\nComo \\(\\pi\\) es una constante con un gran n√∫mero de sus d√≠gitos ya conocidos, los principales m√≥dulos Matem√°ticos de Python ya incluyen su valor en una variable. As√≠ por ejemplo, podemos ver el valor de \\(\\pi\\) importando los m√≥dulos math o sympy.\n# Pi utilizando el m√≥dulo math import math math.pi 3.141592653589793 # Pi utiizando sympy, dps nos permite variar el n√∫mero de d√≠gitos de Pi from sympy.mpmath import mp mp.dps = 33 # n√∫mero de d√≠gitos print(mp.pi) 3.1415926535897932384626433832795 Si queremos calcular alguna aproximaci√≥n al valor de \\(\\pi\\), podr√≠amos implementar por ejemplo la aproximaci√≥n de Arqu√≠medes de la siguiente manera.\n# Implementacion de aproximaci√≥n de Arqu√≠medes from decimal import Decimal, getcontext def pi_archimedes(digitos): \u0026#34;\u0026#34;\u0026#34; Calcula pi utilizando el m√©todo de aproximacion de Arqu√≠medes en n iteraciones. \u0026#34;\u0026#34;\u0026#34; def pi_archimedes_iter(n): \u0026#34;\u0026#34;\u0026#34;funcion auxiliar utilizada en cada iteracion\u0026#34;\u0026#34;\u0026#34; polygon_edge_length_squared = Decimal(2) polygon_sides = 2 for i in range(n): polygon_edge_length_squared = 2 - 2 * (1 - polygon_edge_length_squared / 4).sqrt() polygon_sides *= 2 return polygon_sides * polygon_edge_length_squared.sqrt() #itera dependiendo de la cantidad de digitos old_result = None for n in range(10*digitos): # Calcular con doble precision getcontext().prec = 2*digitos result = pi_archimedes_iter(n) # Devolver resultados en precision simple. getcontext().prec = digitos result = +result # redondeo del resultado. if result == old_result: return result old_result = result # Aproximacion de Arqu√≠medes con 33 d√≠gitos print(pi_archimedes(33)) 3.14159265358979323846264338327950 Por √∫ltimo, tambi√©n podr√≠amos implementar las series infinitas de Gregory-Leibniz, lo cual es realmente bastante sencillo.\ndef pi_leibniz(precision): \u0026#34;\u0026#34;\u0026#34;Calcula Pi utilizando las series infinitas de Gregory-Leibniz\u0026#34;\u0026#34;\u0026#34; pi = 0 modificador = 1 for i in range(1, precision, 2): pi += ((4 / i) * modificador) modificador *= -1 return pi # Pi con una precision de 10000000 repeticiones. print(pi_leibniz(10000000)) 3.1415924535897797 Como se puede observar, el m√©todo de las series infinitas de Leibniz, si bien es de f√°cil implementaci√≥n, no es muy preciso adem√°s de ser sumamente ineficiente.\nCon esto concluyo y a festejar el d√≠a de \\(\\pi\\)!!\nSaludos!\nEste post fue escrito utilizando IPython notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2015-03-14","id":41,"permalink":"/blog/2015/03/14/el-dia-pi/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nHoy, 14 de Marzo se celebra el d√≠a de Pi(\\(\\pi\\)), esta celebraci√≥n fue una ocurrencia del f√≠sico Larry Shaw, quien eligi√≥ esta fecha por su semejanza con el valor de dos d√≠gitos de Pi. (en el formato de fecha de Estados Unidos, el 14 de Marzo se escribe 3/14).","tags":["python","matematica","pi","finanazas"],"title":"El dia Pi"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Es bien sabido que existen muchas formas de resolver un mismo problema, esto, llevado al mundo de la programaci√≥n, a generado que existan o co-existan diferentes estilos en los que podemos programar, los cuales son llamados generalmente paradigmas. As√≠, podemos encontrar basicamente 4 paradigmas principales de programaci√≥n:\nProgramaci√≥n imperativa: Este suele ser el primer paradigma con el que nos encontramos, el mismo describe a la programaci√≥n en t√©rminos de un conjunto de intrucciones que modifican el estado del programa y especifican claramente c√≥mo se deben realizar las cosas y modificar ese estado. Este paradigma esta representado por el lenguaje C.\nProgramaci√≥n l√≥gica: En este paradigma los programas son escritos en forma declarativa utilizando expresiones l√≥gicas. El principal exponente es el lenguaje Prolog (programar en este esot√©rico lenguaje suele ser una experiencia interesante!).\nProgramaci√≥n Orientada a Objetos: La idea b√°sica detr√°s de este paradigma es que tanto los datos como las funciones que operan sobre estos datos deben estar contenidos en un mismo objeto. Estos objetos son entidades que tienen un determinado estado, comportamiento (m√©todo) e identidad. La Programaci√≥n Orientada a Objetos es sumamente utilizada en el desarrollo de software actual; uno de sus principales impulsores es el lenguaje de programaci√≥n Java.\nProgramaci√≥n Funcional: Este √∫ltimo paradigma enfatiza la utilizaci√≥n de funciones puras, es decir, funciones que no tengan efectos secundarios, que no manejan datos mutables o de estado. Esta en clara contraposici√≥n con la programaci√≥n imperativa. Uno de sus principales representantes es el lenguaje Haskell (lenguaje, que compite en belleza, elegancia y expresividad con Python!).\nLa mayor√≠a de los lenguajes modernos son multiparadigma, es decir, nos permiten programar utilizando m√°s de uno de los paradigmas arriba descritos. En este art√≠culo voy a intentar explicar como podemos aplicar la Programaci√≥n Funcional con Python.\n¬øPor qu√© Programaci√≥n Funcional?# En estos √∫ltimos a√±os hemos visto el resurgimiento de la Programaci√≥n Funcional, nuevos lenguajes como Scala y Apple Swift ya traen por defecto montones de herramientas para facilitar el paradigma funcional. La principales razones del crecimiento de la popularidad de la Programaci√≥n Funcional son:\nLos programas escritos en un estilo funcional son m√°s f√°ciles de testear y depurar. Por su caracter√≠stica modular facilita la computaci√≥n concurrente y paralela; permitiendonos obtener muchas m√°s ventajas de los procesadores multin√∫cleo modernos. El estilo funcional se lleva muy bien con los datos; permitiendonos crear algoritmos y programas m√°s expresivos para manejar la enorme cantidad de datos de la Big Data.(Aplicar el estilo funcional me suele recordar a utilizar las formulas en Excel). Programaci√≥n Funcional con Python# Antes de comenzar con ejemplos les voy a mencionar algunos de los modulos que que nos facilitan la Programaci√≥n Funcional en Python, ellos son:\nIntertools: Este es un modulo que viene ya instalado con la distribuci√≥n oficial de Python; nos brinda un gran n√∫mero de herramientas para facilitarnos la creaci√≥n de iteradores.\nOperator: Este modulo tambi√©n la vamos a encontrar ya instalado con Python, en el vamos a poder encontrar a los principales operadores de Python convertidos en funciones.\nFunctools: Tambi√©n ya incluido dentro de Python este modulo nos ayuda a crear Funciones de orden superior, es decir, funciones que actuan sobre o nos devuelven otras funciones.\nFn: Este modulo, creado por Alexey Kachayev, brinda a Python las \u0026ldquo;bater√≠as\u0026rdquo; adicionales para hacer el estilo funcional de programaci√≥n mucho m√°s f√°cil.\nCytoolz: Modulo creado por Erik Welch que tambi√©n nos proporciona varias herramientas para la Programaci√≥n Funcional, especialmente orientado a operaciones de an√°lisis de datos.\nMacropy: Este modulo, creado por Li Haoyi trae a Python caracter√≠sticas propias de los lenguajes puramente funcionales, como ser, pattern matching, tail call optimization, y case classes.\nEjemplos# Utilizando Map, Reduce, Filter y Zip# Cuando tenemos que realizar operaciones sobre listas, en lugar de utilizar los cl√°sicos loops, podemos utilizar las funciones Map, Reduce, Filter y Zip.\nMap# La funci√≥n Map nos permite aplicar una operaci√≥n sobre cada uno de los items de una lista. El primer argumento es la funci√≥n que vamos a aplicar y el segundo argumento es la lista.\n#creamos una lista de n√∫meros del 1 al 10 items = list(xrange(1, 11)) items [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] #creamos una lista de los cuadrados de la lista items. #forma imperativa. cuadrados = [] for i in items: cuadrados.append(i ** 2) cuadrados [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] #Cuadrados utilizando Map. #forma funcional cuadrados = map(lambda x: x **2, items) cuadrados [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] Como podemos ver, al utilizar map las l√≠neas de c√≥digo se reducen y nuestro programa es mucho m√°s simple de comprender. En el ejemplo le estamos pasando a map una funci√≥n an√≥nima o lambda. Esta es otra caracter√≠stica que nos ofrece Python para la Programaci√≥n Funcional. Map tambi√©n puede ser utilizado con funciones de m√°s de un argumento y m√°s de una lista, por ejemplo:\n#importamos pow. from math import pow #como vemos la funci√≥n pow toma dos argumentos, un n√∫mero y su potencia. pow(2, 3) 8.0 #si tenemos las siguientes listas numeros = [2, 3, 4] potencias = [3, 2, 4] #podemos aplicar map con pow y las dos listas. #nos devolvera una sola lista con las potencias aplicadas sobre los n√∫meros. potenciados = map(pow, numeros, potencias) potenciados [8.0, 9.0, 256.0] Reduce# La funci√≥n Reduce reduce los valores de la lista a un solo valor aplicando una funcion reductora. El primer argumento es la funci√≥n reductora que vamos a aplicar y el segundo argumento es la lista.\n#Sumando los valores de la lista items. #forma imperativa suma = 0 for i in items: suma += i suma 55 #Suma utilizando Reduce. #Forma funcional from functools import reduce #en python3 reduce se encuentra en modulo functools suma = reduce(lambda x, y: x + y, items) suma 55 La funci√≥n Reduce tambi√©n cuenta con un tercer argumento que es el valor inicial o default. Por ejemplo si quisi√©ramos sumarle 10 a la suma de los elementos de la lista items, solo tendr√≠amos que agregar el tercer argumento.\n#10 + suma items suma10 = reduce(lambda x, y: x + y, items, 10) suma10 65 Filter# La funci√≥n Filter nos ofrece una forma muy elegante de filtrar elementos de una lista.El primer argumento es la funci√≥n filtradora que vamos a aplicar y el segundo argumento es la lista.\n#Numeros pares de la lista items. #Forma imperativa. pares = [] for i in items: if i % 2 ==0: pares.append(i) pares [2, 4, 6, 8, 10] #Pares utilizando Filter #Forma funcional. pares = filter(lambda x: x % 2 == 0, items) pares [2, 4, 6, 8, 10] Zip# Zip es una funci√≥n para reorganizar listas. Como par√°metros admite un conjunto de listas. Lo hace es tomar el elemento i√©simo de cada lista y unirlos en una tupla, despu√©s une todas las tuplas en una sola lista.\n#Ejemplo de zip nombres = [\u0026#34;Raul\u0026#34;, \u0026#34;Pedro\u0026#34;, \u0026#34;Sofia\u0026#34;] apellidos = [\u0026#34;Lopez Briega\u0026#34;, \u0026#34;Perez\u0026#34;, \u0026#34;Gonzalez\u0026#34;] #zip une cada nombre con su apellido en una lista de tuplas. nombreApellido = zip(nombres, apellidos) nombreApellido [('Raul', 'Lopez Briega'), ('Pedro', 'Perez'), ('Sofia', 'Gonzalez')] Removiendo Efectos Secundarios# Una de las buenas practicas que hace al estilo funcional es siempre tratar de evitar los efectos secundarios, es decir, evitar que nuestras funciones modifiquen los valores de sus par√°metros, as√≠ en lugar de escribir c√≥digo como el siguiente:\n#Funcion que no sigue las buenas practias de la programacion funcional. #Esta funcion tiene efectos secundarios, ya que modifica la lista que se le pasa como argumento. def cuadrados(lista): for i, v in enumerate(lista): lista[i] = v ** 2 return lista Deber√≠amos escribir c√≥digo como el siguiente, el cual evita los efectos secundarios:\n#Version funcional de la funcion anterior. def fcuadrados(lista): return map(lambda x: x ** 2, lista) #Aplicando fcuadrados sobre items. fcuadrados(items) [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] #items no se modifico items [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] #aplicando cuadrados sobre items cuadrados(items) [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] #Esta funci√≥n tiene efecto secundario. #items fue modificado por cuadrados. items [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] Al escribir funciones que no tengan efectos secundarios nos vamos a ahorrar muchos dolores de cabeza ocasionados por la modificaci√≥n involuntaria de objetos.\nUtilizando el modulo Fn.py# Algunas de las cosas que nos ofrece este modulo son: Estructuras de datos inmutables, lambdas al estilo de Scala, lazy evaluation de streams, nuevas Funciones de orden superior, entre otras.\n#Lambdas al estilo scala from fn import _ (_ + _)(10, 3) 13 items = list(xrange(1,11)) cuadrados = map( _ ** 2, items) cuadrados [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] #Streams from fn import Stream s = Stream() \u0026lt;\u0026lt; [1,2,3,4,5] s \u0026lt;fn.stream.Stream at 0x7f873c1d7a70\u0026gt; list(s) [1, 2, 3, 4, 5] s[1] 2 s \u0026lt;\u0026lt; [6, 7, 8, 9] \u0026lt;fn.stream.Stream at 0x7f873c1d7a70\u0026gt; s[6] 7 #Stream fibonacci from fn.iters import take, drop, map as imap from operator import add f = Stream() fib = f \u0026lt;\u0026lt; [0, 1] \u0026lt;\u0026lt; imap(add, f, drop(1, f)) #primeros 10 elementos de fibonacci list(take(10, fib)) [0, 1, 1, 2, 3, 5, 8, 13, 21, 34] #elemento 20 de la secuencia fibonacci fib[20] 6765 #elementos 40 al 45 de la secuencia fibonacci list(fib[40:45]) [102334155, 165580141, 267914296, 433494437, 701408733] #Funciones de orden superior from fn import F from operator import add, mul #operadores de suma y multiplicacion #composici√≥n de funciones F(add, 1)(10) 11 #f es una funcion que llama a otra funcion. f = F(add, 5) \u0026lt;\u0026lt; F(mul, 100) #\u0026lt;\u0026lt; operador de composicion de funciones. #cada valor de la lista primero se multiplica por 100 y luego #se le suma 5, segun composicion de f de arriba. map(f, [0, 1, 2, 3]) [5, 105, 205, 305] func = F() \u0026gt;\u0026gt; (filter, _ \u0026lt; 6) \u0026gt;\u0026gt; sum #func primero filtra los valores menores a 6 #y luego los suma. func(xrange(10)) 15 Utilizando el modulo cytoolz# Este modulo nos provee varias herramienta para trabajar con funciones, iteradores y diccionarios.\n#Datos a utilizar en los ejemplos cuentas = [(1, \u0026#39;Alice\u0026#39;, 100, \u0026#39;F\u0026#39;), # id, nombre, balance, sexo (2, \u0026#39;Bob\u0026#39;, 200, \u0026#39;M\u0026#39;), (3, \u0026#39;Charlie\u0026#39;, 150, \u0026#39;M\u0026#39;), (4, \u0026#39;Dennis\u0026#39;, 50, \u0026#39;M\u0026#39;), (5, \u0026#39;Edith\u0026#39;, 300, \u0026#39;F\u0026#39;)] from cytoolz.curried import pipe, map as cmap, filter as cfilter, get #seleccionando el id y el nombre de los que tienen un balance mayor a 150 pipe(cuentas, cfilter(lambda (id, nombre, balance, sexo): balance \u0026gt; 150), cmap(get([1, 2])), list) [('Bob', 200), ('Edith', 300)] #este mismo resultado tambien lo podemos lograr con las listas por comprensi√≥n. #mas pythonico. [(nombre, balance) for (id, nombre, balance, sexo) in cuentas if balance \u0026gt; 150] [('Bob', 200), ('Edith', 300)] from cytoolz import groupby #agrupando por sexo groupby(get(3), cuentas) {'F': [(1, 'Alice', 100, 'F'), (5, 'Edith', 300, 'F')], 'M': [(2, 'Bob', 200, 'M'), (3, 'Charlie', 150, 'M'), (4, 'Dennis', 50, 'M')]} #utilizando reduceby from cytoolz import reduceby def iseven(n): return n % 2 == 0 def add(x, y): return x + y reduceby(iseven, add, [1, 2, 3, 4]) {False: 4, True: 6} Ordenando objectos con operator itemgetter, attrgetter y methodcaller# Existen tres funciones dignas de menci√≥n en el modulo operator, las cuales nos permiten ordenar todo tipo de objetos en forma muy sencilla, ellas son itemgetter, attrgetter y methodcaller.\n#Datos para los ejemplos estudiantes_tupla = [ (\u0026#39;john\u0026#39;, \u0026#39;A\u0026#39;, 15), (\u0026#39;jane\u0026#39;, \u0026#39;B\u0026#39;, 12), (\u0026#39;dave\u0026#39;, \u0026#39;B\u0026#39;, 10), ] class Estudiante: def __init__(self, nombre, nota, edad): self.nombre = nombre self.nota = nota self.edad = edad def __repr__(self): return repr((self.nombre, self.nota, self.edad)) def nota_ponderada(self): return \u0026#39;CBA\u0026#39;.index(self.nota) / float(self.edad) estudiantes_objeto = [ Estudiante(\u0026#39;john\u0026#39;, \u0026#39;A\u0026#39;, 15), Estudiante(\u0026#39;jane\u0026#39;, \u0026#39;B\u0026#39;, 12), Estudiante(\u0026#39;dave\u0026#39;, \u0026#39;B\u0026#39;, 10), ] from operator import itemgetter, attrgetter, methodcaller #ordenar por edad tupla sorted(estudiantes_tupla, key=itemgetter(2)) [('dave', 'B', 10), ('jane', 'B', 12), ('john', 'A', 15)] #ordenar por edad objetos sorted(estudiantes_objeto, key=attrgetter(\u0026#39;edad\u0026#39;)) [('dave', 'B', 10), ('jane', 'B', 12), ('john', 'A', 15)] #ordenar por nota y edad tupla sorted(estudiantes_tupla, key=itemgetter(1,2)) [('john', 'A', 15), ('dave', 'B', 10), ('jane', 'B', 12)] #ordenar por nota y edad objetos sorted(estudiantes_objeto, key=attrgetter(\u0026#39;nota\u0026#39;, \u0026#39;edad\u0026#39;)) [('john', 'A', 15), ('dave', 'B', 10), ('jane', 'B', 12)] #ordenando por el resultado del metodo nota_ponderada sorted(estudiantes_objeto, key=methodcaller(\u0026#39;nota_ponderada\u0026#39;)) [('jane', 'B', 12), ('dave', 'B', 10), ('john', 'A', 15)] Hasta aqu√≠ llega esta introducci√≥n. Tengan en cuenta que Python no es un lenguaje puramente funcional, por lo que algunas soluciones pueden verse m√°s como un hack y no ser del todo pythonicas. El concepto m√°s importante es el de evitar los efectos secundarios en nuestras funciones. Debemos mantener un equilibrio entre los diferentes paradigmas y utilizar las opciones que nos ofrece Python que haga m√°s legible nuestro c√≥digo. Para m√°s informaci√≥n sobre la Programaci√≥n Funcional en Python tambi√©n puede visitar el siguiente documento y darse una vuelta por la documentaci√≥n de los m√≥dulos mencionados m√°s arriba. Por √∫ltimo, los que quieran incursionar con un lenguaje puramente funcional, les recomiendo Haskell.\nSaludos!\nEste post fue escrito utilizando IPython notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2015-02-01","id":42,"permalink":"/blog/2015/02/01/programacion-funcional-con-python/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nIntroducci√≥n# Es bien sabido que existen muchas formas de resolver un mismo problema, esto, llevado al mundo de la programaci√≥n, a generado que existan o co-existan diferentes estilos en los que podemos programar, los cuales son llamados generalmente paradigmas. As√≠, podemos encontrar basicamente 4 paradigmas principales de programaci√≥n:","tags":["python","programacion","analisis de datos","funcional"],"title":"Programaci√≥n Funcional con Python"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nUna de las nuevas estrellas en el an√°lisis de datos masivos es Apache Spark. Desarrollado en Scala, Apache Spark es una plataforma de computaci√≥n de c√≥digo abierto para el an√°lisis y procesamiento de grandes vol√∫menes de datos.\nAlgunas de las ventajas que nos ofrece Apache Spark sobre otros frameworks, son:\nVelocidad: Sin dudas la velocidad es una de las principales fortalezas de Apache Spark, como esta dise√±ado para soportar el procesameinto en memoria, puede alcanzar una performance sorprendente en an√°lisis avanzados de datos. Algunos programas escritos utilizando Apache Spark, pueden correr hasta 100x m√°s r√°pido que utilizando Hadoop. F√°cil de usar: Podemos escribir programas en Python, Scala o Java que hagan uso de las herramientas que ofrece Apache Spark; asimismo nos permite trabajar en forma interactiva (con Python o con Scala) y su API es muy f√°cil de aprender. Generalismo: El mundo del an√°lisis de datos incluye muchos subgrupos de distinta √≠ndole, est√°n los que hacen un an√°lisis investigativo, los que que realizan an√°lisis exploratorios, los que construyen sistemas de procesamientos de datos, etc. Los usuarios de cada uno de esos subgrupos, al tener objetivos distintos, suelen utilizar una gran variedad de herramientas totalmente diferentes. Apache Spark nos proporciona un gran n√∫mero de herramientas de alto nivel como Spark SQL, MLlib para machine learning, GraphX, y Spark Streaming; las cuales pueden ser combinadas para crear aplicaciones multiprop√≥sito que ataquen los diferentes dominios del an√°lisis de datos. RDD o Resilient Distributed Datasets# En el coraz√≥n de Apache Spark se encuentran los RDDs. Los RDDs son una abstracci√≥n distribuida que le permite a los programadores realizar c√≥mputos en memoria sobre grandes clusters de computadoras sin errores o p√©rdidas de informaci√≥n. Est√°n especialmente dise√±ados para el an√°lisis de datos interactivo (data mining) y para la aplicaci√≥n de algoritmos iterativos (MapReduce). En ambos casos, mantener los datos en la memoria puede mejorar el rendimiento en una gran proporci√≥n. Para lograr la tolerancia a fallos de manera eficiente, RDDs utiliza una forma restringida de memoria compartida. Los RDDs son los suficientemente expresivos como para capturar una gran variedad de c√°lculos.\nInstalando Apache Spark# Para instalar Apache Spark en forma local y poder comenzar a utilizarlo, pueden seguir los siguientes pasos:\nEn primer lugar, necesitamos tener instalado Oracle JDK. Para instalarlo en Ubuntu podemos utilizar los siguientes comandos: $ sudo add-apt-repository ppa:webupd8team/java $ sudo apt-get update $ sudo apt-get install oracle-jdk7-installer Luego nos instalamos las herramientas para trabajar con Scala, SBT con el siguiente comando: $ sudo apt-get install sbt Despu√©s nos descargamos la √∫ltima versi√≥n de Apache Spark desde aqu√≠ Ahora descomprimimos el archivo: $ tar -xvf spark-1.0.2.tgz Nos movemos a la carpeta reci√©n descomprimida y realizamos la compilaci√≥n de Apache Spark utilizando SBT. (tener paciencia, es sabido que Scala tarda mucho en compilar) $ cd spark-1.0.2 $ sbt/sbt assembly Opcionalmente, para facilitar la utilizaci√≥n de Apache Spark desde la l√≠nea de comando, yo modifique mi archivo .bashrc para incluir los siguientes alias: $ echo \"alias ipyspark='IPYTHON_OPTS=\"notebook --pylab inline\" ~/spark-1.0.2/bin/pyspark'\" \u003e\u003e ~/.bashrc $ echo \"alias pyspark='~/spark-1.0.2/bin/pyspark'\" \u003e\u003e ~/.bashrc $ echo \"alias spark='~/spark-1.0.2/bin/spark-shell'\" \u003e\u003e ~/.bashrc Ahora simplemente tipeando pyspark nos abre el interprete interactivo de Python con Apache Spark, tipeando spark nos abre el interprete interactivo de Scala, y tipeando ipyspark nos abre el notebook de Ipython integrado con Apache Spark! $ ./bin/pyspark #o pyspark si creamos el alias Ejemplo de utilizaci√≥n de Spark con Ipython# Ahora lleg√≥ el momento de ensuciarse las manos y probar Apache Spark, vamos a hacer el t√≠pico ejercicio de wordcounts; en este caso vamos a contar todas las palabras que posee la Biblia en su versi√≥n en ingl√©s y vamos a ver cuantas veces aparece la palabra Dios(God). Para esto nos descargamos la versi√≥n de la Biblia en texto plano del proyecto Gutemberg.\n# Comenzamos con algunos imports. # No necesitamos importar pyspark porque ya se autoimporta como sc. from operator import add import pandas as pd lineas = sc.textFile(\u0026#34;/home/raul/spark-1.0.2/examples/data/Bible.txt\u0026#34;) # usamos la funci√≥n textFile para subir el texto a Spark # Mapeamos las funciones para realizar la cuenta de las palabras y generamos el RDD. cuenta = lineas.flatMap(lambda x: x.split(\u0026#39; \u0026#39;)) \\ .map(lambda x: (x.replace(\u0026#39;,\u0026#39; , \u0026#39;\u0026#39;).upper(), 1)) \\ .reduceByKey(add) # Creamos la lista con las palabras y su respectiva frecuencia. lista = cuenta.collect() # Creamos un DataFrame de Pandas para facilitar el manejo de los datos. dataframe = pd.DataFrame(lista, columns=[\u0026#39;palabras\u0026#39;, \u0026#39;cuenta\u0026#39;]) # Nos quedamos solo con las palabras que hacen referencia a Dios god = dataframe[dataframe[\u0026#39;palabras\u0026#39;].apply(lambda x: x.upper() in [\u0026#39;GOD\u0026#39;, \u0026#39;LORD\u0026#39;, \u0026#39;JESUS\u0026#39;, \u0026#39;FATHER\u0026#39;])] god palabras cuenta 2867 FATHER 814 7329 GOD 3330 8902 LORD 6448 21404 JESUS 893 # Realizamos un gr√°fico de barras sobre los datos god.set_index(\u0026#39;palabras\u0026#39;).plot(kind = \u0026#39;bar\u0026#39;) \u0026lt;matplotlib.axes.AxesSubplot at 0x7f071e10ea10\u0026gt; # Realizamos la sumatoria de las 4 palabras combinadas god.sum() palabras FATHERGODLORDJESUS cuenta 11485 dtype: object Como demuestra el ejemplo, Dios ser√≠a nombrado en la Biblia, ya sea como lord, god, jesus o father; unas 11485 veces!\nConclusi√≥n# Apache Spark es realmente una herramienta muy prometedora, con ella podemos analizar datos con un rendimiento muy alto y combinado con otras herramientas como Python, Numpy, Pandas e IPython; se convierten en un framework sumamente completo y efectivo para el an√°lisis de grandes vol√∫menes de datos en forma muy sencilla.\nPara m√°s informaci√≥n sobre Apache Spark, pueden visitar su la secci√≥n de ejemplos de su p√°gina oficial.\nEspero les haya sido de utilidad este notebook.\nSaludos!!\nEste post fue escrito utilizando IPython notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2014-08-22","id":43,"permalink":"/blog/2014/08/22/ipython-y-spark-para-el-analisis-de-datos/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nUna de las nuevas estrellas en el an√°lisis de datos masivos es Apache Spark. Desarrollado en Scala, Apache Spark es una plataforma de computaci√≥n de c√≥digo abierto para el an√°lisis y procesamiento de grandes vol√∫menes de datos.\nAlgunas de las ventajas que nos ofrece Apache Spark sobre otros frameworks, son:","tags":["python","map-reduce","programacion","machine learning","analisis de datos"],"title":"Ipython y Spark para el analisis de datos"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nUna de las razones por la que solemos amar a Python, es por su sistema de tipado din√°mico, el cual lo convierte en un lenguaje de programaci√≥n sumamente flexible y f√°cil de aprender; al no tener que preocuparnos por definir los tipos de los objetos, ya que Python los infiere por nosotros, podemos escribir programas en una forma mucho m√°s productiva, sin verbosidad y utilizando menos l√≠neas de c√≥digo.\nAhora bien, este sistema de tipado din√°mico tambi√©n puede convertirse en una pesadilla en proyectos de gran escala, requiriendo varias horas de pruebas unitarias para evitar que los objetos adquieran un tipo de datos que no deber√≠an y complicando el su mantenimiento o futura refactorizaci√≥n.\nPor ejemplo, en un c√≥digo tan trivial como el siguiente:\ndef saludo(nombre): return \u0026#39;Hola {}\u0026#39;.format(nombre) Esta simple funci√≥n nos va a devolver el texto \u0026lsquo;Hola\u0026rsquo; seguido del nombre que le ingresemos; pero como no contiene ning√∫n control sobre el tipo de datos que pude admitir la variable nombre, los siguientes casos ser√≠an igualmente v√°lidos:\nprint (saludo(\u0026#39;Raul\u0026#39;)) print (saludo(1)) Hola Raul Hola 1 En cambio, si pusi√©ramos un control sobre el tipo de datos que admitiera la variable nombre, para que siempre fuera un string, entonces el segundo caso ya no ser√≠a v√°lido y lo podr√≠amos detectar f√°cilmente antes de que nuestro programa se llegara a ejecutar.\nObviamente, para poder detectar el segundo error y que nuestra funci√≥n saludo solo admita una variable del tipo string como argumento, podr√≠amos reescribir nuestra funci√≥n, agregando un control del tipo de datos de la siguiente manera:\ndef saludo(nombre): if type(nombre) != str: return \u0026#34;Error: el argumento debe ser del tipo String(str)\u0026#34; return \u0026#39;Hola {}\u0026#39;.format(nombre) print(saludo(\u0026#39;Raul\u0026#39;)) print(saludo(1)) Hola Raul Error: el argumento debe ser del tipo String(str) Pero una soluci√≥n m√°s sencilla a tener que ir escribiendo condiciones para controlar los tipos de las variables o de las funciones es utilizar MyPy\nMyPy# MyPy es un proyecto que busca combinar los beneficios de un sistema de tipado din√°mico con los de uno de tipado est√°tico. Su meta es tener el poder y la expresividad de Python combinada con los beneficios que otorga el chequeo de los tipos de datos al momento de la compilaci√≥n.\nAlgunos de los beneficios que proporciona utilizar MyPy son:\nChequeo de tipos al momento de la compilaci√≥n: Un sistema de tipado est√°tico hace m√°s f√°cil detectar errores y con menos esfuerzo de debugging. Facilita el mantenimiento: Las declaraciones expl√≠citas de tipos act√∫an como documentaci√≥n, haciendo que nuestro c√≥digo sea m√°s f√°cil de entender y de modificar sin introducir nuevos errores. Permite crecer nuestro programa desde un tipado din√°mico hacia uno est√°tico: Nos permite comenzar desarrollando nuestros programas con un tipado din√°mico y a mediada que el mismo vaya madurando podr√≠amos modificarlo hacia un tipado est√°tico de forma muy sencilla. De esta manera, podr√≠amos beneficiarnos no solo de la comodidad de tipado din√°mico en el desarrollo inicial, sino tambi√©n aprovecharnos de los beneficios de los tipos est√°ticos cuando el c√≥digo crece en tama√±o y complejidad. Tipos de datos# Estos son algunos de los tipos de datos m√°s comunes que podemos encontrar en Python:\nint: N√∫mero entero de tama√±o arbitrario float: N√∫mero flotante. bool: Valor booleano (True o False) str: Unicode string bytes: 8-bit string object: Clase base del que derivan todos los objecto en Python. List[str]: lista de objetos del tipo string. Dict[str, int]: Diccionario de string hacia enteros Iterable[int]: Objeto iterable que contiene solo enteros. Sequence[bool]: Secuencia de valores booleanos Any: Admite cualquier valor. (tipado din√°mico) El tipo Any y los constructores List, Dict, Iterable y Sequence est√°n definidos en el modulo typing que viene junto con MyPy.\nEjemplos# Por ejemplo, volviendo al ejemplo del comienzo, podr√≠amos reescribir la funci√≥n saludo utilizando MyPy de forma tal que los tipos de datos sean expl√≠citos y puedan ser chequeados al momento de la compilaci√≥n.\n%%writefile typeTest.py import typing def saludo(nombre: str) -\u0026gt; str: return \u0026#39;Hola {}\u0026#39;.format(nombre) print(saludo(\u0026#39;Raul\u0026#39;)) print(saludo(1)) Overwriting typeTest.py En este ejemplo estoy creando un peque√±o script y guardando en un archivo con el nombre \u0026rsquo;typeTest.py\u0026rsquo;, en la primer l√≠nea del script estoy importando la librer√≠a typing que viene con MyPy y es la que nos agrega la funcionalidad del chequeo de los tipos de datos. Luego simplemente ejecutamos este script utilizando el interprete de MyPy y podemos ver que nos va a detectar el error de tipo de datos en la segunda llamada a la funci√≥n saludo.\n!mypy typeTest.py typeTest.py, line 7: Argument 1 to \u0026quot;saludo\u0026quot; has incompatible type \u0026quot;int\u0026quot;; expected \u0026quot;str\u0026quot; Si ejecut√°ramos este mismo script utilizando el interprete de Python, veremos que obtendremos los mismos resultados que al comienzo de este notebook; lo que quiere decir, que la sintaxis que utilizamos al reescribir nuestra funci√≥n saludo es c√≥digo Python perfectamente v√°lido!\n!python3 typeTest.py Hola Raul Hola 1 Tipado explicito para variables y colecciones# En el ejemplo anterior, vimos como es la sintaxis para asignarle un tipo de datos a una funci√≥n, la cual utiliza la sintaxis de Python3, annotations.\nSi quisi√©ramos asignarle un tipo a una variable, podr√≠amos utilizar la funci√≥n Undefined que viene junto con MyPy.\n%%writefile typeTest.py from typing import Undefined, List, Dict # Declaro los tipos de las variables texto = Undefined(str) entero = Undefined(int) lista_enteros = List[int]() dic_str_int = Dict[str, int]() # Asigno valores a las variables. texto = \u0026#39;Raul\u0026#39; entero = 13 lista_enteros = [1, 2, 3, 4] dic_str_int = {\u0026#39;raul\u0026#39;: 1, \u0026#39;ezequiel\u0026#39;: 2} # Intento asignar valores de otro tipo. texto = 1 entero = \u0026#39;raul\u0026#39; lista_enteros = [\u0026#39;raul\u0026#39;, 1, \u0026#39;2\u0026#39;] dic_str_int = {1: \u0026#39;raul\u0026#39;} Overwriting typeTest.py !mypy typeTest.py typeTest.py, line 16: Incompatible types in assignment (expression has type \u0026quot;int\u0026quot;, variable has type \u0026quot;str\u0026quot;) typeTest.py, line 17: Incompatible types in assignment (expression has type \u0026quot;str\u0026quot;, variable has type \u0026quot;int\u0026quot;) typeTest.py, line 18: List item 1 has incompatible type \u0026quot;str\u0026quot; typeTest.py, line 18: List item 3 has incompatible type \u0026quot;str\u0026quot; typeTest.py, line 19: List item 1 has incompatible type \u0026quot;Tuple[int, str]\u0026quot; Otra alternativa que nos ofrece MyPy para asignar un tipo de datos a las variables, es utilizar comentarios; as√≠, el ejemplo anterior lo podr√≠amos reescribir de la siguiente forma, obteniendo el mismo resultado:\n%%writefile typeTest.py from typing import List, Dict # Declaro los tipos de las variables texto = \u0026#39;\u0026#39; # type: str entero = 0 # type: int lista_enteros = [] # type: List[int] dic_str_int = {} # type: Dict[str, int] # Asigno valores a las variables. texto = \u0026#39;Raul\u0026#39; entero = 13 lista_enteros = [1, 2, 3, 4] dic_str_int = {\u0026#39;raul\u0026#39;: 1, \u0026#39;ezequiel\u0026#39;: 2} # Intento asignar valores de otro tipo. texto = 1 entero = \u0026#39;raul\u0026#39; lista_enteros = [\u0026#39;raul\u0026#39;, 1, \u0026#39;2\u0026#39;] dic_str_int = {1: \u0026#39;raul\u0026#39;} Overwriting typeTest.py !mypy typeTest.py typeTest.py, line 16: Incompatible types in assignment (expression has type \u0026quot;int\u0026quot;, variable has type \u0026quot;str\u0026quot;) typeTest.py, line 17: Incompatible types in assignment (expression has type \u0026quot;str\u0026quot;, variable has type \u0026quot;int\u0026quot;) typeTest.py, line 18: List item 1 has incompatible type \u0026quot;str\u0026quot; typeTest.py, line 18: List item 3 has incompatible type \u0026quot;str\u0026quot; typeTest.py, line 19: List item 1 has incompatible type \u0026quot;Tuple[int, str]\u0026quot; Instalando MyPy# Instalar MyPy es bastante f√°cil, simplemente debemos seguir los siguientes pasos:\nSi utilizan git, pueden clonar el repositorio de mypy:\n**$ git clone https://github.com/JukkaL/mypy.git **\nSi no utilizan git, como alternativa, se pueden descargar la √∫ltima versi√≥n de mypy en el siguiente link:\nhttps://github.com/JukkaL/mypy/archive/master.zip\nUna vez que ya se lo descargaron, se posicionan dentro de la carpeta de mypy y ejecutan el script setup.py para instalarlo:\n**$ python3 setup.py install **\nReemplacen \u0026lsquo;python3\u0026rsquo; con su interprete para python3.\nMyPy como parte de Python 3.5 ?# Guido van Rossum, el creador de Python, ha enviado reciente una propuesta a la lista de correo de python-ideas, en la cual sugiere agregar en la pr√≥xima versi√≥n de Python la sintaxis de MyPy para las functions annotations. Pueden encontrar la propuesta en el siguiente link:\nhttps://mail.python.org/pipermail/python-ideas/2014-August/028618.html\nTambi√©n pueden seguir las discusiones que se generaron sobre este tema en Reddit\nSaludos!\nEste post fue escrito utilizando IPython notebook. Pueden descargar este notebook o ver su version est√°tica en nbviewer.\n","date":"2014-08-18","id":44,"permalink":"/blog/2014/08/18/mypy-python-y-un-sistema-de-tipado-estatico/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nUna de las razones por la que solemos amar a Python, es por su sistema de tipado din√°mico, el cual lo convierte en un lenguaje de programaci√≥n sumamente flexible y f√°cil de aprender; al no tener que preocuparnos por definir los tipos de los objetos, ya que Python los infiere por nosotros, podemos escribir programas en una forma mucho m√°s productiva, sin verbosidad y utilizando menos l√≠neas de c√≥digo.","tags":["python","programacion"],"title":"MyPy - Python y un sistema de tipado est√°tico"},{"content":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nEn mi art√≠culo anterior hice una breve introducci√≥n al mundo de Python, hoy voy a detallar algunas de las librer√≠as que son esenciales para trabajar con Python en la comunidad cient√≠fica o en el an√°lisis de datos.\nUna de las grandes ventajas que ofrece Python sobre otros lenguajes de programaci√≥n, adem√°s de que es que es mucho m√°s f√°cil de aprender; es lo grande y prolifera que es la comunidad de desarrolladores que lo rodean; comunidad que ha contribuido con una gran variedad de librer√≠as de primer nivel que extienden la funcionalidades del lenguaje. Vamos a poder encontrar una librer√≠a en Python para pr√°cticamente cualquier cosa que se nos ocurra.\nAlgunas de las librer√≠as que se han vuelto esenciales y ya forman casi parte del lenguaje en s√≠ mismo son las siguientes:\nNumpy# Numpy, abreviatura de Numerical Python , es el paquete fundamental para la computaci√≥n cient√≠fica en Python. Dispone, entre otras cosas de:\nUn objeto matriz multidimensional ndarray,r√°pido y eficiente. Funciones para realizar c√°lculos elemento a elemento u otras operaciones matem√°ticas con matrices. Herramientas para la lectura y escritura de los conjuntos de datos basados matrices. Operaciones de √°lgebra lineal, transformaciones de Fourier, y generaci√≥n de n√∫meros aleatorios. Herramientas de integraci√≥n para conectar [C](https://es.wikipedia.org/wiki/C_(lenguaje_de_programaci%C3%B3n), C++ y Fortran con Python M√°s all√° de las capacidades de procesamiento r√°pido de matrices que Numpy a√±ade a Python, uno de sus prop√≥sitos principales con respecto al an√°lisis de datos es la utilizaci√≥n de sus estructuras de datos como contenedores para transmitir los datos entre diferentes algoritmos. Para datos num√©ricos , las matrices de Numpy son una forma mucho m√°s eficiente de almacenar y manipular datos que cualquier otra de las estructuras de datos est√°ndar incorporadas en Python. Asimismo, librer√≠as escritas en un lenguaje de bajo nivel, como [C](https://es.wikipedia.org/wiki/C_(lenguaje_de_programaci%C3%B3n) o Fortran, pueden operar en los datos almacenados en matrices de Numpy sin necesidad de copiar o modificar ning√∫n dato.\nComo nomenclatura general, cuando importamos la librer√≠a Numpy en nuestro programa Python se suele utilizar la siguiente:\nimport numpy as np Creando matrices en Numpy# Existen varias maneras de crear matrices en Numpy, por ejemplo desde:\nUna lista o tuple de Python Funciones espec√≠ficas para crear matrices como arange, linspace, etc. Archivos planos con datos, como por ejemplo archivos .csv En Numpy tanto los vectores como las matrices se crean utilizando el objeto ndarray\n#Creando un vector desde una lista de Python vector = np.array([1, 2, 3, 4]) vector array([1, 2, 3, 4]) #Para crear una matriz, simplemente le pasamos una lista anidada al objeto array de Numpy matriz = np.array([[1, 2], [3, 4]]) matriz array([[1, 2], [3, 4]]) #El tipo de objeto de tanto de los vectores como de las matrices es ndarray type(vector), type(matriz) (numpy.ndarray, numpy.ndarray) #Los objetos ndarray de Numpy cuentan con las propiedades shape y size que nos muestran sus dimensiones. print vector.shape, vector.size print matriz.shape, matriz.size (4,) 4 (2, 2) 4 Utilizando funciones para crear matrices# #arange #La funcion arange nos facilita la creaci√≥n de matrices x = np.arange(1, 11, 1) # argumentos: start, stop, step x array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) #linspace #linspace nos devuelve un vector con la cantidad de muestras que le ingresemos y separados uniformamente entre s√≠. np.linspace(1, 25, 25) # argumentos: start, stop, samples array([ 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.]) #mgrid #Con mgrid podemos crear arrays multimensionales. x, y = np.mgrid[0:5, 0:5] x array([[0, 0, 0, 0, 0], [1, 1, 1, 1, 1], [2, 2, 2, 2, 2], [3, 3, 3, 3, 3], [4, 4, 4, 4, 4]]) y array([[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4]]) #zeros y ones #Estas funciones nos permiten crear matrices de ceros o de unos. np.zeros((3,3)) array([[ 0., 0., 0.], [ 0., 0., 0.], [ 0., 0., 0.]]) np.ones((3,3)) array([[ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.]]) #random.randn #Esta funcion nos permite generar una matriz con una distribuci√≥n est√°ndar de n√∫meros. np.random.randn(5,5) array([[ 1.39342127, 0.27553779, 1.60499887, 0.49998319, 0.70528917], [ 0.77384386, 0.13082401, -0.94628073, 1.11938778, -0.03671148], [-1.26643358, -0.49647634, 0.02653584, 1.69748904, 0.83353017], [ 2.37892618, -1.21239237, 1.12638933, 1.70430737, 0.50932112], [-0.67529314, -0.48119409, -0.6064923 , 0.03554073, -0.29703706]]) #diag #Nos permite crear una matriz con la diagonal de n√∫meros que le ingresemos. np.diag([1,1,1]) array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]) Matplotlib# Matplotlib es la librer√≠a m√°s popular en Python para visualizaciones y gr√°ficos. Matplotlib puede producir gr√°ficos de alta calidad dignos de cualquier publicaci√≥n cient√≠fica.\nAlgunas de las muchas ventajas que nos ofrece Matplotlib, incluyen:\nEs f√°cil de aprender. Soporta texto, t√≠tulos y etiquetas en formato \\(\\LaTeX\\). Proporciona un gran control sobre cada uno de los elementos de las figuras, como ser su tama√±o, el trazado de sus l√≠neas, etc. Nos permite crear gr√°ficos y figuras de gran calidad que pueden ser guardados en varios formatos, como ser: PNG, PDF, SVG, EPS, y PGF. Matplotlib se integra de maravilla con IPython (ver m√°s abajo), lo que nos proporciona un ambiente confortable para las visualizaciones y la exploraci√≥n de datos interactiva.\nAlgunos gr√°ficos con Matplotlib# #Generalmente se suele importar matplotlib de la siguiente forma. import matplotlib.pyplot as plt Ahora vamos a graficar la siguiente funci√≥n.\n$$f(x) = e^{-x^2}$$ # Definimos nuestra funci√≥n. def f(x): return np.exp(-x ** 2) #Creamos un vector con los puntos que le pasaremos a la funcion previamente creada. x = np.linspace(-1, 5, num=30) #Representeamos la funci√≥n utilizando el objeto plt de matplotlib plt.xlabel(\u0026#34;Eje $x$\u0026#34;) plt.ylabel(\u0026#34;$f(x)$\u0026#34;) plt.legend() plt.title(\u0026#34;Funcion $f(x)$\u0026#34;) plt.grid(True) fig = plt.plot(x, f(x), label=\u0026#34;Funci√≥n f(x)\u0026#34;) #Grafico de puntos con matplotlib N = 100 x1 = np.random.randn(N) #creando vector x y1 = np.random.randn(N) #creando vector x s = 50 + 50 * np.random.randn(N) #variable para modificar el tama√±o(size) c = np.random.randn(N) #variable para modificar el color(color) plt.scatter(x1, y1, s=s, c=c, cmap=plt.cm.Blues) plt.grid(True) plt.colorbar() fig = plt.scatter(x1, y1) Interfase orientada a objetos de matplotlib# La idea principal con la programaci√≥n orientada a objetos es que a los objetos que se pueden aplicar funciones y acciones, y ning√∫n objeto deber√≠a tener un estado global (como en el caso de la interfase con plt que acabamos de utilizar). La verdadera ventaja de este enfoque se hace evidente cuando se crean m√°s de una figura, o cuando una figura contiene m√°s de una trama secundaria.\nPara utilizar la API orientada a objetos comenzamos de forma similar al ejemplo anterior, pero en lugar de crear una nueva instancia global de plt, almacenamos una referencia a la recientemente creada figura en la variable fig, y a partir de ella creamos un nuevo eje ejes usando el m√©todo add_axes de la instancia Figure:\nx = linspace(0, 5, 10) # Conjunto de puntos y = x ** 2 # Funcion fig = plt.figure() axes1 = fig.add_axes([0.1, 0.1, 0.8, 0.8]) # Eje principal axes2 = fig.add_axes([0.2, 0.5, 0.4, 0.3]) # Eje secundario # Figura principal axes1.plot(x, y, \u0026#39;r\u0026#39;) axes1.set_xlabel(\u0026#39;x\u0026#39;) axes1.set_ylabel(\u0026#39;y\u0026#39;) axes1.set_title(\u0026#39;Ej OOP\u0026#39;) # Insertada axes2.plot(y, x, \u0026#39;g\u0026#39;) axes2.set_xlabel(\u0026#39;y\u0026#39;) axes2.set_ylabel(\u0026#39;x\u0026#39;) axes2.set_title(\u0026#39;insertado\u0026#39;); # Ejemplo con m√°s de una figura. fig, axes = plt.subplots(nrows=1, ncols=2) for ax in axes: ax.plot(x, y, \u0026#39;r\u0026#39;) ax.set_xlabel(\u0026#39;x\u0026#39;) ax.set_ylabel(\u0026#39;y\u0026#39;) ax.set_title(\u0026#39;titulo\u0026#39;) fig.tight_layout() IPython# IPython promueve un ambiente de trabajo de ejecutar-explorar en contraposici√≥n al tradicional modelo de desarrollo de software de editar-compilar-ejecutar. Es decir, que el problema computacional a resolver es m√°s visto como todo un proceso de ejecucion de tareas, en lugar del tradicional modelo de producir una respuesta(output) a una pregunta(input). IPython tambi√©n provee una estrecha integraci√≥n con nuestro sistema operativo, permitiendo acceder f√°cilmente a todos nuestros archivos desde la misma herramienta.\nAlgunas de las caracter√≠sticas sobresalientes de IPython son:\nSu poderoso shell interactivo. Notebook, su interfase web con soporte para c√≥digo, texto, expresiones matem√°ticas, gr√°ficos en l√≠nea y multimedia. Su soporte para poder realizar visualizaciones de datos en forma interactiva. IPython esta totalmente integrado con matplotlib. Su simple y flexible interfase para trabajar con la computaci√≥n paralela. IPython es mucho m√°s que una librer√≠a, es todo un ambiente de trabajo que nos facilita enormemente trabajar con Python; las mismas p√°ginas de este blog est√°n desarrolladas con la ayuda del fant√°stico Notebook de IPython. (para ver el Notebook en el que se basa este art√≠culo, visiten el siguiente enlace.)\nPara m√°s informaci√≥n sobre IPython y algunas de sus funciones los invito tambi√©n a visitar el art√≠culo que escrib√≠ en mi otro blog.\nPandas# Pandas es una librer√≠a open source que aporta a Python unas estructuras de datos f√°ciles de user y de alta performance, junto con un gran n√∫mero de funciones esenciales para el an√°lisis de datos. Con la ayuda de Pandas podemos trabajar con datos estructurados de una forma m√°s r√°pida y expresiva.\nAlgunas de las cosas sobresalientes que nos aporta Pandas son:\nUn r√°pido y eficiente objeto DataFrame para manipular datos con indexaci√≥n integrada; herramientas para la lectura y escritura de datos entre estructuras de datos r√°pidas y eficientes manejadas en memoria, como el DataFrame, con la mayor√≠a de los formatos conocidos para el manejo de datos, como ser: CSV y archivos de texto, archivos Microsoft Excel, bases de datos SQL, y el formato cient√≠fico HDF5. Proporciona una alineaci√≥n inteligente de datos y un manejo integrado de los datos faltantes; con estas funciones podemos obtener una ganancia de performace en los c√°lculos entre DataFrames y una f√°cil manipulaci√≥n y ordenamiento de los datos de nuestro data set; Flexibilidad para manipular y redimensionar nuestro data set, facilidad para construir tablas pivote; La posibilidad de filtrar los datos, agregar o eliminar columnas de una forma sumamente expresiva; Operaciones de merge y join altamente eficientes sobre nuestros conjuntos de datos; Indexaci√≥n jer√°rquica que proporciona una forma intuitiva de trabajar con datos de alta dimensi√≥n en una estructura de datos de menor dimensi√≥n ; Posibilidad de realizar c√°lculos agregados o transformaciones de datos con el poderoso motor group by que nos permite dividir-aplicar-combinar nuestros conjuntos de datos; combina las caracter√≠sticas de las matrices de alto rendimiento de Numpy con las flexibles capacidades de manipulaci√≥n de datos de las hojas de c√°lculo y bases de datos relacionales (tales como SQL); Gran n√∫mero de funcionalidades para el manejo de series de tiempo ideales para el an√°lisis financiero; Todas sus funciones y estructuras de datos est√°n optimizadas para el alto rendimiento, con las partes cr√≠ticas del c√≥digo escritas en Cython o [C](https://es.wikipedia.org/wiki/C_(lenguaje_de_programaci%C3%B3n); Estructuras de datos de Pandas# # Importando pandas import pandas as pd Series# # Las series son matrices de una sola dimension similares a los vectores, pero con su propio indice. # Creando una Serie serie = pd.Series([2, 4, -8, 3]) serie 0 2 1 4 2 -8 3 3 dtype: int64 # podemos ver tantos los √≠ndices como los valores de las Series. print serie.values print serie.index [ 2 4 -8 3] Int64Index([0, 1, 2, 3], dtype='int64') # Creando Series con nuestros propios √≠ndices. serie2 = pd.Series([2, 4, -8, 3], index=[\u0026#39;d\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;c\u0026#39;]) serie2 d 2 b 4 a -8 c 3 dtype: int64 # Accediendo a los datos a trav√©s de los √≠ndices print serie2[\u0026#39;a\u0026#39;] print serie2[[\u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;]] print serie2[serie2 \u0026gt; 0] -8 b 4 c 3 d 2 dtype: int64 d 2 b 4 c 3 dtype: int64 DataFrame# # El DataFrame es una estructura de datos tabular similar a las hojas de c√°lculo de Excel. # Posee tanto indices de columnas como de filas. # Creando un DataFrame. data = {\u0026#39;state\u0026#39;: [\u0026#39;Ohio\u0026#39;, \u0026#39;Ohio\u0026#39;, \u0026#39;Ohio\u0026#39;, \u0026#39;Nevada\u0026#39;, \u0026#39;Nevada\u0026#39;], \u0026#39;year\u0026#39; : [2000, 2001, 2002, 2001, 2002], \u0026#39;pop\u0026#39; : [1.5, 1.7, 3.6, 2.4, 2.9]} frame = pd.DataFrame(data) # Creando un DataFrame desde un diccionario frame pop state year 0 1.5 Ohio 2000 1 1.7 Ohio 2001 2 3.6 Ohio 2002 3 2.4 Nevada 2001 4 2.9 Nevada 2002 5 rows √ó 3 columns\n# Creando un DataFrame desde un archivo. !cat \u0026#39;dataset.csv\u0026#39; # ejemplo archivo csv. pop,state,year 1.5,Ohio,2000 1.7,Ohio,2001 3.6,Ohio,2002 2.4,Nevada,2001 2.9,Nevada,2002 # Leyendo el archivo dataset.csv para crear el DataFrame frame2 = pd.read_csv(\u0026#39;dataset.csv\u0026#39;, header=0) frame2 pop state year 0 1.5 Ohio 2000 1 1.7 Ohio 2001 2 3.6 Ohio 2002 3 2.4 Nevada 2001 4 2.9 Nevada 2002 5 rows √ó 3 columns\n# Seleccionando una columna como una Serie frame[\u0026#39;state\u0026#39;] 0 Ohio 1 Ohio 2 Ohio 3 Nevada 4 Nevada Name: state, dtype: object # Seleccionando una l√≠nea como una Serie. frame.ix[1] pop 1.7 state Ohio year 2001 Name: 1, dtype: object # Verificando las columnas frame.columns Index([u'pop', u'state', u'year'], dtype='object') # Verificando los √≠ndices. frame.index Int64Index([0, 1, 2, 3, 4], dtype='int64') Otras librer√≠as dignas de mencion# Otras librer√≠as que tambi√©n son muy importantes para el an√°lisis de datos con Python son:\nSciPy# SciPy es un conjunto de paquetes donde cada uno ellos ataca un problema distinto dentro de la computaci√≥n cient√≠fica y el an√°lisis num√©rico. Algunos de los paquetes que incluye, son:\nscipy.integrate: que proporciona diferentes funciones para resolver problemas de integraci√≥n num√©rica. scipy.linalg: que proporciona funciones para resolver problemas de √°lgebra lineal. scipy.optimize: para los problemas de optimizaci√≥n y minimizaci√≥n. scipy.signal: para el an√°lisis y procesamiento de se√±ales. scipy.sparse: para matrices dispersas y solucionar sistemas lineales dispersos scipy.stats: para el an√°lisis de estad√≠stica y probabilidades. Scikit-learn# Scikit-learn es una librer√≠a especializada en algoritmos para data mining y machine learning.\nAlgunos de los problemas que podemos resolver utilizando las herramientas de Scikit-learn, son:\nClasificaciones: Identificar las categor√≠as a que cada observaci√≥n del conjunto de datos pertenece. Regresiones: Predecire el valor continuo para cada nuevo ejemplo. Agrupaciones: Agrupaci√≥n autom√°tica de objetos similares en un conjunto. Reducci√≥n de dimensiones: Reducir el n√∫mero de variables aleatorias a considerar. Selecci√≥n de Modelos: Comparar, validar y elegir par√°metros y modelos. Preprocesamiento: Extracci√≥n de caracter√≠sticas a analizar y normalizaci√≥n de datos. ","date":"2014-05-28","id":45,"permalink":"/blog/2014/05/28/python-librerias-esenciales-para-el-analisis-de-datos/","summary":"Esta notebook fue creada originalmente como un blog post por Ra√∫l E. L√≥pez Briega en Mi blog sobre Python. El contenido esta bajo la licencia BSD.\nEn mi art√≠culo anterior hice una breve introducci√≥n al mundo de Python, hoy voy a detallar algunas de las librer√≠as que son esenciales para trabajar con Python en la comunidad cient√≠fica o en el an√°lisis de datos.\nUna de las grandes ventajas que ofrece Python sobre otros lenguajes de programaci√≥n, adem√°s de que es que es mucho m√°s f√°cil de aprender; es lo grande y prolifera que es la comunidad de desarrolladores que lo rodean; comunidad que ha contribuido con una gran variedad de librer√≠as de primer nivel que extienden la funcionalidades del lenguaje.","tags":["python","estadistica","programacion","machine learning","analisis de datos"],"title":"Python - Librer√≠as esenciales para el an√°lisis de datos"},{"content":"Hoy comienzo mi nuevo blog en github; este nuevo blog, que se suma a a mi otro blog relopezbriega.com.ar, lo voy a dedicar enteramente a Python. Por tal motivo, en este primer art√≠culo, voy a explicar a grandes rasgos que es Python y por qu√© me gusta tanto trabajar con Python como para dedicarle este blog.\nPython es un lenguaje de programaci√≥n de alto nivel que se caracteriza por hacer hincapi√© en una sintaxis limpia, que favorece un c√≥digo legible y f√°cilmente administrable. Python funciona en las plataformas Windows, Linux/Unix, Mac OS X e incluso ha sido portado a las m√°quinas virtuales de Java (a trav√©s de Jython) y .Net (a trav√©s de IronPython). Python es un lenguaje libre y f√°cil de aprender que te permite trabajar m√°s r√°pido e integrar tus sistemas de manera m√°s eficaz; con Python se puede ganar r√°pidamente en productividad.\nPython, a diferencia de otros lenguajes de programaci√≥n como C, C++ o Java es interpretado y dinamicamente tipado; lo que quiere decir que no es necesario compilar el fuente para poder ejecutarlo (interpretado) y que sus variables pueden tomar distintos tipos de objetos (dinamicamente tipado); esto hace que el lenguaje sea sumamente flexible y de r√°pida implementaci√≥n; aunque pierde en rendimiento y es m√°s propenso a errores de programaci√≥n que los lenguajes antes mencionados.\nPrincipales fortalezas de Python# Las principales fortalezas que hacen que uno ame a Python son:\nEs Orientado a Objetos. Python es un lenguaje de programaci√≥n Orientado a Objetos desde casi su concepci√≥n, su modelo de clases soporta las notaciones avanzadas de polimorfismo, sobrecarga de operadores y herencia m√∫ltiple. La programaci√≥n Orientado a Objetos es sumamente f√°cil de aplicar con la sintaxis simple que nos proporciona Python. Asimismo, tambi√©n es importante destacar que en Python, la programaci√≥n Orientado a Objetos es una opci√≥n y no algo obligatorio como es en Java; ya que Python es multiparadigma y nos permite programar siguiendo un modelo Orientado a Objetos o un modelo imperativo.\nEs software libre. Python es completamente libre para ser utilizado y redistribuido; no posee restricciones para copiarlo, embeberlo en nuestros sistemas o ser vendido junto con otros productos. Python es un proyecto open source que es administrado por Python Software Foundation, instituci√≥n que se encarga de su soporte y desarrollo.\nEs portable. La implementaci√≥n estandar de Python esta escrita en C, y puede ser compilada y ejecutada en pr√°cticamente cualquier plataforma que se les ocurra. Podemos encontrar a Python en peque√±os dispositivos, como tel√©fonos celulares, hasta grandes infraestructuras de Hardware, como las supercomputadoras. Al ser un lenguaje interpretado el mismo c√≥digo fuente puede ser ejecutado en cualquier plataforma sin necesidad de realizar grandes cambios.\nEs poderoso. Python proporciona toda la sencillez y facilidad de uso de un lenguaje de programaci√≥n interpretado, junto con las m√°s avanzadas herramientas de ingenier√≠a de software que se encuentran t√≠picamente en los lenguajes compilados. A diferencia de otros lenguajes interpretados, esta combinaci√≥n hace a Python sumamente √∫til para proyectos de desarrollo a gran escala.\nF√°cil integraci√≥n. Los programas escritos en Python pueden ser f√°cilmente integrados con componentes escritos en otros lenguajes. Por ejemplo la C API de Python permite una f√°cil integraci√≥n entre los dos lenguajes, permitiendo que los programas escritos en Python puedan llamar a funciones escritas en C y viceversa.\nF√°cil de usar. Para ejecutar un programa en Python simplemente debemos escribirlo y ejecutarlo, no existen pasos intermedios de linkeo o compilaci√≥n como podemos tener en otros lenguajes de programaci√≥n. Con Python podemos programar en forma interactiva, basta tipear una sentencia para poder ver inmediatamente el resultado. Adem√°s los programas en Python son m√°s simples, m√°s peque√±os y m√°s flexibles que los programas equivalentes en lenguajes como C, C++ o Java.\nF√°cil de aprender. Desde mi punto de vista, esta es sin duda la principal fortaleza del lenguaje; comparado con otros lenguajes de programaci√≥n, Python es sumamente f√°cil de aprender, en tan s√≥lo un par de d√≠as se puede estar programando eficientemente con Python.\nInstalando Python# En Linux# Instalar Python en Linux no es necesario, ya que viene preinstalado en todas las distribuciones m√°s populares.\nEn Windows# La forma m√°s sencilla de poder instalar Python en Windows es instalando alguna de las distribuciones de Python que ya vienen armadas con los principales m√≥dulos. Yo les recomiendo la distribuci√≥n Anaconda, que se puede descargar en forma gratuita y viene integrada con todos los principales paquetes que vamos a necesitar para trabajar con Python. Una vez que la descargan, simplemente siguen los pasos del instalador y listo, ya tendr√°n todo un ambiente Python para trabajar en Windows.\nOtra distribuci√≥n de Python que pueden utilizar en Windows, es WinPython, la cual puede ser utilizada incluso en forma portable.\nHasta aqu√≠ este primer art√≠culo de mi nuevo blog; los invito a que se instalen y exploren Python, no solo es f√°cil de aprender, tambi√©n es muy divertido programar en Python!\n","date":"2014-05-25","id":46,"permalink":"/blog/2014/05/25/mi-python-blog-introduccion-a-python/","summary":"Hoy comienzo mi nuevo blog en github; este nuevo blog, que se suma a a mi otro blog relopezbriega.com.ar, lo voy a dedicar enteramente a Python. Por tal motivo, en este primer art√≠culo, voy a explicar a grandes rasgos que es Python y por qu√© me gusta tanto trabajar con Python como para dedicarle este blog.\nPython es un lenguaje de programaci√≥n de alto nivel que se caracteriza por hacer hincapi√© en una sintaxis limpia, que favorece un c√≥digo legible y f√°cilmente administrable.","tags":["python","programacion"],"title":"Mi Python Blog: Introducci√≥n a Python "}]